# v1.7.0 Intelligence Layer Specification

**Version:** 1.1 Final
**Date:** 2025-12-17
**Status:** Approved (AI Studio Review Complete)
**Authors:** Multi-AI Development Team
**Reviewers:** Claude.ai (Architect), AI Studio/Gemini (Performance Analyst)

---

## Executive Summary

v1.7.0 introduces the **Intelligence Layer** - the bridge between pm_encoder's stateless mechanism and CFD's adaptive policy. This release adds:

1. **Priority Groups** - Ranked file selection within lenses
2. **Token Budgeting** - Intelligent context fitting for LLM windows
3. **Protocol Adapter Interfaces** - Abstractions for future MCP/WASM delivery

**Guiding Principle:** *"pm_encoder remains stateless. Intelligence comes from configuration, not memory."*

---

## 1. Priority Groups Schema

### 1.1 Design Goals

- **Backward Compatible**: Existing `include`/`exclude` configs must work unchanged
- **Layered**: Groups override lens defaults, CLI overrides groups
- **Explicit**: No magic - priority is a visible, tunable number
- **Deterministic**: Same config → Same output (always)

### 1.2 Schema Definition

#### Current Schema (v1.6.0)

```json
{
  "lenses": {
    "architecture": {
      "description": "High-level code structure",
      "include": ["*.py", "*.rs"],
      "exclude": ["tests/**"],
      "truncate": 2000,
      "truncate_mode": "structure",
      "sort_by": "name",
      "sort_order": "asc"
    }
  }
}
```

#### Extended Schema (v1.7.0)

```json
{
  "lenses": {
    "architecture": {
      "description": "High-level code structure",

      "groups": [
        {
          "name": "core",
          "pattern": "src/core/**",
          "priority": 100,
          "truncate_mode": "full",
          "truncate": 0
        },
        {
          "name": "lib",
          "pattern": "src/lib/**",
          "priority": 80,
          "truncate_mode": "structure"
        },
        {
          "name": "utils",
          "pattern": "src/utils/**",
          "priority": 50,
          "truncate_mode": "smart",
          "truncate": 500
        },
        {
          "name": "tests",
          "pattern": "tests/**",
          "priority": 10,
          "truncate_mode": "simple",
          "truncate": 100
        }
      ],

      "fallback": {
        "priority": 50,
        "truncate_mode": "smart",
        "truncate": 300
      },

      "include": ["*.py", "*.rs", "*.md"],
      "exclude": ["*.pyc", "__pycache__"],
      "sort_by": "priority",
      "sort_order": "desc"
    }
  }
}
```

### 1.3 Group Object Schema

| Field | Type | Required | Default | Description |
|-------|------|----------|---------|-------------|
| `name` | string | No | auto | Human-readable group identifier |
| `pattern` | string | **Yes** | - | Glob pattern for file matching |
| `priority` | integer | No | 50 | Arbitrary integer, higher = more important (standard range: 0-100) |
| `truncate_mode` | string | No | inherit | `full`/`structure`/`smart`/`simple` |
| `truncate` | integer | No | inherit | Max lines (0 = no limit) |

**Priority Range:** Arbitrary signed integers are allowed (-9999 to 9999+). The "standard range" of 0-100 is documented for convention, but users may use any integer value. This supports multiple mental models (percentages, tiered systems, negative deprioritization).

### 1.4 Precedence Rules

```
Priority Resolution Order (highest to lowest):
1. CLI flags (--truncate, --include, etc.)
2. Group-specific settings (per-pattern)
3. Lens defaults (include, exclude, truncate)
4. Fallback settings (for unmatched files)
5. Global config file defaults
6. Hardcoded pm_encoder defaults
```

### 1.5 Backward Compatibility

**Rule:** If `groups` is absent, behavior is identical to v1.6.0.

```python
# Pseudocode for compatibility
def get_file_priority(file_path: Path, lens_config: dict) -> tuple[int, dict]:
    """Returns (priority, settings) for a file."""

    # v1.7.0 path: groups defined
    if "groups" in lens_config:
        for group in lens_config["groups"]:
            if fnmatch(file_path, group["pattern"]):
                return group.get("priority", 50), group

        # No group matched, use fallback (default: 50 - Principle of Least Surprise)
        fallback = lens_config.get("fallback", {})
        return fallback.get("priority", 50), fallback

    # v1.6.0 path: no groups, all files equal priority
    return 50, lens_config
```

### 1.6 Sort Mode Extension

New `sort_by` option: `"priority"`

```python
# Sort key function
if sort_by == "priority":
    sort_key_func = lambda p: (-get_file_priority(p, lens_config)[0], p.name.lower())
```

**Tie-breaking:** When priority is equal, sort alphabetically by path (deterministic).

---

## 2. Token Budgeting Algorithm

### 2.1 Design Goals

- **Fit to Window**: Automatically trim context to fit LLM token limits
- **Priority-Aware**: High-priority files are never dropped before low-priority
- **Predictable**: Same budget → Same output
- **Graceful Degradation**: Warn when budget forces aggressive cuts

### 2.2 CLI Interface

```bash
# New flags - supports shorthand notation
pm_encoder . --lens architecture --token-budget 100000
pm_encoder . --lens architecture --token-budget 100k      # 100,000 tokens
pm_encoder . --lens architecture --token-budget 2M        # 2,000,000 tokens

# With explicit model hint (for accurate tokenization)
pm_encoder . --lens architecture --token-budget 100k --token-model cl100k_base

# With budget strategy
pm_encoder . --lens architecture --token-budget 50k --budget-strategy hybrid
```

| Flag | Type | Default | Description |
|------|------|---------|-------------|
| `--token-budget` | string | 0 (disabled) | Max tokens (supports `100k`, `2M` shorthand) |
| `--token-model` | string | `cl100k_base` | Tokenizer model (tiktoken) |
| `--budget-strategy` | string | `drop` | `drop`, `truncate`, or `hybrid` |

### 2.2.1 Shorthand Notation Parsing

**DECIDED:** Support `k`/`K` (thousands) and `m`/`M` (millions) suffixes.

```python
import re

def parse_token_budget(value: str) -> int:
    """Parse token budget with optional k/M suffix."""
    match = re.match(r'^(\d+)([kKmM]?)$', value.strip())
    if not match:
        raise ValueError(f"Invalid token budget: {value}")

    number = int(match.group(1))
    suffix = match.group(2).lower()

    multipliers = {'': 1, 'k': 1_000, 'm': 1_000_000}
    return number * multipliers[suffix]

# Examples:
# "100000" → 100000
# "100k"   → 100000
# "100K"   → 100000
# "2M"     → 2000000
# "2m"     → 2000000
```

**Rationale:** Standard CLI UX pattern (similar to `head -n 1000`, `du -h`). Reduces typos in large numbers.

### 2.3 Token Estimation

**DECIDED:** tiktoken is an **optional dependency** with heuristic fallback.

**Rationale:** Preserves Zero Dependency philosophy while enabling accurate counting when available.

```python
_TIKTOKEN_AVAILABLE = None
_TIKTOKEN_WARNING_SHOWN = False

def estimate_tokens(content: str, model: str = "cl100k_base") -> int:
    """Estimate token count for content."""
    global _TIKTOKEN_AVAILABLE, _TIKTOKEN_WARNING_SHOWN

    # Lazy check for tiktoken availability
    if _TIKTOKEN_AVAILABLE is None:
        try:
            import tiktoken
            _TIKTOKEN_AVAILABLE = True
        except ImportError:
            _TIKTOKEN_AVAILABLE = False

    if _TIKTOKEN_AVAILABLE:
        import tiktoken
        enc = tiktoken.get_encoding(model)
        return len(enc.encode(content))
    else:
        # Fallback: ~4 chars per token (rough estimate)
        if not _TIKTOKEN_WARNING_SHOWN:
            print("WARNING: tiktoken not installed, using heuristic token estimation (~4 chars/token).",
                  file=sys.stderr)
            print("         Install with: pip install tiktoken", file=sys.stderr)
            _TIKTOKEN_WARNING_SHOWN = True
        return len(content) // 4
```

**Installation:** `pip install tiktoken` (optional)

**Per-File Estimation:**
```python
def estimate_file_tokens(file_path: Path, content: str, was_truncated: bool) -> int:
    """Estimate tokens including PM format overhead."""
    # Content tokens
    content_tokens = estimate_tokens(content)

    # Header/footer overhead: "++++++++++ path ++++++++++\n" + "---------- path checksum ----------\n"
    overhead_tokens = estimate_tokens(f"++++++++++ {file_path} ++++++++++\n---------- {file_path} {' '*32} ----------\n")

    return content_tokens + overhead_tokens
```

### 2.4 Budgeting Algorithm

#### Budget Strategies

| Strategy | Behavior | Use Case |
|----------|----------|----------|
| `drop` | Exclude files that don't fit | Default, clean context |
| `truncate` | Aggressively truncate files to fit | Maximum file coverage |
| `hybrid` | Truncate large files, drop if still too big | Balanced approach |

**NEW (AI Studio):** The `hybrid` strategy prevents a single massive file from consuming the entire budget.

```python
def apply_token_budget(
    files: List[Tuple[Path, str, int]],  # (path, content, priority)
    budget: int,
    strategy: str = "drop"
) -> Tuple[List[Tuple[Path, str]], BudgetReport]:
    """
    Select files to fit within token budget.

    Args:
        files: List of (path, content, priority) tuples
        budget: Maximum tokens allowed
        strategy: "drop", "truncate", or "hybrid"

    Returns:
        (selected_files, report)
    """
    # Step 1: Calculate tokens for each file
    file_tokens = []
    for path, content, priority in files:
        tokens = estimate_file_tokens(path, content, was_truncated=False)
        file_tokens.append((path, content, priority, tokens))

    # Step 2: Sort by priority (DESC) then path (ASC) for determinism
    file_tokens.sort(key=lambda x: (-x[2], x[0].as_posix()))

    # Step 3: Accumulate until budget exceeded
    selected = []
    total_tokens = 0
    dropped = []
    truncated_files = []

    # Hybrid strategy threshold: files > 10% of budget get structure-truncated
    HYBRID_THRESHOLD = 0.10

    for path, content, priority, tokens in file_tokens:
        # Hybrid: Pre-truncate large files
        if strategy == "hybrid" and tokens > budget * HYBRID_THRESHOLD:
            target_tokens = int(budget * HYBRID_THRESHOLD)
            content = truncate_to_structure(content, path)
            tokens = estimate_tokens(content)
            truncated_files.append(path)

        if total_tokens + tokens <= budget:
            selected.append((path, content))
            total_tokens += tokens
        else:
            if strategy in ("truncate", "hybrid"):
                # Try aggressive truncation to fit
                remaining_budget = budget - total_tokens
                if remaining_budget > 100:  # Minimum viable file
                    truncated_content = aggressive_truncate(content, remaining_budget)
                    if truncated_content:
                        selected.append((path, truncated_content))
                        total_tokens += estimate_tokens(truncated_content)
                        truncated_files.append(path)
                        continue

            dropped.append((path, priority, tokens))

    # Step 4: Generate report
    report = BudgetReport(
        budget=budget,
        used=total_tokens,
        selected_count=len(selected),
        dropped_count=len(dropped),
        dropped_files=dropped,
        truncated_files=truncated_files,
        strategy=strategy
    )

    return selected, report
```

### 2.5 Budget Report Format

**DECIDED:** Budget report goes to **stderr** (Unix philosophy: stdout=data, stderr=diagnostics).

**Rationale:** Enables clean piping: `pm_encoder . --token-budget 100k | llm`

```
================================================================================
TOKEN BUDGET REPORT
================================================================================
Budget:     100,000 tokens
Used:        95,234 tokens (95.2%)
Remaining:    4,766 tokens
Estimation:  Heuristic (~4 chars/token)  # or "tiktoken (cl100k_base)"

Files included: 42
Files dropped:   8 (lowest priority first)
Files truncated: 3 (hybrid strategy)

Dropped files:
  [P:10] tests/test_legacy.py (3,200 tokens)
  [P:10] tests/test_utils.py (2,800 tokens)
  [P:20] docs/archive/old_readme.md (1,500 tokens)
  ...

Truncated files (hybrid):
  src/large_module.py (15,000 → 2,500 tokens, structure mode)

Strategy: hybrid (files >10% budget auto-truncated)
================================================================================
```

### 2.5.1 Meta-Header Budget Info

**NEW (AI Studio):** Include budget summary in `.pm_encoder_meta` so LLMs know they're seeing a partial view.

```
++++++++++ .pm_encoder_meta ++++++++++
Context generated with lens: "architecture"
Focus: High-level code structure and configuration

Budget: 50,000 tokens (Heuristic) | Used: 49,500 | Cut: 12 files
Strategy: hybrid | Truncated: 3 files

Implementation details truncated using structure mode
...
---------- .pm_encoder_meta <checksum> .pm_encoder_meta ----------
```

**Implementation:**
```python
def get_meta_content(self, budget_report: Optional[BudgetReport] = None) -> str:
    """Generate .pm_encoder_meta file content with optional budget info."""
    lines = [
        f'Context generated with lens: "{self.active_lens}"',
        f"Focus: {self.active_lens_config.get('description', 'Custom lens')}",
        ""
    ]

    # NEW: Add budget summary if applicable
    if budget_report:
        estimation_method = "tiktoken" if _TIKTOKEN_AVAILABLE else "Heuristic"
        lines.append(f"Budget: {budget_report.budget:,} tokens ({estimation_method}) | "
                     f"Used: {budget_report.used:,} | Cut: {budget_report.dropped_count} files")
        lines.append(f"Strategy: {budget_report.strategy} | "
                     f"Truncated: {len(budget_report.truncated_files)} files")
        lines.append("")

    # ... rest of meta content
    return '\n'.join(lines)
```

**Rationale:** LLM needs to know it's seeing a partial view to avoid hallucinating about missing files.

### 2.6 Streaming Conflict Resolution

**Problem:** Token budgeting requires knowing total tokens before output, which conflicts with streaming.

**Solution:** Budgeting mode implicitly disables streaming.

```python
# In serialize()
if token_budget > 0 and stream_mode:
    print("WARNING: --token-budget disables --stream (requires buffering for budget calculation)",
          file=sys.stderr)
    stream_mode = False
```

**Alternative (Future v2.0):** Two-pass streaming
1. Pass 1: Scan files, estimate tokens, build inclusion list
2. Pass 2: Stream selected files

---

## 3. Protocol Adapter Interfaces

### 3.1 Design Goals

- **Decouple I/O**: Separate file reading from serialization logic
- **Enable Future Drivers**: CLI, MCP Server, WASM, Library API
- **Maintain Statelessness**: Adapters are pure I/O, no business logic

### 3.2 Python Interface (v1.7.0)

```python
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Iterator, Optional
from dataclasses import dataclass


@dataclass
class FileEntry:
    """Represents a file to be processed."""
    path: Path
    relative_path: Path
    size: int
    mtime: float

    # Optional metadata from Context Store (v2.0+)
    priority: Optional[int] = None
    tags: Optional[list[str]] = None
    shadow_path: Optional[Path] = None


class RepoSource(ABC):
    """Abstract interface for reading repository files."""

    @abstractmethod
    def get_root(self) -> Path:
        """Return the repository root path."""
        pass

    @abstractmethod
    def walk_files(self,
                   include_patterns: list[str] = None,
                   exclude_patterns: list[str] = None) -> Iterator[FileEntry]:
        """
        Yield files matching the given patterns.

        Args:
            include_patterns: Glob patterns to include (None = all)
            exclude_patterns: Glob patterns to exclude

        Yields:
            FileEntry objects for matching files
        """
        pass

    @abstractmethod
    def read_file(self, entry: FileEntry) -> Optional[str]:
        """
        Read file content as string.

        Returns:
            File content, or None if file should be skipped (binary, too large)
        """
        pass

    def get_file_priority(self, entry: FileEntry, lens_config: dict) -> int:
        """
        Get priority for a file based on lens configuration.
        Default implementation uses pattern matching.
        """
        if "groups" not in lens_config:
            return 50

        for group in lens_config["groups"]:
            if fnmatch(entry.relative_path.as_posix(), group["pattern"]):
                return group.get("priority", 50)

        return lens_config.get("fallback", {}).get("priority", 50)


class OutputSink(ABC):
    """Abstract interface for writing serialized output."""

    @abstractmethod
    def write_header(self, meta: dict) -> None:
        """Write output header (e.g., .pm_encoder_meta)."""
        pass

    @abstractmethod
    def write_file(self,
                   relative_path: Path,
                   content: str,
                   checksum: str,
                   was_truncated: bool = False,
                   original_lines: int = 0) -> None:
        """Write a single file in PM format."""
        pass

    @abstractmethod
    def write_footer(self, stats: dict) -> None:
        """Write output footer (optional stats/summary)."""
        pass

    @abstractmethod
    def finalize(self) -> None:
        """Finalize output (flush buffers, close connections)."""
        pass


# Concrete implementations for v1.7.0

class FileSystemSource(RepoSource):
    """Read files from local filesystem."""

    def __init__(self, root: Path):
        self._root = root.resolve()

    def get_root(self) -> Path:
        return self._root

    def walk_files(self, include_patterns=None, exclude_patterns=None) -> Iterator[FileEntry]:
        # Implementation: recursive walk with pattern matching
        ...

    def read_file(self, entry: FileEntry) -> Optional[str]:
        # Implementation: read with binary detection, encoding fallback
        ...


class StreamSink(OutputSink):
    """Write to a stream (file or stdout)."""

    def __init__(self, stream):
        self._stream = stream

    def write_header(self, meta: dict) -> None:
        # Write .pm_encoder_meta in PM format
        ...

    def write_file(self, relative_path, content, checksum, was_truncated=False, original_lines=0) -> None:
        # Write in PM format
        ...

    def write_footer(self, stats: dict) -> None:
        # Optional: write stats comment
        pass

    def finalize(self) -> None:
        self._stream.flush()
```

### 3.3 Future Rust Traits (v1.0.0 Rust)

```rust
pub trait RepoSource {
    fn get_root(&self) -> PathBuf;
    fn walk_files(&self, config: &WalkConfig) -> Box<dyn Iterator<Item = FileEntry>>;
    fn read_file(&self, entry: &FileEntry) -> Option<String>;
}

pub trait OutputSink {
    fn write_header(&mut self, meta: &Metadata);
    fn write_file(&mut self, entry: &FileEntry, content: &str, checksum: &str);
    fn write_footer(&mut self, stats: &Stats);
    fn finalize(&mut self);
}

// Drivers compose Source + Sink
pub fn encode(source: &dyn RepoSource, sink: &mut dyn OutputSink, config: &Config) {
    // Core encoding logic
}
```

### 3.4 Adapter Pattern Benefits

```
┌─────────────────────────────────────────────────────────────┐
│                    pm_encoder Core                          │
│  ┌─────────────────────────────────────────────────────┐   │
│  │              Encoding Logic (Stateless)              │   │
│  │  • Pattern matching   • Truncation   • Checksums    │   │
│  └────────────────┬────────────────────┬───────────────┘   │
│                   │                    │                    │
│            RepoSource              OutputSink               │
│                   │                    │                    │
├───────────────────┼────────────────────┼────────────────────┤
│                   ▼                    ▼                    │
│  ┌────────────────────────────────────────────────────┐    │
│  │                   Drivers                           │    │
│  │  • CLI (FileSystem → Stream)                       │    │
│  │  • MCP Server (FileSystem → JSON-RPC)              │    │
│  │  • WASM (VirtualFS → Memory)                       │    │
│  │  • Library (Custom → Custom)                       │    │
│  └────────────────────────────────────────────────────┘    │
└─────────────────────────────────────────────────────────────┘
```

---

## 4. Test Strategy

### 4.1 New Test Vector Category: `budgeting`

**Location:** `test_vectors/budgeting.json`

```json
{
  "category": "budgeting",
  "description": "Token budgeting and priority group tests",
  "vectors": [
    {
      "name": "priority_basic_sort",
      "description": "Files sorted by priority descending",
      "input": {
        "files": [
          {"path": "src/core/main.py", "content": "# core", "group": "core"},
          {"path": "tests/test.py", "content": "# test", "group": "tests"},
          {"path": "utils/helper.py", "content": "# util", "group": "utils"}
        ],
        "lens_config": {
          "groups": [
            {"pattern": "src/core/**", "priority": 100},
            {"pattern": "utils/**", "priority": 50},
            {"pattern": "tests/**", "priority": 10}
          ]
        },
        "sort_by": "priority"
      },
      "expected": {
        "order": ["src/core/main.py", "utils/helper.py", "tests/test.py"]
      }
    },
    {
      "name": "budget_drop_lowest",
      "description": "Drop lowest priority files when over budget",
      "input": {
        "files": [
          {"path": "core.py", "tokens": 500, "priority": 100},
          {"path": "lib.py", "tokens": 300, "priority": 80},
          {"path": "test.py", "tokens": 400, "priority": 10}
        ],
        "budget": 850,
        "strategy": "drop"
      },
      "expected": {
        "included": ["core.py", "lib.py"],
        "dropped": ["test.py"],
        "total_tokens": 800,
        "budget_used_pct": 94.1
      }
    },
    {
      "name": "budget_truncate_strategy",
      "description": "Aggressive truncation instead of dropping",
      "input": {
        "files": [
          {"path": "core.py", "tokens": 500, "priority": 100},
          {"path": "lib.py", "tokens": 500, "priority": 80}
        ],
        "budget": 800,
        "strategy": "truncate"
      },
      "expected": {
        "included": ["core.py", "lib.py"],
        "lib.py_truncated": true,
        "total_tokens_lte": 800
      }
    },
    {
      "name": "fallback_priority",
      "description": "Files not matching any group use fallback priority",
      "input": {
        "files": [
          {"path": "src/core/main.py", "group": "core"},
          {"path": "random/file.py", "group": null}
        ],
        "lens_config": {
          "groups": [
            {"pattern": "src/core/**", "priority": 100}
          ],
          "fallback": {"priority": 50}
        }
      },
      "expected": {
        "priorities": {
          "src/core/main.py": 100,
          "random/file.py": 50
        }
      }
    },
    {
      "name": "backward_compat_no_groups",
      "description": "Lens without groups works like v1.6.0",
      "input": {
        "files": [
          {"path": "a.py"},
          {"path": "b.py"}
        ],
        "lens_config": {
          "include": ["*.py"],
          "truncate": 500
        }
      },
      "expected": {
        "all_priority_equal": true,
        "default_priority": 50
      }
    },
    {
      "name": "hybrid_strategy_large_file",
      "description": "Hybrid strategy auto-truncates files >10% of budget",
      "input": {
        "files": [
          {"path": "core.py", "tokens": 500, "priority": 100},
          {"path": "massive.py", "tokens": 15000, "priority": 80},
          {"path": "small.py", "tokens": 200, "priority": 60}
        ],
        "budget": 10000,
        "strategy": "hybrid"
      },
      "expected": {
        "included": ["core.py", "massive.py", "small.py"],
        "massive.py_truncated": true,
        "massive.py_mode": "structure",
        "total_tokens_lte": 10000
      }
    },
    {
      "name": "shorthand_parsing",
      "description": "Token budget shorthand notation",
      "input": {
        "budget_strings": ["100000", "100k", "100K", "2m", "2M"]
      },
      "expected": {
        "parsed_values": [100000, 100000, 100000, 2000000, 2000000]
      }
    }
  ]
}
```

### 4.2 Unit Test Cases

```python
class TestPriorityGroups(unittest.TestCase):
    """Test priority group resolution."""

    def test_group_match_first_wins(self):
        """First matching group takes priority."""
        ...

    def test_fallback_when_no_match(self):
        """Unmatched files use fallback config."""
        ...

    def test_backward_compat_no_groups(self):
        """Lens without groups behaves like v1.6.0."""
        ...

    def test_group_truncate_override(self):
        """Group-specific truncate overrides lens default."""
        ...


class TestTokenBudgeting(unittest.TestCase):
    """Test token budgeting algorithm."""

    def test_budget_exact_fit(self):
        """Files exactly filling budget are all included."""
        ...

    def test_budget_drop_lowest_priority(self):
        """Lowest priority files dropped first."""
        ...

    def test_budget_deterministic(self):
        """Same input → same output (no randomness)."""
        ...

    def test_budget_disables_streaming(self):
        """--token-budget with --stream emits warning."""
        ...

    def test_token_estimation_accuracy(self):
        """Token estimation within 10% of tiktoken."""
        ...


class TestProtocolAdapters(unittest.TestCase):
    """Test RepoSource and OutputSink interfaces."""

    def test_filesystem_source_walk(self):
        """FileSystemSource correctly walks directory."""
        ...

    def test_stream_sink_pm_format(self):
        """StreamSink produces valid PM format."""
        ...

    def test_adapter_decoupling(self):
        """Core logic works with mock adapters."""
        ...
```

### 4.3 Integration Test: End-to-End Budgeting

```python
def test_budget_e2e():
    """End-to-end test: priority groups + token budget."""
    # Create temp directory with files of known sizes
    # Apply lens with groups
    # Set token budget
    # Verify correct files included/dropped
    # Verify budget report accuracy
```

---

## 5. Implementation Phases

### Phase 1: Priority Groups (Week 1)

1. Extend `LensManager` to parse `groups` array
2. Implement `get_file_priority()` function
3. Add `sort_by="priority"` option
4. Update `serialize()` to respect priorities
5. Add unit tests for priority resolution

### Phase 2: Token Budgeting (Week 2)

1. Add tiktoken dependency (optional, with fallback)
2. Implement `estimate_tokens()` function
3. Implement `apply_token_budget()` algorithm
4. Add `--token-budget` and `--budget-strategy` CLI flags
5. Implement budget report generation
6. Add unit tests for budgeting

### Phase 3: Protocol Adapters (Week 3)

1. Define `RepoSource` and `OutputSink` ABCs
2. Implement `FileSystemSource`
3. Implement `StreamSink`
4. Refactor `serialize()` to use adapters
5. Ensure backward compatibility

### Phase 4: Testing & Documentation (Week 4)

1. Create `budgeting` test vector category
2. Add integration tests
3. Update README and CHANGELOG
4. Update CLAUDE.md generation

---

## 6. Decisions (AI Studio Review)

All open questions have been resolved through Multi-AI consensus (Claude.ai + AI Studio/Gemini).

### Q1: Shorthand Notation → **DECIDED: YES**

**Decision:** Support `k`/`K` (thousands) and `m`/`M` (millions) suffixes.

**Rationale:** Standard CLI UX pattern, reduces typos in large numbers.

```bash
pm_encoder . --token-budget 100k   # ✓ Supported
pm_encoder . --token-budget 2M     # ✓ Supported
```

### Q2: Default Fallback Priority → **DECIDED: 50**

**Decision:** Default fallback priority is **50** (neutral center).

**Rationale:** "Principle of Least Surprise" - users expect files included by default to have equal standing. 50 allows explicit up (100) or down (10) without breaking expectations.

### Q3: tiktoken Dependency → **DECIDED: Optional with Fallback**

**Decision:** `try: import tiktoken` with `len(text) / 4` fallback.

**Rationale:** Preserves Zero Dependency philosophy while enabling accurate counting when available. Debug warning shown when using heuristic.

### Q4: Budget Report Format → **DECIDED: stderr**

**Decision:** Budget report goes to **stderr**.

**Rationale:** Unix philosophy - stdout=data, stderr=diagnostics. Enables clean piping: `pm_encoder ... | llm`

### Q5: Priority Range → **DECIDED: Arbitrary Integers**

**Decision:** Allow arbitrary signed integers (-9999 to 9999+). Document 0-100 as "standard range" in help text.

**Rationale:** Supports multiple mental models (percentages, tiered systems, negative deprioritization). Maximum flexibility for power users.

### Q6: Hybrid Strategy → **DECIDED: Added**

**Decision:** Add `--budget-strategy hybrid` as third option.

**Logic:**
1. If file > 10% of budget → auto-truncate to structure mode
2. If still too big → drop
3. Prevents one massive file from consuming entire budget

**Rationale:** Balanced approach between `drop` (clean context) and `truncate` (max coverage).

### Q7: Meta-Header Budget Info → **DECIDED: Required**

**Decision:** Include budget summary in `.pm_encoder_meta`.

**Format:** `Budget: 50000 tokens (Heuristic) | Used: 49500 | Cut: 12 files`

**Rationale:** LLM needs to know it's seeing a partial view to avoid hallucinating about missing files.

---

## 7. Success Criteria

### Functional

- [ ] Priority groups correctly sort files by priority
- [ ] Token budget drops lowest priority files first
- [ ] Backward compatible with v1.6.0 configs
- [ ] Budget report accurately reflects token usage

### Performance

- [ ] Token estimation < 10ms per file (average)
- [ ] No regression in streaming mode (when budget disabled)
- [ ] Memory usage < 2x for budgeting mode

### Quality

- [ ] Test coverage > 95% for new code
- [ ] All existing tests pass
- [ ] Documentation updated

---

## Appendix A: Full Schema Reference

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "pm_encoder Configuration v1.7.0",
  "type": "object",
  "properties": {
    "ignore_patterns": {
      "type": "array",
      "items": {"type": "string"},
      "description": "Glob patterns for files/directories to ignore"
    },
    "include_patterns": {
      "type": "array",
      "items": {"type": "string"},
      "description": "Glob patterns for files to include"
    },
    "lenses": {
      "type": "object",
      "additionalProperties": {
        "type": "object",
        "properties": {
          "description": {"type": "string"},
          "groups": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": {
                "name": {"type": "string"},
                "pattern": {"type": "string"},
                "priority": {"type": "integer"},
                "truncate_mode": {"enum": ["full", "structure", "smart", "simple"]},
                "truncate": {"type": "integer", "minimum": 0}
              },
              "required": ["pattern"]
            }
          },
          "fallback": {
            "type": "object",
            "properties": {
              "priority": {"type": "integer", "minimum": 0, "maximum": 100},
              "truncate_mode": {"enum": ["full", "structure", "smart", "simple"]},
              "truncate": {"type": "integer", "minimum": 0}
            }
          },
          "include": {"type": "array", "items": {"type": "string"}},
          "exclude": {"type": "array", "items": {"type": "string"}},
          "truncate": {"type": "integer", "minimum": 0},
          "truncate_mode": {"enum": ["full", "structure", "smart", "simple"]},
          "sort_by": {"enum": ["name", "mtime", "ctime", "priority"]},
          "sort_order": {"enum": ["asc", "desc"]}
        }
      }
    }
  }
}
```

---

**Document Status:** Approved
**Review Date:** 2025-12-17
**Reviewers:** Claude.ai (Architect), AI Studio/Gemini (Performance Analyst)
**Owner:** pm_encoder Multi-AI Development Team
**Next Step:** Implementation Phase 1 (Priority Groups)

