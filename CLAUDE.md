# pm_encoder

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

pm_encoder - Context automatically generated by pm_encoder

## Quick Start

This is the project context serialized using the `architecture` lens for optimal AI understanding.

## Project Structure

```
++++++++++ .pm_encoder_meta ++++++++++
Context generated with lens: "architecture"
Focus: High-level structure, interfaces, configuration

Implementation details truncated using structure mode
Output shows only:
  - Import/export statements
  - Class and function signatures
  - Type definitions and interfaces
  - Module-level documentation

Generated: 2025-12-13T21:23:01.799379
pm_encoder version: 1.3.1
---------- .pm_encoder_meta aea9b19d19954be688e75e82a4a834b8 .pm_encoder_meta ----------
++++++++++ .github/ISSUE_TEMPLATE/bug_report.md ++++++++++
---
name: Bug report
about: Create a report to help us improve pm_encoder
title: '[BUG] '
labels: bug
assignees: ''
---

## Bug Description
A clear and concise description of the bug.

## Environment
- **pm_encoder version**: (run `./pm_encoder.py --version`)
- **Python version**: (run `python3 --version`)
- **Operating System**: (e.g., Ubuntu 22.04, macOS 14, Windows 11)

## Steps to Reproduce
1. Run command: `./pm_encoder.py ...`
2. With config file: (if applicable, paste your `.pm_encoder_config.json`)
3. ...

## Expected Behavior
What you expected to happen.

## Actual Behavior
What actually happened.

## Error Output
```
Paste any error messages or unexpected output here
```

## Sample Files (if applicable)
If the bug involves specific file types or content, describe them or provide minimal examples.

## Additional Context
Add any other context about the problem here.

## Checklist
- [ ] I have searched existing issues to ensure this is not a duplicate
- [ ] I have verified this bug exists in the latest version
- [ ] I have included all relevant environment information
---------- .github/ISSUE_TEMPLATE/bug_report.md 7e60529dcbb4595523c9e4efb69aa56d .github/ISSUE_TEMPLATE/bug_report.md ----------
++++++++++ .github/workflows/quality.yml ++++++++++
name: Quality Assurance

on:
  push:
    branches: [ main, develop, claude/* ]
  pull_request:
    branches: [ main, develop ]

jobs:
  test:
    name: Test Suite (Python ${{ matrix.python-version }})
    runs-on: ubuntu-latest

    strategy:
      matrix:
        python-version: ["3.6", "3.7", "3.8", "3.9", "3.10", "3.11", "3.12"]

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install coverage

    - name: Run tests
      run: |
        python -m unittest discover -s tests -p 'test_*.py' -v

    - name: Check version output
      run: |
        ./pm_encoder.py --version

    - name: Test self-serialization
      run: |
        ./pm_encoder.py . -o /tmp/test_output.txt
        head -1 /tmp/test_output.txt | grep -q '++++++++++' || exit 1

  coverage:
    name: Coverage Analysis
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install coverage

    - name: Run coverage
      run: |
        python -m coverage run -m unittest discover -s tests -p 'test_*.py'
        python -m coverage report -m
        python -m coverage html

    - name: Check coverage threshold (98%)
      run: |
        python -m coverage report --fail-under=98 || echo "Warning: Coverage below 98% threshold"

    - name: Upload coverage artifacts
      uses: actions/upload-artifact@v3
      with:
        name: coverage-report
        path: htmlcov/

  lint:
    name: Code Quality Checks
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"

    - name: Python syntax check
      run: |
        python -m py_compile pm_encoder.py
        python -m py_compile tests/test_pm_encoder.py

    - name: Check for TODOs in critical files
      run: |
        ! grep -n "TODO\|FIXME" pm_encoder.py | grep -v "# TODO:" || echo "TODOs found (non-blocking)"

  integration:
    name: Integration Tests
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"

    - name: Test architecture lens
      run: |
        ./pm_encoder.py . --lens architecture --truncate 100 -o /tmp/arch.txt
        grep -q "\.pm_encoder_meta" /tmp/arch.txt || exit 1

    - name: Test structure mode
      run: |
        ./pm_encoder.py . --truncate-mode structure -o /tmp/struct.txt
        grep -q "STRUCTURE MODE" /tmp/struct.txt || exit 1

    - name: Test Rust support
      run: |
        mkdir -p /tmp/rust_test
        echo 'fn main() { println!("test"); }' > /tmp/rust_test/main.rs
        ./pm_encoder.py /tmp/rust_test --truncate-mode structure -o /tmp/rust_output.txt
        grep -q "fn main" /tmp/rust_output.txt || exit 1

  performance:
    name: Performance Benchmarks
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"

    - name: Benchmark self-serialization
      run: |
        time timeout 10s ./pm_encoder.py . -o /tmp/benchmark.txt || exit 1
        echo "Self-serialization completed within timeout"

    - name: Check output size
      run: |
        OUTPUT_SIZE=$(wc -l < /tmp/benchmark.txt)
        echo "Output lines: $OUTPUT_SIZE"
        [ "$OUTPUT_SIZE" -gt 0 ] || exit 1
---------- .github/workflows/quality.yml 6d03bc92e3673b41d0cc9cad83371074 .github/workflows/quality.yml ----------
++++++++++ .github/workflows/test.yml ++++++++++
name: Test

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.8', '3.9', '3.11', '3.12']

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Verify version flag
        run: |
          chmod +x pm_encoder.py
          ./pm_encoder.py --version

      - name: Run self-serialization test
        run: |
          ./pm_encoder.py . -o /tmp/self_serialize_output.txt
          echo "Self-serialization completed successfully"

      - name: Verify output format
        run: |
          # Check that output starts with Plus/Minus format header
          head -1 /tmp/self_serialize_output.txt | grep -q "^++++++++++"
          echo "Output format verified"

      - name: Verify checksum presence
        run: |
          # Check that output contains checksum lines
          grep -q "^----------.*[a-f0-9]\{32\}.*----------$" /tmp/self_serialize_output.txt
          echo "Checksum format verified"
---------- .github/workflows/test.yml c0eb6c3db1850f10f7b63c67ad140289 .github/workflows/test.yml ----------
++++++++++ .pm_encoder_config.json ++++++++++
{
  "ignore_patterns": [
    ".git",
    ".idea",
    ".vscode",
    "target",
    "build",
    "dist",
    ".venv",
    "__pycache__",
    "*.pyc",
    "*.log",
    "*.swp",
    "*.bak",
    "*.tmp",
    "node_modules",
    "recordings.db*",
    "compare_output"
  ],
  "include_patterns": [
    "*.rs",
    "*.toml",
    "*.md",
    "*.py",
    "*.sh",
    "*.json",
    "*.xml",
    "*.txt",
    "Dockerfile",
    "LICENSE"
  ],
  "lenses": {
    "docs": {
      "description": "Documentation and guides for pm_encoder",
      "include": [
        "*.md",
        "CHANGELOG.md",
        "LICENSE",
        "examples/**"
      ],
      "exclude": [
        "LLM/**"
      ],
      "truncate_mode": "smart",
      "truncate": 500,
      "sort_by": "name",
      "sort_order": "asc"
    },
    "core": {
      "description": "Core implementation (pm_encoder.py only)",
      "include": [
        "pm_encoder.py"
      ],
      "exclude": [],
      "truncate_mode": "structure",
      "sort_by": "name"
    }
  }
}
---------- .pm_encoder_config.json 8065c525e844432334394dde37497110 .pm_encoder_config.json ----------
++++++++++ CHANGELOG.md ++++++++++
# Changelog

All notable changes to pm_encoder will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.1.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [1.2.2] - 2025-12-12

### Added - Native Rust Support
- **RustAnalyzer** (pm_encoder.py:650-778): Full native support for Rust source files
  - `analyze_lines()`: Detects structs, traits, impls, functions (sync/async), use statements, mod declarations, markers
  - `get_structure_ranges()`: Structure mode support - extracts signatures while removing implementation details
  - Entry point detection: Identifies `fn main()` as application entry point
  - Categorization: Distinguishes between application, library, and test code
- **Architecture lens**: Updated to include `*.rs` files in structure mode
- **Language support matrix**: Rust now joins Python, JavaScript/TypeScript, Shell as fully supported languages with structure mode

### Added - Technical Blueprint
- **docs/BLUEPRINT.md**: Technical vision document for pm_encoder evolution
  - Outlines transition from "Context Compression" to "AI Collaboration Infrastructure"
  - Defines Core Philosophy: Context as the new Compilation, Intent beats Syntax
  - Roadmap for v1.3.0 (Streaming + Interactive) and v1.4.0 (Declarative JSON Patterns)
  - Establishes Python as the Reference Implementation

### Added - Test Coverage
- **test_rust_structure**: Comprehensive test for Rust structure extraction (tests/test_pm_encoder.py:219-296)
  - Verifies use statements, struct definitions, impl blocks, trait definitions
  - Validates async fn signatures, main function detection
  - Confirms implementation details are removed (struct fields, function bodies)
- **Test Results**: âœ… 10/10 tests passing (up from 9)

### Changed
- **Version bumped** to 1.2.2
- **Built-in analyzers**: Now 7 total (added Rust to Python, JS/TS, Shell, Markdown, JSON, YAML)

### Technical Details
- Zero new external dependencies (still 100% Python standard library)
- Rust patterns use regex (no proc-macro parsing required)
- Structure mode preserves: `use`, `mod`, `struct`, `fn`, `async fn`, `trait`, `impl`
- Smart truncation for Rust: Prioritizes type definitions, main function, and exports

### Use Cases Unlocked
- **Rust projects**: Architecture views for Rust codebases (cargo projects, embedded systems)
- **Polyglot repos**: Seamless mixed Python/Rust/TypeScript project serialization
- **Systems programming**: Share low-level code context with LLMs effectively

## [1.2.1] - 2025-12-12

### Fixed - Critical Logic Bugs
- **Structure mode triggering**: Fixed bug where structure mode wouldn't trigger unless `--truncate N` was specified
  - Changed condition from `if truncate_lines > 0:` to `if truncate_lines > 0 or truncate_mode == 'structure':`
  - Now `--truncate-mode structure` works correctly without requiring numeric line limit
  - **Impact**: Users can now use `--lens architecture` or `--truncate-mode structure` without setting `--truncate`
- **Lens precedence mapping**: Fixed bug in `apply_lens()` where lens "include"/"exclude" keys weren't properly mapped to "include_patterns"/"ignore_patterns"
  - Lens "include" now correctly overrides base "include_patterns"
  - Lens "exclude" now correctly extends base "ignore_patterns"
  - **Impact**: Custom and built-in lenses now work as documented

### Added - Comprehensive Test Suite
- **tests/test_pm_encoder.py**: 9 comprehensive tests using standard library only (unittest, tempfile, shutil)
  - `test_structure_mode_trigger`: Verifies structure mode works with truncate=0 (the bug fix)
  - `test_lens_precedence`: Verifies layered precedence (CLI > Lens > Config > Defaults)
  - `test_python_structure`: Verifies Python signature extraction (keeps signatures, removes bodies)
  - `test_js_structure`: Verifies JavaScript structure extraction
  - `test_json_fallback`: Verifies JSON files fall back to smart mode (no structure support)
  - `test_meta_injection`: Verifies .pm_encoder_meta file is injected when using lenses
  - `test_ignore_patterns`: Verifies .git and __pycache__ are properly ignored
  - `test_all_lenses_exist`: Verifies all 4 built-in lenses exist
  - `test_architecture_lens_has_safety_limit`: Verifies architecture lens has truncate: 2000 safety limit
- **Test Results**: âœ… 9/9 tests passing, 0 failures, 0 errors

### Changed
- **architecture lens**: Added `truncate: 2000` safety limit for non-code files
- **Version bumped** to 1.2.1

### Why This Release Matters
v1.2.1 fixes critical bugs that prevented structure mode from working as designed in v1.2.0. The comprehensive test suite ensures these bugs won't regress and validates all core functionality. This is a **recommended upgrade** for all v1.2.0 users.

## [1.2.0] - 2025-12-12

### Added - Context Lenses System
- **Context Lenses**: Pre-configured serialization profiles for specific use cases
- **Built-in lenses** (4 total):
  - `architecture`: High-level structure with structure mode (signatures only)
  - `debug`: Recent changes with full files, sorted by mtime DESC
  - `security`: Security-critical code with smart truncation (300 lines)
  - `onboarding`: Balanced overview for new developers (400 lines)
- **LensManager class**: Handles lens application with layered precedence
- **Lens manifest printing**: Transparent stderr output showing active lens configuration
- **Meta-aware output**: Injects `.pm_encoder_meta` file to document lens and filtering for LLMs
- **Custom lens support**: Define project-specific lenses in `.pm_encoder_config.json`

### Added - Structure Mode Truncation
- **Structure mode**: New truncation mode showing only signatures (imports, class/function declarations)
- **Language-specific structure extraction** for:
  - Python: Imports, class definitions, function signatures, decorators
  - JavaScript/TypeScript: Imports/exports, classes, functions, arrow functions, interfaces
  - Shell: Shebang, function declarations, source statements
- **Graceful fallback**: Unsupported languages automatically fall back to smart mode
- **Structure mode markers**: Clear indicators showing what was included/excluded

### Added - CLI Options
- `--lens NAME`: Apply a context lens (architecture|debug|security|onboarding|custom)
- `--truncate-mode structure`: New truncation mode option (in addition to simple|smart)

### Changed
- **load_config()**: Now returns custom lenses from config file (3rd return value)
- **serialize()**: Accepts optional `lens_manager` parameter for lens integration
- **Layered precedence**: CLI flags > Lens settings > Config file > Defaults
- **Version bumped** to 1.2.0

### Performance
- **Optimized analyzers**: All analyzers now use `analyze_lines()` to eliminate redundant string splitting
- **~50% reduction** in string allocation overhead from v1.1.0

### Documentation
- **README.md**: Added comprehensive Context Lenses section with examples
- **TUTORIAL.md**: Added 4 new lens examples (Examples 8-11) and Workflow 6
- **CHANGELOG.md**: This entry

### Use Cases Unlocked
- **Architecture exploration**: Get codebase overview without implementation details (80%+ reduction)
- **Rapid debugging**: Immediately focus on recently modified files
- **Security audits**: Automated filtering for security-relevant code
- **Team onboarding**: Pre-configured balanced context for new developers
- **Custom workflows**: Define project-specific lenses for common tasks

### Technical Details
- Zero new external dependencies (still 100% standard library)
- Python 3.6+ compatibility maintained
- Backward compatible: existing v1.1.0 workflows unchanged
- Lens system purely additive (no breaking changes)

## [1.1.0] - 2025-12-12

### Added - Intelligent Truncation System
- **Language-aware truncation**: Smart truncation that understands code structure across multiple languages
- **Built-in language analyzers** for:
  - Python (`.py`, `.pyw`): Classes, functions, imports, `__main__` blocks, docstrings, markers
  - JavaScript/TypeScript (`.js`, `.jsx`, `.ts`, `.tsx`, `.mjs`, `.cjs`): Classes, functions, imports, exports, JSDoc
  - Shell (`.sh`, `.bash`, `.zsh`, `.fish`): Functions, sourced files, shebang detection
  - Markdown (`.md`, `.markdown`): Headers, code blocks, links, structure-aware truncation
  - JSON (`.json`): Structural analysis with key/depth detection
  - YAML (`.yaml`, `.yml`): Key structure preservation
- **Truncation modes**:
  - `simple`: Fast truncation keeping first N lines
  - `smart`: Language-aware truncation preserving critical code sections
- **Detailed truncation summaries** showing:
  - Language and file category (application/library/test/config)
  - Detected classes, functions, imports
  - Entry points and markers (TODO/FIXME)
  - Instructions for retrieving full content
- **Truncation statistics** with `--truncate-stats` flag showing:
  - Files analyzed vs truncated
  - Line and size reduction percentages
  - Per-language breakdown
  - Estimated token count reduction

### Added - Plugin System
- **Extensible language analyzer architecture** allowing community contributions
- **Plugin template generator**: `--create-plugin LANGUAGE` command
- **AI prompt generator**: `--plugin-prompt LANGUAGE` for getting AI assistance
- **Plugin loading system** from `~/.pm_encoder/plugins/` or custom directory
- **Example Rust analyzer** in `examples/plugins/rust_analyzer.py`
- **Comprehensive plugin development guide** (`PLUGIN_GUIDE.md`)

### Added - CLI Options
- `--truncate N`: Truncate files exceeding N lines (default: 0 = no truncation)
- `--truncate-mode {simple|smart}`: Choose truncation strategy (default: simple)
- `--truncate-summary`: Include analysis summary in truncation markers (default: true)
- `--no-truncate-summary`: Disable truncation summaries
- `--truncate-exclude PATTERN [PATTERN ...]`: Exclude files from truncation by glob pattern
- `--truncate-stats`: Show detailed truncation statistics report
- `--language-plugins DIR`: Specify custom language analyzer plugins directory
- `--create-plugin LANGUAGE`: Generate plugin template for a language
- `--plugin-prompt LANGUAGE`: Generate AI prompt for creating a plugin

### Changed
- **Enhanced Plus/Minus format**: Truncated files show `[TRUNCATED: N lines]` in headers and `[TRUNCATED:Nâ†’M]` in footers
- **Version bumped** to 1.1.0
- **Performance optimized**: Language analysis adds <100ms overhead per file

### Documentation
- **README.md**: Added language support matrix and truncation examples
- **TUTORIAL.md**: New "Token Optimization" section with practical truncation workflows
- **PLUGIN_GUIDE.md**: Complete guide for creating custom language analyzers
- **Examples**: Added `examples/plugins/` with Rust analyzer sample

### Technical Details
- Zero new external dependencies (still 100% standard library)
- Python 3.6+ compatibility maintained
- Backward compatible: existing workflows unchanged without truncation flags
- Regex-based analyzers (no AST parsing) for speed and portability

### Use Cases Unlocked
- **LLM context optimization**: Reduce large codebases to fit token limits
- **Cost reduction**: Lower API costs for token-based LLM services
- **Faster processing**: Smaller context = faster LLM responses
- **Better code understanding**: Summaries help AI grasp project structure
- **Multi-language projects**: Single tool handles polyglot repositories

## [1.0.0] - 2025-12-12

### Added
- Initial public release of pm_encoder
- Plus/Minus format serialization with MD5 checksums
- JSON configuration file support (`.pm_encoder_config.json`)
- CLI flags for filtering: `--include`, `--exclude`
- Sorting options: `--sort-by` (name, mtime, ctime) and `--sort-order` (asc, desc)
- Binary file detection (null-byte heuristic)
- Large file skipping (>5MB)
- UTF-8 encoding with latin-1 fallback
- POSIX-style paths in output for cross-platform compatibility
- Directory pruning for efficient traversal
- Standard output by default with `-o` option for file output

### Technical Details
- Python 3.6+ compatibility
- Zero external dependencies (standard library only)
- Single-file distribution (`pm_encoder.py`)

[1.0.0]: https://github.com/alanbld/pm_encoder/releases/tag/v1.0.0
---------- CHANGELOG.md 1e6fb9e7892856aef45f93ae523354d9 CHANGELOG.md ----------
++++++++++ CLAUDE.md ++++++++++
# pm_encoder

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

pm_encoder - Context automatically generated by pm_encoder

## Quick Start

This is the project context serialized using the `debug` lens for optimal AI understanding.

## Project Structure

```
++++++++++ .pm_encoder_meta ++++++++++
Context generated with lens: "debug"
Focus: Recent changes for debugging

Full file contents included (no truncation)

Generated: 2025-12-13T21:22:47.674850
pm_encoder version: 1.3.1
---------- .pm_encoder_meta 56716a50fccda8f21a12437f94f7fb4b .pm_encoder_meta ----------
++++++++++ CLAUDE.md [TRUNCATED: 6166 lines] ++++++++++


======================================================================
TRUNCATED at line 0/6166 (100% reduction)
Language: Markdown
Category: documentation
Key imports: https://keepachangelog.com/en/1.1.0/, https://semver.org/spec/v2.0.0.html, #overview, #quick-start, #plugin-architecture, #creating-a-plugin, #testing-your-plugin, #contributing-plugins, ...
Entry points: H1: pm_encoder, H2: Project Overview, H2: Quick Start, H2: Project Structure, H2: Bug Description

To get full content: --include "CLAUDE.md" --truncate 0
======================================================================
---------- CLAUDE.md [TRUNCATED:6166â†’11] 9f4d2397f76347b127ddf2a6c9170825 CLAUDE.md ----------
++++++++++ pm_encoder.py [TRUNCATED: 1884 lines] ++++++++++
"""
"""
import argparse
import hashlib
import json
import os
import re
import sys
import tempfile
from pathlib import Path
from fnmatch import fnmatch
from typing import Optional, Tuple, List, Dict, Any
from collections import defaultdict
import signal
class LanguageAnalyzer:
    def analyze(self, content: str, file_path: Path) -> Dict[str, Any]:
    def analyze_lines(self, lines: List[str], file_path: Path) -> Dict[str, Any]:
    def get_structure_ranges(self, lines: List[str]) -> List[Tuple[int, int]]:
    def get_truncate_ranges(self, content: str, max_lines: int) -> Tuple[List[Tuple[int, int]], Dict[str, Any]]:
class PythonAnalyzer(LanguageAnalyzer):
    def analyze_lines(self, lines: List[str], file_path: Path) -> Dict[str, Any]:
    def get_structure_ranges(self, lines: List[str]) -> List[Tuple[int, int]]:
    def _merge_consecutive_ranges(self, ranges: List[Tuple[int, int]]) -> List[Tuple[int, int]]:
    def get_truncate_ranges(self, content: str, max_lines: int) -> Tuple[List[Tuple[int, int]], Dict[str, Any]]:
class JavaScriptAnalyzer(LanguageAnalyzer):
    def analyze_lines(self, lines: List[str], file_path: Path) -> Dict[str, Any]:
    def get_structure_ranges(self, lines: List[str]) -> List[Tuple[int, int]]:
    def _merge_consecutive_ranges(self, ranges: List[Tuple[int, int]]) -> List[Tuple[int, int]]:
    def get_truncate_ranges(self, content: str, max_lines: int) -> Tuple[List[Tuple[int, int]], Dict[str, Any]]:
class ShellAnalyzer(LanguageAnalyzer):
    def analyze_lines(self, lines: List[str], file_path: Path) -> Dict[str, Any]:
    def get_structure_ranges(self, lines: List[str]) -> List[Tuple[int, int]]:
class MarkdownAnalyzer(LanguageAnalyzer):
    def analyze_lines(self, lines: List[str], file_path: Path) -> Dict[str, Any]:
    def get_truncate_ranges(self, content: str, max_lines: int) -> Tuple[List[Tuple[int, int]], Dict[str, Any]]:
class JSONAnalyzer(LanguageAnalyzer):
    def analyze_lines(self, lines: List[str], file_path: Path) -> Dict[str, Any]:
            def count_keys(obj, depth=0, max_depth=0):
    def get_truncate_ranges(self, content: str, max_lines: int) -> Tuple[List[Tuple[int, int]], Dict[str, Any]]:
class YAMLAnalyzer(LanguageAnalyzer):
    def analyze_lines(self, lines: List[str], file_path: Path) -> Dict[str, Any]:
class RustAnalyzer(LanguageAnalyzer):
    def analyze_lines(self, lines: List[str], file_path: Path) -> Dict[str, Any]:
    def get_structure_ranges(self, lines: List[str]) -> List[Tuple[int, int]]:
    def _merge_consecutive_ranges(self, ranges: List[Tuple[int, int]]) -> List[Tuple[int, int]]:
class LanguageAnalyzerRegistry:
    def __init__(self):
    def _register_builtin(self):
    def load_plugins(self, plugin_dir: Optional[Path] = None):
    def get_analyzer(self, file_path: Path) -> LanguageAnalyzer:
    def get_supported_languages(self) -> List[str]:
class TruncationStats:
    def __init__(self):
    def add_file(self, language: str, original_lines: int, final_lines: int, was_truncated: bool):
    def print_report(self):
    def _reduction_pct(self, original, final):
def truncate_content(
class LensManager:
    def __init__(self, config_lenses: Dict = None):
    def apply_lens(self, lens_name: str, base_config: Dict) -> Dict:
    def print_manifest(self):
    def get_meta_content(self) -> str:
def load_config(config_path: Optional[Path]) -> Tuple[List[str], List[str], Dict[str, Dict]]:
def is_binary(file_path: Path) -> bool:
def read_file_content(file_path: Path) -> Optional[str]:
def write_pm_format(output_stream, relative_path: Path, content: str, was_truncated: bool = False, original_lines: int = 0):
def serialize(
    def collect_files(current_dir: Path):
def create_plugin_template(language_name: str):
import re
from pathlib import Path
from typing import Dict, List, Tuple, Any
class LanguageAnalyzer:
    def analyze(self, content: str, file_path: Path) -> Dict[str, Any]:
    def get_truncate_ranges(self, content: str, max_lines: int) -> Tuple[List[Tuple[int, int]], Dict[str, Any]]:
def create_plugin_prompt(language_name: str):
class LanguageAnalyzer:
    def analyze(self, content: str, file_path: Path) -> Dict[str, Any]:
    def get_truncate_ranges(self, content: str, max_lines: int) -> Tuple[List[Tuple[int, int]], Dict[str, Any]]:
def init_prompt(project_root: Path, lens_name: str = "architecture"):
def main():

======================================================================
STRUCTURE MODE: Showing only signatures (81/1884 lines)
Language: Python

Included: imports, class/function signatures, type definitions
Excluded: function bodies, implementation details

To get full content: --include "pm_encoder.py" --truncate 0
======================================================================
---------- pm_encoder.py [TRUNCATED:1884â†’91] c41a68dabe6f61e2ebf3213c3e5b7051 pm_encoder.py ----------
++++++++++ test_vectors/large_file_skip.json [TRUNCATED: 40 lines] ++++++++++


======================================================================
TRUNCATED at line 0/40 (100% reduction)
Language: JSON
Category: config
Entry points: name, description, version, input, expected

To get full content: --include "test_vectors/large_file_skip.json" --truncate 0
======================================================================
---------- test_vectors/large_file_skip.json [TRUNCATED:40â†’10] d692d11aa18f3a52e792ea3fbf390995 test_vectors/large_file_skip.json ----------
++++++++++ test_vectors/basic_serialization.json [TRUNCATED: 32 lines] ++++++++++


======================================================================
TRUNCATED at line 0/32 (100% reduction)
Language: JSON
Category: config
Entry points: name, description, version, input, expected

To get full content: --include "test_vectors/basic_serialization.json" --truncate 0
======================================================================
---------- test_vectors/basic_serialization.json [TRUNCATED:32â†’10] d6420d88f7f7b719f00defbd264b3b25 test_vectors/basic_serialization.json ----------
++++++++++ test_vectors/binary_detection.json [TRUNCATED: 35 lines] ++++++++++


======================================================================
TRUNCATED at line 0/35 (100% reduction)
Language: JSON
Category: config
Entry points: name, description, version, input, expected

To get full content: --include "test_vectors/binary_detection.json" --truncate 0
======================================================================
---------- test_vectors/binary_detection.json [TRUNCATED:35â†’10] bd556e25fd077e065bfc2a15eeddca2e test_vectors/binary_detection.json ----------
++++++++++ tests/generate_vectors.py [TRUNCATED: 366 lines] ++++++++++
"""
import json
import os
import sys
import tempfile
import hashlib
from pathlib import Path
import pm_encoder
def generate_basic_serialization_vector():
        import subprocess
def generate_binary_detection_vector():
            import subprocess
def generate_large_file_skip_vector():
            import subprocess
def save_vector(vector, filename):
def main():

======================================================================
STRUCTURE MODE: Showing only signatures (16/366 lines)
Language: Python

Included: imports, class/function signatures, type definitions
Excluded: function bodies, implementation details

To get full content: --include "tests/generate_vectors.py" --truncate 0
======================================================================
---------- tests/generate_vectors.py [TRUNCATED:366â†’26] 80d195a67289b3d7c3885fedf96fcb29 tests/generate_vectors.py ----------
++++++++++ tests/fixtures/rust/sample.rs [TRUNCATED: 36 lines] ++++++++++
use std::collections::HashMap;
pub struct Config {
impl Config {
    pub fn new(name: &str) -> Self {
    pub fn add_value(&mut self, key: String, value: i32) {
mod tests {
    use super::*;
    fn test_config_creation() {

======================================================================
STRUCTURE MODE: Showing only signatures (8/36 lines)
Language: Rust

Included: imports, class/function signatures, type definitions
Excluded: function bodies, implementation details

To get full content: --include "tests/fixtures/rust/sample.rs" --truncate 0
======================================================================
---------- tests/fixtures/rust/sample.rs [TRUNCATED:36â†’18] 30689946bdd64fee0a35b2a90b5c4aab tests/fixtures/rust/sample.rs ----------
++++++++++ docs/KNOWLEDGE_BASE.md [TRUNCATED: 234 lines] ++++++++++


======================================================================
TRUNCATED at line 0/234 (100% reduction)
Language: Markdown
Category: documentation
Entry points: H1: pm_encoder Knowledge Base, H2: Single Source of Truth for AI Context, H2: Quick Status, H2: What pm_encoder Does, H2: Architecture: The Twins

To get full content: --include "docs/KNOWLEDGE_BASE.md" --truncate 0
======================================================================
---------- docs/KNOWLEDGE_BASE.md [TRUNCATED:234â†’10] 5aa829a93516d92cb7ec9deb20ac1756 docs/KNOWLEDGE_BASE.md ----------
++++++++++ README.md [TRUNCATED: 472 lines] ++++++++++


======================================================================
TRUNCATED at line 0/472 (100% reduction)
Language: Markdown
Category: documentation
Key imports: https://img.shields.io/badge/License-MIT-yellow.svg, https://img.shields.io/badge/python-3.6+-blue.svg, docs/KNOWLEDGE_BASE.md, docs/THE_TWINS_ARCHITECTURE.md, docs/RUST_GROWTH_STRATEGY.md, docs/BLUEPRINT.md, docs/THE_TURING_AUDIT.md, TESTING.md, ...
Entry points: H1: Project Encoder (`pm_encoder.py`), H2: Features, H2: Project Structure, H3: Python Implementation (Current Production), H3: Rust Implementation (v2.0 Foundation)

To get full content: --include "README.md" --truncate 0
======================================================================
---------- README.md [TRUNCATED:472â†’11] 053c045075b8d8c4d7aa29fdb452970c README.md ----------
++++++++++ docs/RUST_GROWTH_STRATEGY.md [TRUNCATED: 639 lines] ++++++++++


======================================================================
TRUNCATED at line 0/639 (100% reduction)
Language: Markdown
Category: documentation
Entry points: H1: Rust Growth Strategy, H2: Fast-Track to Feature Parity with Python v1.3.1, H2: Executive Summary, H2: Feature Gap Analysis, H3: Python v1.3.1 Feature Matrix

To get full content: --include "docs/RUST_GROWTH_STRATEGY.md" --truncate 0
======================================================================
---------- docs/RUST_GROWTH_STRATEGY.md [TRUNCATED:639â†’10] 181143836bdee7ba16df76e9f7cefcb5 docs/RUST_GROWTH_STRATEGY.md ----------
++++++++++ docs/THE_TWINS_ARCHITECTURE.md [TRUNCATED: 348 lines] ++++++++++


======================================================================
TRUNCATED at line 0/348 (100% reduction)
Language: Markdown
Category: documentation
Entry points: H1: The Twins Architecture, H2: Python + Rust: Growing Together, H2: The Decision, H3: Why Two Engines?, H3: Why Open Source from Day 1?

To get full content: --include "docs/THE_TWINS_ARCHITECTURE.md" --truncate 0
======================================================================
---------- docs/THE_TWINS_ARCHITECTURE.md [TRUNCATED:348â†’10] 04c9a65c6893bc40cb60b881a456fe53 docs/THE_TWINS_ARCHITECTURE.md ----------
++++++++++ test_vectors/python_analyzer.json [TRUNCATED: 24 lines] ++++++++++


======================================================================
TRUNCATED at line 0/24 (100% reduction)
Language: JSON
Category: config
Entry points: name, description, version, input, expected

To get full content: --include "test_vectors/python_analyzer.json" --truncate 0
======================================================================
---------- test_vectors/python_analyzer.json [TRUNCATED:24â†’10] 834784cc779f58a82b9b92ea02a9ff4f test_vectors/python_analyzer.json ----------
++++++++++ test_vectors/README.md [TRUNCATED: 125 lines] ++++++++++


======================================================================
TRUNCATED at line 0/125 (100% reduction)
Language: Markdown
Category: documentation
Entry points: H1: Test Vectors - The Contract, H2: Purpose, H2: Format, H2: Structure, H2: Usage

To get full content: --include "test_vectors/README.md" --truncate 0
======================================================================
---------- test_vectors/README.md [TRUNCATED:125â†’10] a9acb04d195efb1b4e25e3b17242ec82 test_vectors/README.md ----------
++++++++++ rust/README.md [TRUNCATED: 143 lines] ++++++++++


======================================================================
TRUNCATED at line 0/143 (100% reduction)
Language: Markdown
Category: documentation
Entry points: H1: pm_encoder (Rust Engine), H2: Architecture: Library-First Pattern, H3: The Brain: `lib.rs`, H3: The Interface: `bin/main.rs`, H2: Future Bindings

To get full content: --include "rust/README.md" --truncate 0
======================================================================
---------- rust/README.md [TRUNCATED:143â†’10] 0df8bbbf52e6a1ca37405b09a8cc7a8a rust/README.md ----------
++++++++++ rust/src/bin/main.rs [TRUNCATED: 49 lines] ++++++++++
use pm_encoder; // Import our own library
use std::env;
fn main() {

======================================================================
STRUCTURE MODE: Showing only signatures (3/49 lines)
Language: Rust

Included: imports, class/function signatures, type definitions
Excluded: function bodies, implementation details

To get full content: --include "rust/src/bin/main.rs" --truncate 0
======================================================================
---------- rust/src/bin/main.rs [TRUNCATED:49â†’13] e322dea66384839d6f390c8706ab0c7c rust/src/bin/main.rs ----------
++++++++++ rust/src/lib.rs [TRUNCATED: 123 lines] ++++++++++
pub struct EncoderConfig {
impl Default for EncoderConfig {
    fn default() -> Self {
pub fn version() -> &'static str {
pub fn serialize_project(root: &str) -> Result<String, String> {
pub fn serialize_project_with_config(
mod tests {
    use super::*;
    fn test_version() {
    fn test_serialize_project() {
    fn test_serialize_with_config() {
    fn test_default_config() {

======================================================================
STRUCTURE MODE: Showing only signatures (12/123 lines)
Language: Rust

Included: imports, class/function signatures, type definitions
Excluded: function bodies, implementation details

To get full content: --include "rust/src/lib.rs" --truncate 0
======================================================================
---------- rust/src/lib.rs [TRUNCATED:123â†’22] 555dae7e8b6670e0c62c6ed8667211e2 rust/src/lib.rs ----------
++++++++++ rust/Cargo.toml [TRUNCATED: 18 lines] ++++++++++


======================================================================
TRUNCATED at line 0/18 (100% reduction)
Language: Unknown
Category: unknown

To get full content: --include "rust/Cargo.toml" --truncate 0
======================================================================
---------- rust/Cargo.toml [TRUNCATED:18â†’9] b3e05ba48ce3d322e80fb32b43060eff rust/Cargo.toml ----------
++++++++++ tests/test_comprehensive.py [TRUNCATED: 1663 lines] ++++++++++
"""
import unittest
import tempfile
import shutil
import json
import sys
import subprocess
from pathlib import Path
from io import StringIO
import pm_encoder
class TestAllLanguageAnalyzers(unittest.TestCase):
    def setUp(self):
    def test_python_analyzer_comprehensive(self):
    def test_javascript_analyzer_comprehensive(self):
    def test_rust_analyzer_comprehensive(self):
    def test_shell_analyzer_comprehensive(self):
    def test_markdown_analyzer_comprehensive(self):
    def test_json_analyzer_comprehensive(self):
    def test_yaml_analyzer_comprehensive(self):
    def test_python_get_truncate_ranges(self):
    def test_javascript_get_truncate_ranges(self):
    def test_shell_get_truncate_ranges(self):
    def test_markdown_get_truncate_ranges(self):
    def test_analyzer_registry_get_analyzer(self):
    def test_analyzer_registry_get_supported_languages(self):
class TestCLIComprehensive(unittest.TestCase):
    def setUp(self):
    def tearDown(self):
    def test_main_with_truncate_simple(self):
    def test_main_with_truncate_smart(self):
    def test_main_with_lens_architecture(self):
    def test_main_with_sorting_mtime_desc(self):
        import time
    def test_main_version_flag(self):
    def test_main_create_plugin(self):
    def test_main_plugin_prompt(self):
class TestEdgeCasesComprehensive(unittest.TestCase):
    def setUp(self):
    def tearDown(self):
    def test_empty_directory(self):
    def test_binary_file_skipped(self):
    def test_large_file_skipped(self):
    def test_unicode_content(self):
    def test_deeply_nested_json(self):
    def test_truncate_content_structure_mode(self):
class Test:
    def method(self):
class TestConfigurationSystem(unittest.TestCase):
    def setUp(self):
    def tearDown(self):
    def test_load_config_no_file(self):
    def test_load_config_with_file(self):
    def test_load_config_malformed_json(self):
    def test_lens_manager_custom_lens(self):
    def test_lens_manager_invalid_lens(self):
class TestPerformanceRegression(unittest.TestCase):
    def setUp(self):
    def tearDown(self):
    def test_large_number_of_files_performance(self):
        import time
class TestTruncationWithSummary(unittest.TestCase):
    def test_simple_truncation_with_summary(self):
    def test_smart_truncation_with_summary(self):
import sys
class TestClass:
    def method1(self):
    def method2(self):
def function1():
def function2():
    def test_structure_mode_without_summary(self):
class TestDirectFunctionCalls(unittest.TestCase):
    def test_create_plugin_template_direct(self):
        import sys
        from io import StringIO
    def test_create_plugin_prompt_direct(self):
        import sys
        from io import StringIO
    def test_truncation_stats_print_report(self):
        import sys
        from io import StringIO
    def test_lens_manager_print_manifest(self):
        import sys
        from io import StringIO
    def test_analyzer_registry_load_plugins(self):
    def test_json_analyzer_get_truncate_ranges(self):
    def test_shell_analyzer_get_structure_ranges(self):
class TestMainFunctionDirect(unittest.TestCase):
    def setUp(self):
    def tearDown(self):
    def test_main_basic_serialization(self):
    def test_main_with_lens_and_manifest(self):
    def test_main_with_truncation_enabled(self):
    def test_main_with_include_override(self):
    def test_main_with_exclude_addition(self):
    def test_main_with_custom_config(self):
    def test_main_with_sort_options(self):
    def test_main_with_structure_mode(self):
    def test_main_with_truncate_exclude_pattern(self):
class TestEdgeCasesForCoverage(unittest.TestCase):
    def test_truncation_summary_with_many_classes(self):
    def test_truncation_summary_with_many_functions(self):
    def test_truncation_summary_with_many_imports(self):
    def test_lens_manager_print_manifest_with_truncate_disabled(self):
        import sys
        from io import StringIO
    def test_lens_manager_print_manifest_with_exclusions(self):
        import sys
        from io import StringIO
    def test_lens_manager_get_meta_content(self):
    def test_truncation_stats_empty(self):
        import sys
        from io import StringIO
    def test_truncation_stats_reduction_pct_zero_original(self):
    def test_json_analyzer_short_content(self):
    def test_truncation_summary_with_many_entry_points(self):
def main1():
def main2():
def main3():
def main4():
def main5():
def main6():
    def test_lens_manager_print_manifest_with_structure_mode(self):
        import sys
        from io import StringIO
    def test_lens_manager_print_manifest_with_limited_truncate(self):
        import sys
        from io import StringIO
    def test_lens_manager_print_manifest_with_includes(self):
        import sys
        from io import StringIO
    def test_lens_manager_no_active_lens_manifest(self):
        import sys
        from io import StringIO
class TestAdditionalCoverage(unittest.TestCase):
    def test_base_language_analyzer_analyze(self):
    def test_base_language_analyzer_get_truncate_ranges_no_truncation(self):
    def test_unknown_file_extension(self):
    def test_json_analyzer_recursion_error(self):
    def test_truncation_summary_with_all_metadata(self):
import sys
import json
class Class1:
class Class2:
def func1():
def func2():
    def test_main_with_plugin_template_via_main(self):
        import sys
        from io import StringIO
    def test_main_with_plugin_prompt_via_main(self):
        import sys
        from io import StringIO
class TestCLIAdditional(unittest.TestCase):
    def setUp(self):
    def tearDown(self):
    def test_main_with_truncate_stats(self):
    def test_main_with_truncate_exclude(self):
    def test_main_with_no_truncate_summary(self):
    def test_main_with_exclude_flag(self):
    def test_main_with_include_flag(self):
    def test_main_missing_project_root(self):
    def test_main_invalid_project_root(self):
    def test_main_with_invalid_lens(self):
class TestErrorHandlingWithMocks(unittest.TestCase):
    def test_unicode_decode_error_latin1_fallback(self):
        from unittest.mock import patch, mock_open, MagicMock
    def test_file_read_io_error(self):
        from unittest.mock import patch, MagicMock
    def test_broken_pipe_error_handling(self):
        from unittest.mock import patch, MagicMock
        import sys
def run_tests():

======================================================================
STRUCTURE MODE: Showing only signatures (171/1663 lines)
Language: Python

Included: imports, class/function signatures, type definitions
Excluded: function bodies, implementation details

To get full content: --include "tests/test_comprehensive.py" --truncate 0
======================================================================
---------- tests/test_comprehensive.py [TRUNCATED:1663â†’181] 92dd91e3826eed194cc2acef8ebdcd17 tests/test_comprehensive.py ----------
++++++++++ docs/THE_TURING_AUDIT.md [TRUNCATED: 42 lines] ++++++++++


======================================================================
TRUNCATED at line 0/42 (100% reduction)
Language: Markdown
Category: documentation
Entry points: H1: ðŸ“œ The Turing Audit: A Multi-AI Saga, H2: Prologue: The Meta-Tool Paradox, H2: Act I: The Setup, H2: Act II: The Investigation, H2: Act III: The Reveal

To get full content: --include "docs/THE_TURING_AUDIT.md" --truncate 0
======================================================================
---------- docs/THE_TURING_AUDIT.md [TRUNCATED:42â†’10] 107181cd4c6ad16fa02105f85745f866 docs/THE_TURING_AUDIT.md ----------
++++++++++ htmlcov/status.json [TRUNCATED: 1 lines] ++++++++++


======================================================================
TRUNCATED at line 0/1 (100% reduction)
Language: JSON
Category: config
Entry points: note, format, version, globals, files

To get full content: --include "htmlcov/status.json" --truncate 0
======================================================================
---------- htmlcov/status.json [TRUNCATED:1â†’10] bdeaf6dc15c8a00206228523d6141a80 htmlcov/status.json ----------
++++++++++ tests/fixtures/json/sample.json [TRUNCATED: 27 lines] ++++++++++


======================================================================
TRUNCATED at line 0/27 (100% reduction)
Language: JSON
Category: config
Entry points: name, version, description, nested, arrays

To get full content: --include "tests/fixtures/json/sample.json" --truncate 0
======================================================================
---------- tests/fixtures/json/sample.json [TRUNCATED:27â†’10] eb08a97ac38f04f76f022ef57f39e978 tests/fixtures/json/sample.json ----------
++++++++++ tests/fixtures/markdown/sample.md [TRUNCATED: 54 lines] ++++++++++


======================================================================
TRUNCATED at line 0/54 (100% reduction)
Language: Markdown
Category: documentation
Key imports: https://example.com, #features, image.png
Entry points: H1: Sample Markdown Document, H2: Features, H3: Code Blocks, H3: Links and Images, H3: Lists

To get full content: --include "tests/fixtures/markdown/sample.md" --truncate 0
======================================================================
---------- tests/fixtures/markdown/sample.md [TRUNCATED:54â†’11] b1b8b88a35909111b8a6e0319b698e71 tests/fixtures/markdown/sample.md ----------
++++++++++ tests/fixtures/shell/sample.sh [TRUNCATED: 44 lines] ++++++++++
#!/bin/bash
source /etc/profile
. ~/.bashrc
function setup() {
cleanup() {
process_data() {
main() {

======================================================================
STRUCTURE MODE: Showing only signatures (7/44 lines)
Language: Shell (bash)

Included: imports, class/function signatures, type definitions
Excluded: function bodies, implementation details

To get full content: --include "tests/fixtures/shell/sample.sh" --truncate 0
======================================================================
---------- tests/fixtures/shell/sample.sh [TRUNCATED:44â†’17] beefa100a88ff925f744804c64dfe468 tests/fixtures/shell/sample.sh ----------
++++++++++ QA_INFRASTRUCTURE.md [TRUNCATED: 323 lines] ++++++++++


======================================================================
TRUNCATED at line 0/323 (100% reduction)
Language: Markdown
Category: documentation
Entry points: H1: Quality Assurance Infrastructure, H2: ðŸŽ¯ What Was Delivered, H3: âœ… Phase 1: Foundation (COMPLETE), H2: ðŸ“Š Current State, H3: Test Coverage Breakdown

To get full content: --include "QA_INFRASTRUCTURE.md" --truncate 0
======================================================================
---------- QA_INFRASTRUCTURE.md [TRUNCATED:323â†’10] 799749a103ba8819c291eea3b22584df QA_INFRASTRUCTURE.md ----------
++++++++++ scripts/doc_gen.py [TRUNCATED: 168 lines] ++++++++++
"""
"""
import re
import sys
from pathlib import Path
import pm_encoder
def get_version():
def get_help_text():
    import subprocess
def get_lens_table():
def get_language_support():
def process_file(file_path: Path, dry_run=False):
def main():
    import argparse

======================================================================
STRUCTURE MODE: Showing only signatures (14/168 lines)
Language: Python

Included: imports, class/function signatures, type definitions
Excluded: function bodies, implementation details

To get full content: --include "scripts/doc_gen.py" --truncate 0
======================================================================
---------- scripts/doc_gen.py [TRUNCATED:168â†’24] eb6ecdadaebba7d82ff94ef2967787b0 scripts/doc_gen.py ----------
++++++++++ TESTING.md [TRUNCATED: 402 lines] ++++++++++


======================================================================
TRUNCATED at line 0/402 (100% reduction)
Language: Markdown
Category: documentation
Entry points: H1: Testing Guide for pm_encoder, H2: Quick Start, H1: Run all tests, H1: Run tests with coverage report, H1: Run all quality checks

To get full content: --include "TESTING.md" --truncate 0
======================================================================
---------- TESTING.md [TRUNCATED:402â†’10] 697959c9eed94657d3b0dff967025b61 TESTING.md ----------
++++++++++ tests/fixtures/python/sample.py [TRUNCATED: 53 lines] ++++++++++
"""
"""
import os
import sys
from typing import List, Optional
from pathlib import Path
class DataProcessor:
    def __init__(self, name: str):
    def process(self, items: List[str]) -> int:
    @staticmethod
    def validate(value: str) -> bool:
@decorator_example
def decorated_function(x: int, y: int) -> int:
async def async_handler(data: bytes) -> Optional[str]:
def main():

======================================================================
STRUCTURE MODE: Showing only signatures (15/53 lines)
Language: Python

Included: imports, class/function signatures, type definitions
Excluded: function bodies, implementation details

To get full content: --include "tests/fixtures/python/sample.py" --truncate 0
======================================================================
---------- tests/fixtures/python/sample.py [TRUNCATED:53â†’25] 10c7c0251a2da00924e639e1e57ebec1 tests/fixtures/python/sample.py ----------
++++++++++ CHANGELOG.md [TRUNCATED: 224 lines] ++++++++++


======================================================================
TRUNCATED at line 0/224 (100% reduction)
Language: Markdown
Category: documentation
Key imports: https://keepachangelog.com/en/1.1.0/, https://semver.org/spec/v2.0.0.html
Entry points: H1: Changelog, H2: [1.2.2] - 2025-12-12, H3: Added - Native Rust Support, H3: Added - Technical Blueprint, H3: Added - Test Coverage

To get full content: --include "CHANGELOG.md" --truncate 0
======================================================================
---------- CHANGELOG.md [TRUNCATED:224â†’11] 58ad5b46a346902b49b0c329a3fe0102 CHANGELOG.md ----------
++++++++++ tests/test_pm_encoder.py [TRUNCATED: 505 lines] ++++++++++
"""
import unittest
import tempfile
import shutil
import json
import sys
from pathlib import Path
from io import StringIO
import pm_encoder
class TestStructureMode(unittest.TestCase):
    def setUp(self):
    def tearDown(self):
    def test_structure_mode_trigger(self):
import sys
class MyClass:
    def __init__(self):
    def method_one(self):
def standalone_function():
    def test_python_structure(self):
@decorator
def decorated_function(arg1: str, arg2: int) -> bool:
class DataProcessor:
    def process(self, data: List[str]):
    def test_js_structure(self):
import { useState } from 'react';
    def test_json_fallback(self):
    def test_rust_structure(self):
class TestLenses(unittest.TestCase):
    def setUp(self):
    def tearDown(self):
    def test_meta_injection(self):
    def test_lens_precedence(self):
class TestIgnorePatterns(unittest.TestCase):
    def setUp(self):
    def tearDown(self):
    def test_ignore_patterns(self):
class TestBuiltInLenses(unittest.TestCase):
    def test_all_lenses_exist(self):
    def test_architecture_lens_has_safety_limit(self):
def run_tests():

======================================================================
STRUCTURE MODE: Showing only signatures (40/505 lines)
Language: Python

Included: imports, class/function signatures, type definitions
Excluded: function bodies, implementation details

To get full content: --include "tests/test_pm_encoder.py" --truncate 0
======================================================================
---------- tests/test_pm_encoder.py [TRUNCATED:505â†’50] 8bb1b2c463a16a68c892279569f34c5f tests/test_pm_encoder.py ----------
++++++++++ docs/BLUEPRINT.md [TRUNCATED: 41 lines] ++++++++++


======================================================================
TRUNCATED at line 0/41 (100% reduction)
Language: Markdown
Category: documentation
Entry points: H1: pm_encoder: The Technical Blueprint, H2: From Context Compression to AI Collaboration Infrastructure, H2: Executive Summary, H2: Core Philosophy, H2: Architecture: The Shared Grammar

To get full content: --include "docs/BLUEPRINT.md" --truncate 0
======================================================================
---------- docs/BLUEPRINT.md [TRUNCATED:41â†’10] 8ccfb72b6cda6fdbdf7bab854140d7e0 docs/BLUEPRINT.md ----------
++++++++++ CONTRIBUTING.md [TRUNCATED: 144 lines] ++++++++++


======================================================================
TRUNCATED at line 0/144 (100% reduction)
Language: Markdown
Category: documentation
Entry points: H1: Contributing to pm_encoder, H2: Development Philosophy, H3: Core Principles, H2: Getting Started, H2: Making Changes

To get full content: --include "CONTRIBUTING.md" --truncate 0
======================================================================
---------- CONTRIBUTING.md [TRUNCATED:144â†’10] eae38e6d46c418544961153a5d5c5a47 CONTRIBUTING.md ----------
++++++++++ .pm_encoder_config.json [TRUNCATED: 60 lines] ++++++++++


======================================================================
TRUNCATED at line 0/60 (100% reduction)
Language: JSON
Category: config
Entry points: ignore_patterns, include_patterns, lenses

To get full content: --include ".pm_encoder_config.json" --truncate 0
======================================================================
---------- .pm_encoder_config.json [TRUNCATED:60â†’10] fa2d489200facce141a33dfa98696727 .pm_encoder_config.json ----------
++++++++++ TUTORIAL.md [TRUNCATED: 587 lines] ++++++++++


======================================================================
TRUNCATED at line 0/587 (100% reduction)
Language: Markdown
Category: documentation
Entry points: H1: PM Encoder Tutorial, H2: Quick Start, H3: Step 1: Make the script executable, H3: Step 2: Run your first encoding, H1: Serialize current directory and copy to clipboard (macOS)

To get full content: --include "TUTORIAL.md" --truncate 0
======================================================================
---------- TUTORIAL.md [TRUNCATED:587â†’10] c677dd76748f4b5dd05316fd40006933 TUTORIAL.md ----------
++++++++++ PLUGIN_GUIDE.md [TRUNCATED: 453 lines] ++++++++++


======================================================================
TRUNCATED at line 0/453 (100% reduction)
Language: Markdown
Category: documentation
Key imports: #overview, #quick-start, #plugin-architecture, #creating-a-plugin, #testing-your-plugin, #contributing-plugins, #examples
Entry points: H1: pm_encoder Plugin Development Guide, H2: Table of Contents, H2: Overview, H3: Built-in Language Support, H2: Quick Start

To get full content: --include "PLUGIN_GUIDE.md" --truncate 0
======================================================================
---------- PLUGIN_GUIDE.md [TRUNCATED:453â†’11] 4170ae1b99be19efae30bb14bde0c5fe PLUGIN_GUIDE.md ----------
++++++++++ examples/plugins/rust_analyzer.py [TRUNCATED: 116 lines] ++++++++++
"""
"""
import re
from pathlib import Path
from typing import Dict, List, Tuple, Any
class LanguageAnalyzer:
    def analyze(self, content: str, file_path: Path) -> Dict[str, Any]:
    def get_truncate_ranges(self, content: str, max_lines: int) -> Tuple[List[Tuple[int, int]], Dict[str, Any]]:

======================================================================
STRUCTURE MODE: Showing only signatures (8/116 lines)
Language: Python

Included: imports, class/function signatures, type definitions
Excluded: function bodies, implementation details

To get full content: --include "examples/plugins/rust_analyzer.py" --truncate 0
======================================================================
---------- examples/plugins/rust_analyzer.py [TRUNCATED:116â†’18] f8fabe3f3e102bb44f7dcf9699aec46c examples/plugins/rust_analyzer.py ----------
++++++++++ .github/ISSUE_TEMPLATE/bug_report.md [TRUNCATED: 43 lines] ++++++++++


======================================================================
TRUNCATED at line 0/43 (100% reduction)
Language: Markdown
Category: documentation
Entry points: H2: Bug Description, H2: Environment, H2: Steps to Reproduce, H2: Expected Behavior, H2: Actual Behavior

To get full content: --include ".github/ISSUE_TEMPLATE/bug_report.md" --truncate 0
======================================================================
---------- .github/ISSUE_TEMPLATE/bug_report.md [TRUNCATED:43â†’10] 0030092b4f808a83f93e1626b0f287a2 .github/ISSUE_TEMPLATE/bug_report.md ----------
++++++++++ context_script.sh [TRUNCATED: 26 lines] ++++++++++
#!/bin/bash

======================================================================
STRUCTURE MODE: Showing only signatures (1/26 lines)
Language: Shell (bash)

Included: imports, class/function signatures, type definitions
Excluded: function bodies, implementation details

To get full content: --include "context_script.sh" --truncate 0
======================================================================
---------- context_script.sh [TRUNCATED:26â†’11] 8451ac53c38c95938732c9e3ad034661 context_script.sh ----------
++++++++++ LICENSE [TRUNCATED: 22 lines] ++++++++++


======================================================================
TRUNCATED at line 0/22 (100% reduction)
Language: Unknown
Category: unknown

To get full content: --include "LICENSE" --truncate 0
======================================================================
---------- LICENSE [TRUNCATED:22â†’9] c11c1ace82fe49f7e827c3edb436db5a LICENSE ----------
++++++++++ LLM/2025-07-23_sip-4_enhance-pm-encoder-with-sorting.md [TRUNCATED: 220 lines] ++++++++++


======================================================================
TRUNCATED at line 0/220 (100% reduction)
Language: Markdown
Category: documentation
Entry points: H3: Files to Create/Update:, H3: Test Plan:

To get full content: --include "LLM/2025-07-23_sip-4_enhance-pm-encoder-with-sorting.md" --truncate 0
======================================================================
---------- LLM/2025-07-23_sip-4_enhance-pm-encoder-with-sorting.md [TRUNCATED:220â†’10] 1a18ff949d6ef459a577ed9e925ae2a1 LLM/2025-07-23_sip-4_enhance-pm-encoder-with-sorting.md ----------
++++++++++ scripts/backup.sh [TRUNCATED: 147 lines] ++++++++++
#!/bin/bash
log_info() { echo -e "${BLUE}[INFO]${NC} $1" >&2; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1" >&2; }
log_warn() { echo -e "${YELLOW}[WARN]${NC} $1" >&2; }
log_error() { echo -e "${RED}[ERROR]${NC} $1" >&2; }
create_backup_dirs() {
create_bundle() {
verify_bundle() {
copy_to_backup_locations() {
cleanup_old_bundles() {
main() {

======================================================================
STRUCTURE MODE: Showing only signatures (11/147 lines)
Language: Shell (bash)

Included: imports, class/function signatures, type definitions
Excluded: function bodies, implementation details

To get full content: --include "scripts/backup.sh" --truncate 0
======================================================================
---------- scripts/backup.sh [TRUNCATED:147â†’21] 15ff0c01b825b585b37f62077b5163d7 scripts/backup.sh ----------
++++++++++ SYSTEM_INSTRUCTIONS.md [TRUNCATED: 516 lines] ++++++++++


======================================================================
TRUNCATED at line 0/516 (100% reduction)
Language: Markdown
Category: documentation
Entry points: H1: System Instructions for pm_encoder, H2: Core Philosophy, H3: The Meta-Tool Paradox, H2: Session Management Protocol, H3: Session Identification

To get full content: --include "SYSTEM_INSTRUCTIONS.md" --truncate 0
======================================================================
---------- SYSTEM_INSTRUCTIONS.md [TRUNCATED:516â†’10] 4b05ac88daba59acb5e48043fb859668 SYSTEM_INSTRUCTIONS.md ----------

```

---

**Regenerate this file:**
```bash
./pm_encoder.py . --init-prompt --init-lens debug
```

*Generated by pm_encoder v1.3.1 using the 'debug' lens*
---------- CLAUDE.md 893873ce1f4f5aba1657a8c2e8b12dc2 CLAUDE.md ----------
++++++++++ CONTRIBUTING.md ++++++++++
# Contributing to pm_encoder

Thank you for your interest in contributing to pm_encoder! This document provides guidelines and information for contributors.

## Development Philosophy

pm_encoder is a **meta-tool for AI collaboration** â€” it exists to facilitate effective context sharing between developers and AI assistants. Please review `SYSTEM_INSTRUCTIONS.md` for the full development protocol.

### Core Principles

1. **Format-Preserving**: The Plus/Minus format is sacred. Changes must maintain backward compatibility.
2. **Zero Dependencies**: Standard library only. No external packages.
3. **Python 3.6+**: Maintain compatibility with Python 3.6 and above.
4. **Utility-Focused**: Every feature must solve a real context-sharing pain point.

## Getting Started

1. Fork the repository
2. Clone your fork:
   ```bash
   git clone https://github.com/alanbld/pm_encoder.git
   cd pm_encoder
   ```
3. Create a branch for your changes:
   ```bash
   git checkout -b feature/your-feature-name
   ```

## Making Changes

### Before You Code

- Read `SYSTEM_INSTRUCTIONS.md` for development protocol
- Check existing issues to avoid duplicate work
- For significant changes, open an issue first to discuss

### Code Style

- Follow existing code conventions
- Use type hints for public functions
- Prefer `pathlib.Path` over `os.path`
- Handle errors gracefully (never crash on bad input)
- Keep it simple â€” avoid over-engineering

### Quality Checklist

Before submitting, verify:

- [ ] `./pm_encoder.py --version` outputs correct version
- [ ] All unit tests pass: `python3 -m unittest tests/test_pm_encoder.py`
- [ ] `./pm_encoder.py . -o /tmp/test.txt` succeeds (self-serialization)
- [ ] Plus/Minus format output is valid
- [ ] No external dependencies added
- [ ] Code works on Python 3.6+
- [ ] New features include unit tests (see Testing section)

### Testing

**Run the comprehensive test suite:**
```bash
# Run all tests
python3 -m unittest tests/test_pm_encoder.py

# Run with verbose output
python3 -m unittest tests/test_pm_encoder.py -v

# Run specific test
python3 -m unittest tests.test_pm_encoder.TestStructureMode.test_structure_mode_trigger
```

**Self-serialization test:**
```bash
# Basic functionality test
./pm_encoder.py . -o /tmp/test_output.txt
echo $?  # Should be 0

# Verify checksum integrity
head -1 /tmp/test_output.txt  # Should start with ++++++++++
```

**Test requirements for new features:**
- All new features **must include unit tests** in `tests/test_pm_encoder.py`
- Tests should use only standard library (unittest, tempfile, shutil)
- Verify tests pass before submitting PR
- Add test descriptions in docstrings

## Submitting Changes

1. Commit your changes with clear messages:
   ```bash
   git commit -m "feat: Add depth limiting with --depth flag"
   ```

2. Push to your fork:
   ```bash
   git push origin feature/your-feature-name
   ```

3. Open a Pull Request with:
   - Clear description of changes
   - Reference to any related issues
   - Test results (unit tests + self-serialization)

## Commit Message Format

Use conventional commits:
- `feat:` New feature
- `fix:` Bug fix
- `docs:` Documentation only
- `refactor:` Code change that neither fixes a bug nor adds a feature
- `test:` Adding or updating tests
- `chore:` Maintenance tasks

## Reporting Issues

When reporting bugs, include:
- pm_encoder version (`./pm_encoder.py --version`)
- Python version (`python3 --version`)
- Operating system
- Minimal reproduction steps
- Expected vs actual behavior

## Feature Requests

For new features, consider:
- Does this help developers share better context with AI?
- Can it be implemented without external dependencies?
- Does it maintain format backward compatibility?

## Code of Conduct

- Be respectful and constructive
- Focus on technical merit
- Welcome newcomers

## Questions?

Open an issue with the `question` label or check existing documentation:
- `README.md` â€” Usage guide
- `TUTORIAL.md` â€” Step-by-step examples
- `SYSTEM_INSTRUCTIONS.md` â€” Development protocol

Thank you for contributing!
---------- CONTRIBUTING.md b7e8481feae212f676c815ae0f7cc156 CONTRIBUTING.md ----------
++++++++++ LLM/2025-07-23_sip-4_enhance-pm-encoder-with-sorting.md ++++++++++
**2025-07-23 | SIP-ID: 4 | Suggested Filename: 2025-07-23_sip-4_enhance-pm-encoder-with-sorting.md**

**Restated Task:**

Enhance the `pm_encoder.py` tool to allow sorting the serialized files by either creation time (`ctime`) or modification time (`mtime`), in addition to the default name sorting.

---

âœ… Self-correction complete. The current implementation sorts files alphabetically *within* each directory, leading to a depth-first, alphabetically sorted output. The request implies a global sort across all discovered files. I will refactor the `serialize` function to first collect a list of all valid file paths, then sort this entire list according to the user's criteria, and finally, iterate through the sorted list to write the file contents. This ensures the final output is correctly ordered globally.

### Files to Create/Update:
- `../pm_encoder/pm_encoder.py`

### Test Plan:
1.  **Default Behavior:** Run the script with no sorting flags. The output should be sorted by file path alphabetically, identical to the old behavior's effective output.
2.  **Sort by Modification Time (Descending):** Create/touch a few files in a specific order. Run the script with `--sort-by mtime --sort-order desc`. The output file should list the most recently modified file first.
3.  **Sort by Creation Time (Ascending):** Create several files. Run with `--sort-by ctime --sort-order asc`. The output should list the oldest created file first.
4.  **Argument Validation:** Test with an invalid choice for `--sort-by` (e.g., `--sort-by size`) to ensure `argparse` correctly rejects it.

++++++++++ ../pm_encoder/pm_encoder.py ++++++++++
#!/usr/bin/env python3
"""
Serializes a project directory's contents into a single text file
using the Plus/Minus format, with robust directory pruning,
filtering, and sorting capabilities.
"""

import argparse
import hashlib
import json
import sys
from pathlib import Path
from fnmatch import fnmatch

def load_config(config_path: Path | None) -> tuple[list[str], list[str]]:
    """Loads ignore and include patterns from a JSON config file."""
    # Default patterns to ignore common build artifacts and vcs folders
    ignore_patterns = [".git", "target", ".venv", "__pycache__", "*.pyc", "*.swp"]
    include_patterns = []

    if config_path and config_path.is_file():
        try:
            with config_path.open("r", encoding="utf-8") as f:
                data = json.load(f)
                ignore_patterns.extend(data.get("ignore_patterns", []))
                include_patterns.extend(data.get("include_patterns", []))
        except (json.JSONDecodeError, IOError) as e:
            print(f"Warning: Could not read or parse {config_path}: {e}", file=sys.stderr)

    return ignore_patterns, include_patterns

def is_binary(file_path: Path) -> bool:
    """
    Checks if a file is likely binary by reading a chunk and looking for null bytes.
    This is a common and effective heuristic.
    """
    try:
        with file_path.open('rb') as f:
            chunk = f.read(1024)  # Read the first 1KB
        return b'\x00' in chunk
    except IOError:
        return True # If we can't read it, treat it as problematic

def read_file_content(file_path: Path) -> str | None:
    """
    Reads a file's content, skipping binary files and large files.
    Tries UTF-8 then latin-1 encoding for text files.
    """
    try:
        # 1. Check for large files first
        if file_path.stat().st_size > 5 * 1024 * 1024: # 5 MB limit
            print(f"[SKIP] {file_path.as_posix()} (file too large)", file=sys.stderr)
            return None

        # 2. Check for binary files using the null-byte heuristic
        if is_binary(file_path):
            print(f"[SKIP] {file_path.as_posix()} (likely binary)", file=sys.stderr)
            return None

        # 3. If it seems like a text file, read it
        return file_path.read_text(encoding="utf-8")
    except UnicodeDecodeError:
        try:
            # Fallback for other text encodings that are not UTF-8
            return file_path.read_text(encoding="latin-1")
        except (IOError, UnicodeDecodeError) as e:
            print(f"Error: Could not read file {file_path}: {e}. Skipping.", file=sys.stderr)
            return None
    except IOError as e:
        print(f"Error: Could not read file {file_path}: {e}. Skipping.", file=sys.stderr)
        return None

def write_pm_format(output_stream, relative_path: Path, content: str):
    """Writes a single file's data in the Plus/Minus format."""
    path_str = relative_path.as_posix()

    checksum = hashlib.md5(content.encode('utf-8')).hexdigest()

    output_stream.write(f"++++++++++ {path_str} ++++++++++\n")
    output_stream.write(content)
    if not content.endswith('\n'):
        output_stream.write('\n')
    output_stream.write(f"---------- {path_str} {checksum} {path_str} ----------\n")

def serialize(
    project_root: Path,
    output_stream,
    ignore_patterns: list,
    include_patterns: list,
    sort_by: str,
    sort_order: str,
):
    """Collects, sorts, and serializes files based on specified criteria."""
    files_to_process = []

    # Step 1: Collect all valid file paths recursively
    def collect_files(current_dir: Path):
        try:
            # Sort items locally for deterministic traversal, preventing filesystem order dependency
            sorted_items = sorted(list(current_dir.iterdir()), key=lambda p: p.name.lower())
        except OSError as e:
            print(f"Warning: Could not read directory {current_dir}: {e}", file=sys.stderr)
            return

        for item in sorted_items:
            relative_path = item.relative_to(project_root)

            if any(fnmatch(part, pattern) for part in relative_path.parts for pattern in ignore_patterns):
                if item.is_dir():
                    print(f"[SKIP DIR] {relative_path.as_posix()} (matches ignore pattern)", file=sys.stderr)
                continue

            if item.is_dir():
                collect_files(item)
            elif item.is_file():
                if include_patterns and not any(fnmatch(relative_path.as_posix(), pattern) for pattern in include_patterns):
                    continue
                files_to_process.append(item)

    collect_files(project_root)

    # Step 2: Sort the collected list of files globally
    reverse_order = sort_order == 'desc'
    sort_key_func = None

    if sort_by == 'name':
        sort_key_func = lambda p: p.relative_to(project_root).as_posix()
    elif sort_by == 'mtime':
        sort_key_func = lambda p: p.stat().st_mtime
    elif sort_by == 'ctime':
        sort_key_func = lambda p: p.stat().st_ctime

    print(f"\nSorting {len(files_to_process)} files by {sort_by} ({sort_order})...", file=sys.stderr)
    files_to_process.sort(key=sort_key_func, reverse=reverse_order)

    # Step 3: Process and write the sorted files
    for file_path in files_to_process:
        relative_path = file_path.relative_to(project_root)
        content = read_file_content(file_path)
        if content is not None:
            print(f"[KEEP] {relative_path.as_posix()}", file=sys.stderr)
            write_pm_format(output_stream, relative_path, content)

def main():
    """Main entry point for the script."""
    parser = argparse.ArgumentParser(
        description="Serialize project files into the Plus/Minus format.",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument("project_root", type=Path, help="The root directory of the project to serialize.")
    parser.add_argument("-o", "--output", type=argparse.FileType('w', encoding='utf-8'), default=sys.stdout,
                        help="Output file path. Defaults to standard output.")
    parser.add_argument("-c", "--config", type=Path, default=Path(".pm_encoder_config.json"),
                        help="Path to a JSON configuration file for ignore/include patterns.\nDefaults to ./.pm_encoder_config.json")
    parser.add_argument("--include", nargs='*', default=[],
                        help="One or more glob patterns for files to include. Overrides config includes.")
    parser.add_argument("--exclude", nargs='*', default=[],
                        help="One or more glob patterns for files/dirs to exclude. Adds to config excludes.")
    parser.add_argument("--sort-by", choices=["name", "mtime", "ctime"], default="name",
                        help="Sort files by 'name' (default), 'mtime' (modification time), or 'ctime' (creation time).")
    parser.add_argument("--sort-order", choices=["asc", "desc"], default="asc",
                        help="Sort order: 'asc' (ascending, default) or 'desc' (descending).")

    args = parser.parse_args()

    if not args.project_root.is_dir():
        print(f"Error: Project root '{args.project_root}' is not a valid directory.", file=sys.stderr)
        sys.exit(1)

    ignore_patterns, include_patterns = load_config(args.config)

    # Handle command-line overrides
    if args.include:
        print(f"Overriding include patterns with CLI arguments: {args.include}", file=sys.stderr)
        include_patterns = args.include

    if args.exclude:
        print(f"Adding CLI exclude patterns: {args.exclude}", file=sys.stderr)
        ignore_patterns.extend(args.exclude)

    print(f"\nSerializing '{args.project_root}'...", file=sys.stderr)

    try:
        serialize(
            args.project_root,
            args.output,
            ignore_patterns,
            include_patterns,
            args.sort_by,
            args.sort_order,
        )
        print(f"\nSuccessfully serialized project.", file=sys.stderr)
    finally:
        if args.output is not sys.stdout:
            args.output.close()

if __name__ == "__main__":
    main()
------------ ../pm_encoder/pm_encoder.py ------------
---------- LLM/2025-07-23_sip-4_enhance-pm-encoder-with-sorting.md 26268618af298dc6de5f0a10161485be LLM/2025-07-23_sip-4_enhance-pm-encoder-with-sorting.md ----------
++++++++++ PLUGIN_GUIDE.md ++++++++++
# pm_encoder Plugin Development Guide

This guide explains how to create custom language analyzer plugins for pm_encoder to extend truncation support to additional programming languages.

## Table of Contents

- [Overview](#overview)
- [Quick Start](#quick-start)
- [Plugin Architecture](#plugin-architecture)
- [Creating a Plugin](#creating-a-plugin)
- [Testing Your Plugin](#testing-your-plugin)
- [Contributing Plugins](#contributing-plugins)
- [Examples](#examples)

## Overview

pm_encoder v1.1+ supports intelligent file truncation based on language-specific analysis. The plugin system allows you to add support for languages not included in the built-in analyzers.

### Built-in Language Support

pm_encoder includes analyzers for:

| Language | Extensions | Features Detected |
|----------|-----------|-------------------|
| Python | `.py`, `.pyw` | Classes, functions, imports, `__main__`, markers |
| JavaScript/TypeScript | `.js`, `.jsx`, `.ts`, `.tsx`, `.mjs`, `.cjs` | Classes, functions, imports, exports |
| Shell | `.sh`, `.bash`, `.zsh`, `.fish` | Functions, sourced files, shebang |
| Markdown | `.md`, `.markdown` | Headers, code blocks, links |
| JSON | `.json` | Keys, depth, structure |
| YAML | `.yaml`, `.yml` | Keys, structure |

## Quick Start

### 1. Generate a Plugin Template

```bash
./pm_encoder.py --create-plugin MyLanguage > my_language_analyzer.py
```

### 2. Customize the Template

Edit the generated file to add language-specific parsing logic:

```python
class LanguageAnalyzer:
    SUPPORTED_EXTENSIONS = ['.mylang', '.ml']  # Your file extensions
    LANGUAGE_NAME = "MyLanguage"

    def analyze(self, content: str, file_path: Path) -> Dict[str, Any]:
        # Add your language parsing here
        # Return structure with classes, functions, imports, etc.
        pass
```

### 3. Install the Plugin

```bash
mkdir -p ~/.pm_encoder/plugins/
cp my_language_analyzer.py ~/.pm_encoder/plugins/
```

### 4. Use Your Plugin

```bash
./pm_encoder.py . --truncate 500 --truncate-mode smart --language-plugins ~/.pm_encoder/plugins/
```

## Plugin Architecture

### The LanguageAnalyzer Interface

Every plugin must implement the `LanguageAnalyzer` class with two key methods:

```python
class LanguageAnalyzer:
    """Base interface for language analyzers."""

    SUPPORTED_EXTENSIONS = ['.ext']  # Required: List of file extensions
    LANGUAGE_NAME = "LanguageName"    # Required: Display name

    def analyze(self, content: str, file_path: Path) -> Dict[str, Any]:
        """
        Analyze file content and extract language constructs.

        Args:
            content: The complete file content as a string
            file_path: Path object for the file (may be None)

        Returns:
            Dictionary with keys:
            - language: str         # Language name
            - classes: List[str]    # Class/type names
            - functions: List[str]  # Function/method names
            - imports: List[str]    # Import statements
            - entry_points: List    # Main functions, exports
            - config_keys: List     # For config files
            - documentation: List   # Doc types found
            - markers: List         # TODO, FIXME, etc.
            - category: str         # application|library|test|config|documentation|script
            - critical_sections: List[Tuple[int, int]]  # (start_line, end_line) ranges
        """

    def get_truncate_ranges(self, content: str, max_lines: int) -> Tuple[List[Tuple[int, int]], Dict[str, Any]]:
        """
        Determine which line ranges to preserve during truncation.

        Args:
            content: The complete file content
            max_lines: Maximum number of lines to keep

        Returns:
            Tuple of (ranges, analysis) where:
            - ranges: List of (start_line, end_line) tuples (1-indexed)
            - analysis: Result from analyze() method
        """
```

### Analysis Return Structure

The `analyze()` method must return a dictionary with this structure:

```python
{
    "language": "Python",                    # Your language name
    "classes": ["MyClass", "Helper"],        # Classes/structs/types found
    "functions": ["main", "process", ...],   # Functions/methods found
    "imports": ["os", "sys", "re"],          # Import/require/use statements
    "entry_points": ["__main__"],            # Program entry points
    "config_keys": [],                       # Top-level keys (for config files)
    "documentation": ["docstrings"],         # Doc formats present
    "markers": ["TODO (line 42)", ...],      # Code markers found
    "category": "application",               # File categorization
    "critical_sections": [(150, 170), ...]   # Important line ranges
}
```

## Creating a Plugin

### Step-by-Step Example: Go Language Analyzer

#### 1. Generate Template

```bash
./pm_encoder.py --create-plugin Go > examples/plugins/go_analyzer.py
```

#### 2. Define Extensions and Patterns

```python
class LanguageAnalyzer:
    SUPPORTED_EXTENSIONS = ['.go']
    LANGUAGE_NAME = "Go"

    def analyze(self, content: str, file_path: Path) -> Dict[str, Any]:
        lines = content.split('\n')

        # Define regex patterns for Go constructs
        package_pattern = re.compile(r'^\s*package\s+(\w+)')
        struct_pattern = re.compile(r'^\s*type\s+(\w+)\s+struct')
        interface_pattern = re.compile(r'^\s*type\s+(\w+)\s+interface')
        func_pattern = re.compile(r'^\s*func\s+(?:\(\w+\s+\*?\w+\)\s+)?(\w+)')
        import_pattern = re.compile(r'^\s*"([^"]+)"')

        # ... continue implementation
```

#### 3. Parse Language Constructs

```python
        structs = []
        functions = []
        imports = []

        in_import_block = False

        for i, line in enumerate(lines, 1):
            # Handle package and imports
            if 'import (' in line:
                in_import_block = True
                continue
            if in_import_block and ')' in line:
                in_import_block = False
                continue

            if in_import_block or line.strip().startswith('import'):
                if match := import_pattern.search(line):
                    imports.append(match.group(1))

            # Parse structs
            if match := struct_pattern.match(line):
                structs.append(match.group(1))

            # Parse functions
            if match := func_pattern.match(line):
                fn_name = match.group(1)
                functions.append(fn_name)
                if fn_name == 'main':
                    entry_points.append(('func main', i))
```

#### 4. Categorize the File

```python
        # Determine file category
        category = "library"
        if 'main' in functions:
            category = "application"
        if file_path and '_test.go' in str(file_path):
            category = "test"

        return {
            "language": "Go",
            "classes": structs + interfaces,
            "functions": functions[:20],
            "imports": imports[:10],
            "entry_points": entry_points,
            "category": category,
            "critical_sections": [(ep[1], ep[1] + 15) for ep in entry_points]
        }
```

#### 5. Implement Smart Truncation Strategy

```python
    def get_truncate_ranges(self, content: str, max_lines: int) -> Tuple[List[Tuple[int, int]], Dict[str, Any]]:
        lines = content.split('\n')
        total_lines = len(lines)

        if total_lines <= max_lines:
            return [(1, total_lines)], self.analyze(content, None)

        analysis = self.analyze(content, None)

        # Go strategy: preserve package, imports, type definitions, main function
        keep_first = int(max_lines * 0.5)   # Package + imports + types
        keep_last = int(max_lines * 0.15)   # Exports and cleanup

        ranges = [(1, keep_first)]

        # Include main function if present
        if analysis["critical_sections"]:
            for start, end in analysis["critical_sections"]:
                if start > keep_first:
                    ranges.append((start - 3, min(end, total_lines)))

        # Add tail section
        if total_lines - keep_last > keep_first:
            ranges.append((total_lines - keep_last + 1, total_lines))

        return ranges, analysis
```

### Best Practices

1. **Use Regex, Not AST Parsing**: Keep plugins fast and dependency-free
2. **Be Conservative**: It's better to capture too much than too little
3. **Handle Edge Cases**: Empty files, malformed syntax, unusual formatting
4. **Limit List Sizes**: Truncate long lists (e.g., `functions[:20]`)
5. **Test on Real Code**: Use actual files from popular projects
6. **Performance**: Target <100ms per file analysis

### Common Patterns

#### Detecting Entry Points

```python
# Python
if '__name__' in line and '__main__' in line:
    entry_points.append(('__main__ block', i))

# JavaScript
if 'export default' in line:
    entry_points.append(('default export', i))

# Go/Rust/C
if 'func main' in line or 'fn main' in line or 'int main' in line:
    entry_points.append(('main function', i))
```

#### Handling Import Blocks

```python
# Multi-line imports (Go, Python from...import)
in_import_block = False
for line in lines:
    if line.startswith('import (') or line.startswith('from'):
        in_import_block = True
    if in_import_block and ')' in line:
        in_import_block = False
    if in_import_block:
        # Extract import
```

#### Finding Code Markers

```python
marker_pattern = re.compile(r'(?://|#)\s*(TODO|FIXME|XXX|HACK|NOTE):?\s*(.+)', re.IGNORECASE)
for i, line in enumerate(lines, 1):
    if match := marker_pattern.search(line):
        markers.append((match.group(1), match.group(2).strip(), i))
```

## Testing Your Plugin

### 1. Test on Sample Files

Create test files for your language:

```bash
mkdir -p test_files/
# Add sample files in your language
```

### 2. Run with Truncation

```bash
./pm_encoder.py test_files/ --truncate 50 --truncate-mode smart \
    --language-plugins ~/.pm_encoder/plugins/ -o output.txt
```

### 3. Verify Output

Check that the truncated output:
- Preserves imports/dependencies
- Keeps class/function signatures
- Includes entry points
- Shows useful summary in truncation markers

### 4. Test Edge Cases

- Empty files
- Very small files (<10 lines)
- Files with only comments
- Malformed syntax
- Unicode content

## Contributing Plugins

We welcome community contributions! To share your plugin:

### 1. Create a Pull Request

Submit your plugin to the `examples/plugins/` directory:

```
examples/plugins/
â”œâ”€â”€ rust_analyzer.py
â”œâ”€â”€ go_analyzer.py
â”œâ”€â”€ kotlin_analyzer.py  â† Your plugin
â””â”€â”€ ...
```

### 2. Include Documentation

Add a comment block at the top:

```python
"""
pm_encoder Language Plugin: Kotlin
Analyzer for Kotlin source files

Author: Your Name
Tested on: Kotlin 1.9+
Extensions: .kt, .kts

Example usage:
    ./pm_encoder.py . --truncate 500 --language-plugins ~/.pm_encoder/plugins/

Features detected:
    - Classes, data classes, objects
    - Functions and extension functions
    - Imports
    - Package declarations
    - Main functions
"""
```

### 3. Add Tests

Include a sample file showing your plugin in action:

```
examples/
â”œâ”€â”€ plugins/
â”‚   â””â”€â”€ kotlin_analyzer.py
â””â”€â”€ test_files/
    â””â”€â”€ Sample.kt
```

### 4. Update Language Matrix

Add your language to the table in README.md:

```markdown
| Kotlin (community) | `.kt`, `.kts` | Classes, functions, imports |
```

## Examples

### Complete Plugin: Elixir

See `examples/plugins/elixir_analyzer.py` for a full example that detects:
- Modules (`defmodule`)
- Functions (`def`, `defp`)
- Imports (`import`, `alias`, `use`)
- Tests (`describe`, `test`)

### Using AI to Generate Plugins

Generate a prompt to get AI assistance:

```bash
./pm_encoder.py --plugin-prompt Elixir > elixir_prompt.txt
# Send elixir_prompt.txt to an AI assistant
```

The AI will receive a structured prompt with:
- Plugin interface specification
- Example template
- Requirements and constraints
- Example code structure

## Troubleshooting

### Plugin Not Loading

1. Check file location: `~/.pm_encoder/plugins/your_analyzer.py`
2. Verify class name is exactly `LanguageAnalyzer`
3. Ensure `SUPPORTED_EXTENSIONS` and `LANGUAGE_NAME` are defined

### Truncation Not Working

1. Verify your extension is in `SUPPORTED_EXTENSIONS`
2. Check that `get_truncate_ranges()` returns valid line numbers (1-indexed)
3. Test with `--truncate-mode smart` (not simple)

### Performance Issues

1. Avoid complex regex patterns
2. Limit iterations (don't parse every character)
3. Truncate large lists early
4. Use `re.compile()` outside loops

## Support

- **Issues**: https://github.com/alanbld/pm_encoder/issues
- **Discussions**: https://github.com/alanbld/pm_encoder/discussions
- **Examples**: See `examples/plugins/` directory

## License

All contributed plugins should be MIT licensed to match the pm_encoder project license.
---------- PLUGIN_GUIDE.md 19dec673341bb150a582de1c65ec4c48 PLUGIN_GUIDE.md ----------
++++++++++ QA_INFRASTRUCTURE.md ++++++++++
# Quality Assurance Infrastructure

**Status**: Phase 1 Complete - Foundation Established
**Version**: 1.2.2
**Coverage Target**: >98%
**Current Coverage**: ~44% (baseline established)

## ðŸŽ¯ What Was Delivered

This document tracks the QA infrastructure implementation for pm_encoder, establishing it as a reference-quality, production-grade tool.

### âœ… Phase 1: Foundation (COMPLETE)

**1. Comprehensive Testing Framework**
- âœ… `Makefile` - Convenience commands for all QA operations
- âœ… `TESTING.md` - Complete testing guide (2000+ lines)
- âœ… Test fixtures infrastructure (`tests/fixtures/`)
- âœ… Coverage baseline established (44%)

**2. Automation & CI/CD**
- âœ… `.github/workflows/quality.yml` - GitHub Actions CI/CD
  - Multi-version Python testing (3.6-3.12)
  - Coverage analysis and reporting
  - Integration tests
  - Performance benchmarks
- âœ… `scripts/doc_gen.py` - Documentation synchronization tool

**3. Developer Experience**
- âœ… `make help` - Discover all available commands
- âœ… `make test` - Run full test suite
- âœ… `make coverage` - Generate coverage reports
- âœ… `make quality` - Run all quality checks
- âœ… `make ci` - Run full CI pipeline locally

## ðŸ“Š Current State

### Test Coverage Breakdown

```
Component                Coverage    Target
----------------------------------------------
pm_encoder.py            44%         >98%
tests/test_pm_encoder.py 90%         100%
----------------------------------------------
TOTAL                    48%         >98%
```

**What's Covered** (Current 10 tests):
- âœ… Structure mode triggering logic
- âœ… Lens precedence system
- âœ… Python/JavaScript/Rust structure extraction
- âœ… JSON fallback behavior
- âœ… Meta file injection
- âœ… Ignore patterns
- âœ… Built-in lens validation

**What Needs Coverage** (Path to >98%):
- â³ All language analyzers (comprehensive edge cases)
- â³ CLI argument parsing
- â³ Serialize function edge cases
- â³ Error handling paths
- â³ Plugin system
- â³ Configuration loading
- â³ Binary file detection
- â³ Large file handling

### Quality Gates

| Gate | Status | Command |
|------|--------|---------|
| **Unit Tests** | âœ… 10/10 passing | `make test` |
| **Coverage** | â³ 44% (target 98%) | `make coverage` |
| **Linting** | âœ… No syntax errors | `make lint` |
| **Self-Serialization** | âœ… Works | `make self-serialize` |
| **CI Pipeline** | âœ… Configured | `.github/workflows/` |

## ðŸ› ï¸ Available Commands

```bash
# Quick Reference
make help                # Show all commands
make test                # Run test suite
make coverage            # Generate coverage report
make quality             # Run all quality checks
make ci                  # Full CI pipeline locally
make clean               # Clean generated files
make install-dev         # Install coverage tool
```

### Detailed Commands

```bash
# Testing
make test                # Run all tests (verbose)
make test-quick          # Run all tests (quiet)
make coverage            # Run with coverage report
make coverage-check      # Verify â‰¥98% coverage

# Quality
make lint                # Python syntax check
make self-serialize      # Test self-serialization
make quality             # All checks
make ci                  # Full CI pipeline

# Utilities
make clean               # Remove generated files
make version             # Show pm_encoder version
```

## ðŸ“ˆ Path to >98% Coverage

### High-Value Test Additions Needed

**Priority 1: Language Analyzers** (+30% coverage)
```python
# tests/test_analyzers.py
class TestPythonAnalyzer(unittest.TestCase):
    def test_detect_classes(self):
        # Test class detection
    def test_detect_async_functions(self):
        # Test async function detection
    def test_detect_decorators(self):
        # Test decorator detection
    # ... similar for all 7 analyzers
```

**Priority 2: CLI & Main Function** (+15% coverage)
```python
# tests/test_cli.py
class TestCLI(unittest.TestCase):
    def test_argument_parsing(self):
        # Test all CLI arguments
    def test_lens_flag(self):
        # Test --lens flag
    def test_truncate_modes(self):
        # Test all truncation modes
```

**Priority 3: Edge Cases** (+10% coverage)
```python
# tests/test_edge_cases.py
class TestEdgeCases(unittest.TestCase):
    def test_empty_directory(self):
    def test_binary_files(self):
    def test_large_files(self):
    def test_permission_errors(self):
    def test_symlinks(self):
```

**Priority 4: Integration Tests** (+5% coverage)
```python
# tests/test_integration.py
class TestIntegration(unittest.TestCase):
    def test_full_workflow(self):
        # End-to-end serialization
    def test_lens_application(self):
        # Full lens workflow
```

## ðŸŽ® GitHub Actions CI/CD

### Workflow: `.github/workflows/quality.yml`

**Jobs**:
1. **test**: Multi-version Python testing (3.6-3.12)
2. **coverage**: Coverage analysis with reports
3. **lint**: Code quality checks
4. **integration**: End-to-end integration tests
5. **performance**: Benchmark tests

**Triggers**:
- Push to `main`, `develop`, `claude/*` branches
- Pull requests to `main`, `develop`

**Artifacts**:
- HTML coverage reports
- Test results
- Performance benchmarks

### Running CI Locally

```bash
# Before pushing to GitHub
make ci

# This runs:
# 1. Clean up
# 2. Run tests
# 3. Check coverage
# 4. Lint code
# 5. Test self-serialization
```

## ðŸ“ Documentation Generator

### Tool: `scripts/doc_gen.py`

Synchronizes auto-generated content in documentation.

**Usage**:
```bash
# Dry run (show what would change)
python3 scripts/doc_gen.py --dry-run

# Update docs
python3 scripts/doc_gen.py

# Or via Makefile
make docs
```

**Supported Markers**:
- `<!-- BEGIN_GEN:VERSION -->` - Current version
- `<!-- BEGIN_GEN:LENS_TABLE -->` - Lens comparison table
- `<!-- BEGIN_GEN:LANGUAGE_SUPPORT -->` - Language support matrix

**Example**:
```markdown
## Version
<!-- BEGIN_GEN:VERSION -->
1.2.2
<!-- END_GEN:VERSION -->
```

## ðŸ§ª Test Fixtures

Located in `tests/fixtures/`:

```
tests/fixtures/
â”œâ”€â”€ python/sample.py      # Comprehensive Python test file
â”œâ”€â”€ javascript/sample.js  # JS/TS patterns
â”œâ”€â”€ rust/sample.rs        # Rust patterns
â”œâ”€â”€ shell/               # Shell scripts
â”œâ”€â”€ markdown/            # Markdown files
â”œâ”€â”€ yaml/                # YAML configs
â”œâ”€â”€ json/                # JSON data
â””â”€â”€ edge_cases/          # Edge case files
```

**Adding Fixtures**:
```bash
# Create new fixture
echo 'test content' > tests/fixtures/category/file.ext

# Use in tests
fixture = Path(__file__).parent / "fixtures" / "category" / "file.ext"
content = fixture.read_text()
```

## ðŸš€ Next Steps (Phase 2)

### Immediate (Target: >98% Coverage)
- [ ] Create `tests/test_comprehensive.py` with 50+ tests
- [ ] Add edge case tests for all analyzers
- [ ] Add CLI argument parsing tests
- [ ] Add error handling tests
- [ ] Achieve >98% coverage

### Short-term (Enhanced Automation)
- [ ] Add `pre-commit` hooks
- [ ] Add coverage badges to README
- [ ] Set up Codecov integration
- [ ] Add performance regression tests
- [ ] Create `tests/test_documentation.py`

### Medium-term (Living Documentation)
- [ ] Add markers to README.md and TUTORIAL.md
- [ ] Auto-sync version numbers
- [ ] Auto-generate lens comparison tables
- [ ] Auto-generate language support matrices
- [ ] Integrate with GitHub Actions

## ðŸ“š Resources

- **Testing Guide**: `TESTING.md`
- **Makefile**: `Makefile`
- **CI Configuration**: `.github/workflows/quality.yml`
- **Doc Generator**: `scripts/doc_gen.py`
- **Contributing**: `CONTRIBUTING.md`

## ðŸŽ¯ Success Criteria

- [x] Makefile with convenience commands
- [x] TESTING.md guide created
- [x] GitHub Actions CI/CD configured
- [x] Coverage baseline established (44%)
- [x] Test fixtures infrastructure
- [x] Documentation generator tool
- [ ] Coverage >98% (in progress - needs test expansion)
- [ ] All CI jobs passing
- [ ] Pre-commit hooks (pending)

## ðŸ’¡ Why This Matters

### For Quality
```
Foundation â†’ Tests â†’ Coverage â†’ Confidence â†’ Production
```

### For Reputation
```
>98% Coverage = Professional = Trust = Adoption
```

### For the Multi-AI Story
```
"Built by AI" (cool) + "98% tested" (exceptional) = Credibility
```

## ðŸ“ž Support

Questions about QA infrastructure:
1. Check `TESTING.md` for testing guide
2. Run `make help` for commands
3. See CI logs in GitHub Actions
4. Open issue with `qa` label

---

**Status**: Phase 1 foundation complete. Path to >98% coverage established.
**Next**: Expand test suite to achieve coverage target.
---------- QA_INFRASTRUCTURE.md bf6f375f249c50d80abd97d51e5805c6 QA_INFRASTRUCTURE.md ----------
++++++++++ README.md ++++++++++
# Project Encoder (`pm_encoder.py`)

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.6+](https://img.shields.io/badge/python-3.6+-blue.svg)](https://www.python.org/downloads/)

`pm_encoder.py` is a powerful command-line utility designed to serialize a project's source files into a single, well-structured text file. This is ideal for sharing project context with Large Language Models (LLMs), creating context packages for new developers, or for archival purposes.

The script uses a custom "Plus/Minus" format that is both human-readable and easy for other scripts to parse. It offers robust filtering capabilities through a JSON configuration file and command-line arguments.

## Features

- **Plus/Minus Format**: A clean, readable format that wraps each file's content with clear start and end markers.
- **Data Integrity**: The closing marker for each file includes an MD5 checksum to verify content integrity.
- **Binary File Detection**: Automatically detects and skips binary files (e.g., images, executables, archives) to keep the output clean and text-focused.
- **Configuration File**: Uses a `.pm_encoder_config.json` file to define default include and exclude patterns.
- **CLI Overrides**: Command-line flags (`--include`, `--exclude`) allow for flexible, on-the-fly filtering that can override or extend the configuration file settings.
- **Robust Filtering**: Uses glob patterns for powerful matching of files and directories.
- **Directory Pruning**: Efficiently skips entire directories (like `target/` or `node_modules/`) that match ignore patterns.
- **Large File Skipping**: Avoids including files over a certain size (default: 5MB) to keep the output manageable.
- **Standard I/O**: Writes to standard output by default, allowing it to be piped to other commands (e.g., clipboards).
- **ðŸ†• Intelligent Truncation** (v1.1+): Language-aware file truncation to reduce token usage while preserving critical code structures.
- **ðŸ†• Multi-Language Support** (v1.1+): Built-in analyzers for Python, JavaScript/TypeScript, Shell, Markdown, JSON, and YAML.
- **ðŸ†• Plugin System** (v1.1+): Extensible architecture for community-contributed language analyzers.
- **ðŸ†• Token Optimization** (v1.1+): Detailed statistics on size and token reduction.

## Project Structure

This repository contains **two implementations** of pm_encoder:

### Python Implementation (Current Production)

- **Location:** `pm_encoder.py` (root directory)
- **Version:** 1.3.1
- **Status:** Production-ready with 95% test coverage
- **Best for:** Python ecosystem integration, rapid feature development
- **Dependencies:** None (Python 3.6+ stdlib only)

### Rust Implementation (v2.0 Foundation)

- **Location:** `rust/` directory
- **Version:** 0.1.0 (skeleton)
- **Status:** Architecture foundation
- **Best for:** High-performance, WASM/Python bindings, large codebases
- **Architecture:** Library-first pattern (`lib.rs` + `bin/main.rs`)

The Rust implementation is designed with a **Library-First** architecture:
- `rust/src/lib.rs` - Pure logic, reusable by CLI/WASM/PyO3
- `rust/src/bin/main.rs` - Thin CLI wrapper

See `rust/README.md` for details on the Rust architecture and future WASM/Python binding plans.

## Documentation

For AI-assisted development and comprehensive project context:
- **[Knowledge Base](docs/KNOWLEDGE_BASE.md)** - Single source of truth for AI sessions (architecture, decisions, roadmap)
- **[The Twins Architecture](docs/THE_TWINS_ARCHITECTURE.md)** - Dual-engine design philosophy and roadmap
- **[Rust Growth Strategy](docs/RUST_GROWTH_STRATEGY.md)** - Fast-track plan to feature parity (v0.1.0 â†’ v1.0.0)
- **[Blueprint](docs/BLUEPRINT.md)** - Strategic vision and feature planning
- **[The Turing Audit](docs/THE_TURING_AUDIT.md)** - Multi-AI development story
- **[Testing Guide](TESTING.md)** - Test infrastructure and coverage details

## The Plus/Minus Format

The script outputs files in the following format:

```
++++++++++ path/to/your/file.rs ++++++++++
// The full and COMPLETE content of the file goes here.
// Every line must be included.
---------- path/to/your/file.rs <md5_checksum> path/to/your/file.rs ----------
```

## Prerequisites

- Python 3.6+

## Installation

### Option 1: Quick Download (Single File)

Download the script directly:

```bash
# Download to current directory
curl -O https://raw.githubusercontent.com/alanbld/pm_encoder/main/pm_encoder.py
chmod +x pm_encoder.py

# Or download to a scripts/ directory
mkdir -p scripts
curl -o scripts/pm_encoder.py https://raw.githubusercontent.com/alanbld/pm_encoder/main/pm_encoder.py
chmod +x scripts/pm_encoder.py
```

### Option 2: Git Clone (Full Repository)

Clone the repository for full access to examples and documentation:

```bash
git clone https://github.com/alanbld/pm_encoder.git
cd pm_encoder
chmod +x pm_encoder.py
```

### Option 3: Copy to Your Project

1. Place the `pm_encoder.py` script in a `scripts/` directory within your project.
2. Make the script executable:
   ```bash
   chmod +x scripts/pm_encoder.py
   ```

### Configuration (Optional)

Create a `.pm_encoder_config.json` file in your project's root directory to define default filters. See the **Configuration** section below for an example, or copy from `examples/.pm_encoder_config.json`.

## Usage

The script is run from the command line.

```
usage: pm_encoder.py [-h] [--version] [-o OUTPUT] [-c CONFIG] [--include [INCLUDE ...]]
                     [--exclude [EXCLUDE ...]] [--sort-by {name,mtime,ctime}]
                     [--sort-order {asc,desc}] project_root

Serialize project files into the Plus/Minus format.

positional arguments:
  project_root          The root directory of the project to serialize.

options:
  -h, --help            show this help message and exit
  --version             show program's version number and exit
  -o OUTPUT, --output OUTPUT
                        Output file path. Defaults to standard output.
  -c CONFIG, --config CONFIG
                        Path to a JSON configuration file for ignore/include patterns.
                        Defaults to ./.pm_encoder_config.json
  --include [INCLUDE ...]
                        One or more glob patterns for files to include. Overrides config includes.
  --exclude [EXCLUDE ...]
                        One or more glob patterns for files/dirs to exclude. Adds to config excludes.
  --sort-by {name,mtime,ctime}
                        Sort files by 'name' (default), 'mtime' (modification time),
                        or 'ctime' (creation time).
  --sort-order {asc,desc}
                        Sort order: 'asc' (ascending, default) or 'desc' (descending).
```

---

## Examples

### 1. Basic Usage (Using Config File)

Serialize the current project (`.`) and save the output to `context.txt`. This will use the filters defined in `.pm_encoder_config.json`.

```bash
./scripts/pm_encoder.py . -o context.txt
```

### 2. Piping to Clipboard

Serialize the project and pipe the output directly to your system's clipboard.

```bash
# On macOS
./scripts/pm_encoder.py . | pbcopy

# On Linux (requires xclip)
./scripts/pm_encoder.py . | xclip -selection clipboard
```

### 3. Packaging Only Specific Files (`--include`)

To package only the Rust source files from the `api_test_tool_core` crate and the main `Cargo.toml`, you can override the include patterns from the command line.

```bash
./scripts/pm_encoder.py . \
  --include "api_test_tool_core/src/**/*.rs" "Cargo.toml" \
  -o core_crate_context.txt
```
*Note: The `**` glob pattern allows for recursive matching.*

### 4. Temporarily Excluding Files (`--exclude`)

To serialize the project according to the config file but also exclude all Markdown files (`*.md`) and the `docs/` directory for this run:

```bash
./scripts/pm_encoder.py . --exclude "*.md" "docs" -o no_docs_context.txt
```

### 5. Combining Filters

Create a package containing only Python scripts (`*.py`) and shell scripts (`*.sh`), while also ensuring that the `.venv` directory is ignored.

```bash
./scripts/pm_encoder.py . \
  --include "*.py" "*.sh" \
  --exclude ".venv" \
  -o scripts_only.txt
```

### 6. Token Optimization with Truncation (v1.1+)

When sharing large projects with LLMs, you may hit token limits. Use intelligent truncation to reduce file sizes while preserving the most important code:

```bash
# Smart truncation (500 lines per file, language-aware)
./pm_encoder.py . --truncate 500 --truncate-mode smart -o context.txt

# Show truncation statistics
./pm_encoder.py . --truncate 300 --truncate-stats

# Exclude certain files from truncation
./pm_encoder.py . --truncate 500 --truncate-exclude "README.md" "LICENSE"

# Simple truncation (just keep first N lines)
./pm_encoder.py . --truncate 200 --truncate-mode simple -o quick.txt
```

**Smart truncation** analyzes each file's language and preserves:
- Import statements and dependencies
- Class and function signatures
- Entry points (main functions, exports)
- Critical code sections
- Documentation headers

**Example truncation output:**
```
++++++++++ src/database/handler.py [TRUNCATED: 873 lines] ++++++++++
[First 250 lines showing imports, class definitions, key functions]

... [400 lines omitted] ...

[Last 50 lines showing main entry point]

======================================================================
TRUNCATED at line 500/873 (42% reduction)
Language: Python
Category: Application Module
Classes (5): DatabaseHandler, MigrationRunner, SchemaValidator
Functions (23): apply_migration, rollback, validate_schema, ...
Key imports: psycopg2, sqlalchemy, pandas

To get full content: --include "src/database/handler.py" --truncate 0
======================================================================
---------- src/database/handler.py [TRUNCATED:873â†’300] a7b3c9d2... ----------
```

---

## Language Support (v1.1+)

pm_encoder includes built-in intelligent truncation for multiple programming languages:

| Language | Extensions | Detected Features |
|----------|-----------|-------------------|
| **Python** | `.py`, `.pyw` | Classes, functions, imports, `__main__` blocks, docstrings, TODO/FIXME markers |
| **JavaScript/TypeScript** | `.js`, `.jsx`, `.ts`, `.tsx`, `.mjs`, `.cjs` | Classes, functions, arrow functions, imports, exports, JSDoc |
| **Shell** | `.sh`, `.bash`, `.zsh`, `.fish` | Functions, sourced files, shebang |
| **Markdown** | `.md`, `.markdown` | Headers, code blocks, links, table of contents |
| **JSON** | `.json` | Keys, nested structure depth, value types |
| **YAML** | `.yaml`, `.yml` | Keys, nested structure |

### Extending Language Support

Create custom language analyzers for additional languages:

```bash
# Generate a plugin template
./pm_encoder.py --create-plugin Rust > rust_analyzer.py

# Or get AI assistance
./pm_encoder.py --plugin-prompt Kotlin > kotlin_prompt.txt
```

See [PLUGIN_GUIDE.md](PLUGIN_GUIDE.md) for complete plugin development documentation.

### Community Plugins

Example plugins available in `examples/plugins/`:
- **Rust** (`rust_analyzer.py`): Structs, traits, functions, use statements

To use community plugins:
```bash
mkdir -p ~/.pm_encoder/plugins/
cp examples/plugins/rust_analyzer.py ~/.pm_encoder/plugins/
./pm_encoder.py . --truncate 500 --language-plugins ~/.pm_encoder/plugins/
```

---

## Context Lenses (v1.2.0)

Context Lenses provide pre-configured serialization profiles optimized for specific use cases. Each lens configures filters, sorting, and truncation strategies to produce focused, relevant context.

### Built-in Lenses

| Lens | Description | Truncation | Sort | Best For |
|------|-------------|------------|------|----------|
| **architecture** | High-level structure, interfaces, configuration | Structure mode | Name (ASC) | Understanding codebase organization, API surface |
| **debug** | Recent changes for debugging | None (full files) | Mtime (DESC) | Bug investigation, recent modifications |
| **security** | Security-sensitive code for review | Smart (300 lines) | Name (ASC) | Security audits, vulnerability scanning |
| **onboarding** | New developer introduction | Smart (400 lines) | Name (ASC) | Team onboarding, documentation |

### Using Context Lenses

```bash
# Architecture view - signatures only
./pm_encoder.py . --lens architecture -o architecture.txt

# Debug view - recent changes with full content
./pm_encoder.py . --lens debug -o recent_changes.txt

# Security review - focused on security-critical code
./pm_encoder.py . --lens security -o security_review.txt

# Onboarding - balanced overview for new developers
./pm_encoder.py . --lens onboarding -o onboarding.txt
```

### Lens Transparency

When using a lens, pm_encoder injects a `.pm_encoder_meta` file at the start of output:

```
++++++++++ .pm_encoder_meta ++++++++++
Context generated with lens: "architecture"
Focus: High-level structure, interfaces, configuration

Implementation details truncated using structure mode
Output shows only:
  - Import/export statements
  - Class and function signatures
  - Type definitions and interfaces
  - Module-level documentation

Generated: 2025-12-12T22:38:43.850133
pm_encoder version: 1.2.0
---------- .pm_encoder_meta ... ----------
```

This ensures LLMs understand how the context was filtered.

### Structure Mode Truncation

Structure mode (used by `architecture` lens) shows only signatures:

**Original file (100 lines):**
```python
import os
from pathlib import Path

class FileProcessor:
    def __init__(self, root_dir):
        self.root = root_dir
        self.cache = {}

    def process_file(self, file_path):
        # 50 lines of implementation
        ...
        return result
```

**Structure mode output (~10 lines):**
```python
import os
from pathlib import Path

class FileProcessor:
    def __init__(self, root_dir):
    def process_file(self, file_path):

======================================================================
STRUCTURE MODE: Showing only signatures
Included: imports, class/function signatures, type definitions
Excluded: function bodies, implementation details
======================================================================
```

### Custom Lenses

Define custom lenses in `.pm_encoder_config.json`:

```json
{
  "lenses": {
    "frontend": {
      "description": "Frontend components and styles",
      "include": ["src/components/**/*.tsx", "src/styles/**/*.css"],
      "exclude": ["*.test.tsx"],
      "truncate_mode": "smart",
      "truncate": 400,
      "sort_by": "name"
    },
    "api": {
      "description": "Backend API endpoints",
      "include": ["api/**/*.py", "models/**/*.py"],
      "exclude": ["tests/**"],
      "truncate_mode": "structure",
      "sort_by": "name"
    }
  }
}
```

Then use them:
```bash
./pm_encoder.py . --lens frontend -o frontend_context.txt
./pm_encoder.py . --lens api -o api_structure.txt
```

### Lens Precedence

Configuration is merged with layered precedence (highest to lowest):
1. **CLI flags** (e.g., `--include`, `--exclude`)
2. **Lens settings** (from `--lens`)
3. **Config file** (`.pm_encoder_config.json`)
4. **Defaults** (built-in patterns)

Example:
```bash
# Lens sets structure mode, but CLI overrides to smart
./pm_encoder.py . --lens architecture --truncate-mode smart
```

---

## Configuration

You can control the default behavior of the script by placing a `.pm_encoder_config.json` file in your project's root directory.

- `ignore_patterns`: A list of glob patterns. Any file or directory matching these patterns will be completely ignored. This is useful for build artifacts, virtual environments, and version control folders.
- `include_patterns`: A list of glob patterns. If this list is not empty, **only** files matching these patterns will be included in the output.

### Example `.pm_encoder_config.json`

```json
{
  "ignore_patterns": [
    ".git",
    ".idea",
    ".vscode",
    "target",
    "build",
    "dist",
    ".venv",
    "__pycache__",
    "*.pyc",
    "*.log",
    "*.swp",
    "*.bak",
    "*.tmp",
    "node_modules",
    "recordings.db*",
    "compare_output"
  ],
  "include_patterns": [
    "*.rs",
    "*.toml",
    "*.md",
    "*.py",
    "*.sh",
    "*.json",
    "*.xml",
    "*.txt",
    "Dockerfile",
    "LICENSE"
  ]
}
```
---------- README.md 813acc14ba5ea77e7287435375ef4e33 README.md ----------
++++++++++ SYSTEM_INSTRUCTIONS.md ++++++++++
# System Instructions for pm_encoder
**Version: 1.0-pm_encoder**
**Generated: 2025-12-12**
**Language: Python 3**
**Protocol: AI Collaboration Protocol v2.0**

---

## Core Philosophy

pm_encoder is a **meta-tool for AI collaboration** â€” it exists to facilitate effective context sharing between developers and AI assistants. As such, development must be:

1. **Self-Aware**: Recognize that changes to pm_encoder affect how developers collaborate with AI systems
2. **Format-Preserving**: The Plus/Minus format is the core contract; any changes must maintain backward compatibility
3. **Utility-Focused**: Every feature must solve a real context-sharing pain point
4. **Dogfooded**: Use pm_encoder itself to share context during its own development

### The Meta-Tool Paradox

pm_encoder serializes projects for AI consumption, including itself. This creates a recursive relationship where:
- We use AI to develop pm_encoder
- pm_encoder helps us share context with AI
- Changes to pm_encoder affect how we use AI for its development

**Implication**: Every modification must consider its impact on the AI collaboration workflow.

---

## Session Management Protocol

### Session Identification
Each AI response in pm_encoder development begins with:
```
Session: 2025-12-12 | pm_encoder-a7c3f | Turn: 1
Context: [serialized|partial|minimal]
```

**Components**:
- **Date**: ISO format (YYYY-MM-DD)
- **Hash**: First 5 characters of initial prompt SHA-256
- **Turn**: Sequential number within session (resets each session)
- **Context**: Level of project context provided
  - `serialized` - Full pm_encoder context via its own output
  - `partial` - Specific files/modules only
  - `minimal` - Working from memory/documentation only

### Task Classification

Prefix requests with appropriate tags to set expectations:

| Prefix | Purpose | Example |
|--------|---------|---------|
| `format:` | Plus/Minus format changes/fixes | `format: Fix checksum calculation for UTF-8 edge cases` |
| `feature:` | New capability | `feature: Add --depth flag for shallow serialization` |
| `fix:` | Bug resolution | `fix: Binary detection fails on certain file types` |
| `improve:` | Refactoring/optimization | `improve: Reduce memory footprint for large repos` |
| `document:` | Documentation updates | `document: Add migration guide from v1 to v2` |
| `test:` | Test creation/modification | `test: Add integration tests for config handling` |
| `sync:` | Context update/alignment | `sync: Update context with latest main branch changes` |
| `meta:` | Meta-development discussion | `meta: Should we support streaming output?` |

### Context Generation for Sessions

When starting a development session, generate fresh context:

```bash
# Full context (preferred for major changes)
./pm_encoder.py . -o pm_encoder_full_context.txt

# Targeted context (for specific module work)
./pm_encoder.py . --include "pm_encoder.py" "tests/**" "README.md" \
    -o pm_encoder_partial_context.txt

# Review context (for code review)
./pm_encoder.py . --exclude "tests/" "docs/examples/" \
    -o pm_encoder_review_context.txt
```

---

## Development Workflow

### Iterative Enhancement Cycle

pm_encoder follows a pragmatic, iterative approach:

1. **Identify** - Document the specific context-sharing pain point
2. **Design** - Sketch solution maintaining format compatibility
3. **Implement** - Write code with inline documentation
4. **Dogfood** - Use the new feature to serialize pm_encoder itself
5. **Document** - Update README/TUTORIAL with examples
6. **Test** - Verify edge cases and backward compatibility

### Plus/Minus Format Contract

The Plus/Minus format is **sacred** and changes require high consensus:

```
++++++++++ [relative/path/to/file.ext] ++++++++++
[file content, preserved exactly as-is]
---------- [relative/path/to/file.ext] [MD5_checksum] [relative/path/to/file.ext] ----------
```

**Format Rules**:
1. **Delimiters**: Exactly 10 plus signs, 10 minus signs
2. **Paths**: POSIX-style, relative to project root, repeated in footer
3. **Checksum**: MD5 of UTF-8 encoded content, hex lowercase
4. **Newlines**: Content must end with newline; footer adds one if missing
5. **Encoding**: All content UTF-8, fallback to latin-1 if needed

**Backward Compatibility Promise**:
- Any AI system trained on current format must work with future versions
- Format extensions (e.g., metadata) must use optional comment syntax
- Breaking changes require major version bump and migration tool

### Assumption Handling

Given pm_encoder's utility focus, handle assumptions with **mark and proceed**:

```python
# ASSUMPTION: File paths under 4096 chars (PATH_MAX on most systems)
# CONFIDENCE: 95% - extremely rare edge case
# TODO-VERIFY: Test on Windows with \\?\ long path prefix

# ASSUMPTION: Projects under 100K files remain performant
# CONFIDENCE: 80% - may need optimization for monorepos
# PERF-DEBT: Current O(n log n) sort acceptable; consider streaming for massive repos
```

**Confidence Thresholds**:
- **95-100%**: Proceed without comment unless educationally valuable
- **80-94%**: Mark assumption, proceed with implementation
- **50-79%**: Mark assumption, implement with TODO for verification
- **<50%**: Flag for explicit user decision before proceeding

### Documentation Requirements

pm_encoder serves both developers and end-users:

| Change Type | Documentation Required |
|-------------|------------------------|
| Format modification | Update format spec + migration guide |
| New CLI flag | README usage + TUTORIAL example |
| Config option | README config section + JSON schema example |
| Performance change | Benchmark results + scale recommendations |
| Breaking change | CHANGELOG entry + migration path |
| Bug fix | Git commit message + close related issues |

**Style**: Documentation should be **example-driven** and **copy-pasteable**.

---

## Code Generation Standards

### Python Best Practices

pm_encoder targets **Python 3.6+** with minimal dependencies:

- **Typing**: Use type hints for public APIs and complex functions
- **Pathlib**: Prefer `Path` over `os.path` for all file operations
- **Error Handling**: Fail gracefully; never crash on bad input
- **Encoding**: UTF-8 first, latin-1 fallback, skip binary explicitly
- **Performance**: Optimize for common case (1K-10K files, <100MB total)
- **Dependencies**: Standard library only (no external packages)

### Quality Checklist

Before submitting code, verify:

- [ ] **Format Compliance**: Output passes Plus/Minus format validator
- [ ] **Self-Serialization**: `./pm_encoder.py . -o test.txt` succeeds
- [ ] **Cross-Platform**: Works on Linux, macOS, Windows (POSIX paths only in output)
- [ ] **Edge Cases**: Handles empty dirs, symlinks, permission errors
- [ ] **Encoding Safety**: Non-UTF8 files skip gracefully
- [ ] **Performance**: <5s for 10K files on modest hardware
- [ ] **Documentation**: README updated, TUTORIAL example if user-facing
- [ ] **Backward Compat**: Existing encoded outputs still parse correctly

### Deliverable Format

Structure your response as:

```markdown
## Implementation Summary
[Brief description of changes and rationale]

## Files Modified
- `pm_encoder.py` - [Specific changes]
- `README.md` - [Documentation updates]
- `tests/test_*.py` - [New tests added]

## Key Changes
1. [Major modification with technical detail]
2. [Impact on format/API/behavior]
3. [Performance implications if any]

## Testing Performed
- [x] Self-serialization test passed
- [x] Format validation passed
- [x] Tested on sample projects: [list]
- [ ] Windows testing pending (if applicable)

## Breaking Changes
[None] OR [Specific breaking change + migration path]

## Follow-Up Items
- [ ] [Remaining TODOs or improvements identified]
```

---

## Continuous Improvement Protocol

### Session-End Improvement

After each development session, reflect:

```markdown
### Process Improvement
**Observation:** [What friction did we encounter?]
**Suggestion:** [Specific improvement to workflow/tool/docs]
**Benefit:** [Expected impact on future development]
**Effort:** [S/M/L estimation]
```

### Friction Reporting

pm_encoder development should be smooth. Report friction points:

```markdown
**Friction [Confidence: 85%]**: Testing format changes requires manual diff checking
**Impact**: Slows iteration on format-related features
**Suggested Resolution**: Create automated format validator/differ tool
**Priority**: Medium - affects development velocity
```

### Pattern Recognition

Track recurring patterns for abstraction opportunities:

**Threshold**: After 2+ occurrences, propose abstraction

**Example Pattern**:
```python
# Pattern observed 3x: Path filtering with multiple criteria
# Current: Repeated fnmatch logic in multiple functions
# Proposal: PathFilter class with builder pattern
# Reuse potential: 4 current call sites + future features
# Complexity: Low - ~100 LOC, no external deps
```

### Meta-Tool Self-Improvement

Unique to pm_encoder: Use itself to improve itself

**Self-Application Checklist**:
- [ ] After major refactor: Serialize and compare output before/after
- [ ] Before release: Generate context and review with fresh AI session
- [ ] For new features: Dogfood by encoding example projects
- [ ] Documentation: Include pm_encoder's own output as example

---

## Context Requirements

### Essential Context Files

For comprehensive development sessions, include:

```bash
./pm_encoder.py . --include \
    "pm_encoder.py" \
    "README.md" \
    "TUTORIAL.md" \
    "CHANGELOG.md" \
    ".pm_encoder_config.json" \
    "tests/**/*.py" \
    "scripts/*.sh" \
    -o context.txt
```

**Minimal Context** (for quick fixes):
```bash
./pm_encoder.py . --include "pm_encoder.py" "README.md" -o context.txt
```

### State Awareness

Track pm_encoder project state:

- **Current Version**: Check `__version__` in pm_encoder.py or git tags
- **Open Issues**: GitHub issues labeled `bug`, `enhancement`, `format-spec`
- **Roadmap**: See ROADMAP.md or CHANGELOG.md "Planned" section
- **Technical Debt**: Search codebase for `TODO`, `FIXME`, `PERF-DEBT`, `TECH-DEBT`

### Configuration Files

pm_encoder's own configuration:

```json
{
  "ignore_patterns": [
    ".git",
    "__pycache__",
    "*.pyc",
    ".venv",
    "venv",
    ".pytest_cache",
    "*.egg-info",
    "dist",
    "build",
    ".DS_Store",
    "*.swp",
    "*.swo"
  ],
  "include_patterns": []
}
```

**Location**: `.pm_encoder_config.json` in project root

---

## Session Handoff Protocol

### End-of-Session Summary

When closing a development session (or on `sync: end-session`), provide:

```markdown
## Session Summary
**Session ID:** 2025-12-12 | pm_encoder-a7c3f | Turn: 12
**Duration:** ~2 hours
**Context Mode:** Serialized (full project)

### Completed
- Implemented `--depth` flag for shallow serialization
- Added integration tests for new flag
- Updated README and TUTORIAL with examples
- Fixed edge case in path handling for deeply nested dirs

### Decisions Made
1. **Depth Limit**: Defaulted to unlimited (None), user must specify --depth N explicitly
2. **Behavior**: Directories at depth limit are skipped entirely (not listed as empty)
3. **Error Handling**: Log warning if depth truncates expected files

### Pending Tasks
- [ ] Windows path testing for new depth feature
- [ ] Performance benchmark with --depth on large monorepo
- [ ] Consider adding --max-files limit (related feature)

### Next Steps
1. User testing: Try --depth on real-world projects, gather feedback
2. Documentation review: Ensure examples are clear
3. Release preparation: Update CHANGELOG, bump version to 1.2.0

### Context Updates
**Files Changed:**
- `pm_encoder.py` - Added depth tracking and limiting logic
- `tests/test_serialization.py` - New tests for depth feature
- `README.md` - Usage section updated
- `TUTORIAL.md` - Added depth limiting example

**New Context Generated:**
```bash
./pm_encoder.py . -o pm_encoder_v1.2.0-dev.txt
```

### Improvement Opportunities Identified
1. Test suite could benefit from parameterized tests (reduce duplication)
2. Consider adding pre-commit hooks for format validation
3. Documentation examples could be more diverse (not just Python projects)
```

### Cross-Session Continuity

To resume development in a new session:

1. **Load Latest Context**: Use most recent serialized output
2. **Review Session Summary**: Read last session's handoff notes
3. **Check Git Status**: `git status` and `git log -5 --oneline`
4. **Verify State**: Run `./pm_encoder.py . -o /tmp/test.txt` to ensure working state
5. **Declare Intent**: Start with `sync:` command explaining continuation

---

## Special Directives

### Format Specification Changes

Any modification to the Plus/Minus format requires:

1. **Proposal Document**: RFC-style doc explaining change and rationale
2. **Compatibility Analysis**: Impact on existing parsers/consumers
3. **Migration Path**: How existing encoded files remain valid
4. **Version Strategy**: When to bump major vs minor version
5. **Community Input**: If pm_encoder gains users, solicit feedback

### Performance Targets

pm_encoder should be "fast enough to not think about":

| Project Size | Target Time | Notes |
|--------------|-------------|-------|
| <1K files | <1 second | Near-instant feedback |
| 1K-10K files | <5 seconds | Typical mid-size project |
| 10K-50K files | <30 seconds | Large monorepo warning territory |
| >50K files | <5 minutes | Consider pagination/filtering |

**Optimization Priority**: O(n) operations on file list, minimize filesystem calls

### Testing Philosophy

Given the tool's simplicity, balance test coverage with practicality:

- **Unit Tests**: Core functions (filtering, sorting, format generation)
- **Integration Tests**: Full serialization workflows with sample projects
- **Edge Case Tests**: Empty dirs, symlinks, permissions, encoding issues
- **Regression Tests**: Known bugs should have test cases
- **Performance Tests**: Benchmark on synthetic large repos (optional)

**No Testing**: CLI argument parsing (argparse handles), trivial getters/setters

### Dependency Policy

pm_encoder is **standard library only** by design:

**Rationale**:
- Easy installation: Just download pm_encoder.py
- No dependency hell: Works everywhere Python 3.6+ is installed
- Portability: Single file can be copied/shared easily
- Trust: Users can audit ~250 LOC without external deps

**Exception Process**:
If a dependency is absolutely necessary:
1. Justify why standard library cannot solve it
2. Assess dependency health (active maintenance, security record)
3. Consider vendoring (copy into project) for small deps
4. Update installation docs and add `requirements.txt`
5. Requires 2+ maintainer consensus

### User-Facing Language

pm_encoder serves developers of varying experience:

- **Error Messages**: Actionable, suggest fix, include example
- **Help Text**: Examples before flags, common workflows prominent
- **Documentation**: Progressive disclosure (Quick Start â†’ Tutorial â†’ Reference)
- **Defaults**: Safe and intuitive; power users can customize

**Tone**: Friendly but technical. Assume user is competent but may be time-pressured.

---

## Meta-Development Notes

### pm_encoder's Development Meta-Pattern

Developing pm_encoder involves this workflow:

```
1. Work on feature/fix
2. Serialize project with pm_encoder itself
3. Share context with AI for review/next steps
4. AI uses pm_encoder's output to understand current state
5. Iterate on feedback
6. Repeat
```

This creates a **positive feedback loop**: Improvements to pm_encoder improve the experience of developing pm_encoder.

### Questions to Ask When Unsure

When facing design decisions:

1. **Does this help developers share better context with AI?**
2. **Does this maintain format backward compatibility?**
3. **Can this be implemented without external dependencies?**
4. **Will this scale to projects with 10K+ files?**
5. **Is the CLI interface intuitive for first-time users?**
6. **Would I use this feature in my own workflow?**

If answers lean negative, reconsider or simplify.

### Vision Alignment

pm_encoder exists to solve: **"How do I give an AI the full context of my project efficiently?"**

Features should support this by:
- Making context more complete (better filtering, less manual curation)
- Making context more efficient (compression, relevance ranking)
- Making context more maintainable (config files, reproducible commands)

Features that don't serve this vision should be questioned.

---

## Conclusion

pm_encoder is simple by design, powerful by utility. When developing it:

- **Respect the format**: It's the contract with all users and AI systems
- **Use the tool**: Dogfood relentlessly
- **Stay minimal**: Features should earn their complexity
- **Document generously**: Examples over explanations
- **Think recursively**: This tool enables its own development

Every change to pm_encoder ripples through the AI collaboration ecosystem. Develop with care, test thoroughly, and document clearly.

---

**Protocol**: AI Collaboration Protocol v2.0-Universal  
**Last Updated**: 2025-12-12  
**Maintainer**: Review and update as pm_encoder evolves  
**Feedback**: Use pm_encoder's own output to share context when discussing improvements to these instructions
---------- SYSTEM_INSTRUCTIONS.md af1d963d5105438ccf5c427a29fb381c SYSTEM_INSTRUCTIONS.md ----------
++++++++++ TESTING.md ++++++++++
# Testing Guide for pm_encoder

This document explains how to run tests, add new tests, and maintain the >98% coverage requirement.

## Quick Start

```bash
# Run all tests
make test

# Run tests with coverage report
make coverage

# Run all quality checks
make quality

# Run CI pipeline locally
make ci
```

## Current Test Coverage

**Target**: >98% code coverage
**Current**: Check with `make coverage`

Coverage report locations:
- Terminal output: `make coverage`
- HTML report: `htmlcov/index.html` (after running `make coverage`)

## Test Structure

```
tests/
â”œâ”€â”€ test_pm_encoder.py        # Core functionality tests (10 tests)
â”œâ”€â”€ test_comprehensive.py      # Comprehensive coverage tests (coming soon)
â”œâ”€â”€ test_documentation.py      # Documentation sync tests (coming soon)
â””â”€â”€ fixtures/                  # Test data
    â”œâ”€â”€ python/
    â”œâ”€â”€ javascript/
    â”œâ”€â”€ rust/
    â”œâ”€â”€ shell/
    â”œâ”€â”€ markdown/
    â”œâ”€â”€ yaml/
    â”œâ”€â”€ json/
    â””â”€â”€ edge_cases/
```

## Running Tests

### Basic Test Commands

```bash
# Run all tests (verbose)
python3 -m unittest discover -s tests -p 'test_*.py' -v

# Run all tests (quiet)
python3 -m unittest discover -s tests -p 'test_*.py'

# Run specific test file
python3 -m unittest tests.test_pm_encoder

# Run specific test class
python3 -m unittest tests.test_pm_encoder.TestStructureMode

# Run specific test
python3 -m unittest tests.test_pm_encoder.TestStructureMode.test_structure_mode_trigger
```

### Coverage Commands

```bash
# Run with coverage
python3 -m coverage run -m unittest discover -s tests -p 'test_*.py'

# Show coverage report
python3 -m coverage report -m

# Generate HTML report
python3 -m coverage html

# Check if coverage meets 98% threshold
python3 -m coverage report --fail-under=98
```

## Coverage Requirements

### Minimum Coverage

- **Overall**: >98% of pm_encoder.py must be covered
- **Per-file**: No file should drop below 95%
- **Critical paths**: 100% coverage on:
  - All language analyzers
  - Lens system
  - Structure mode logic
  - CLI argument parsing

### What Doesn't Count

Coverage excludes:
- Lines with `# pragma: no cover`
- `if __name__ == "__main__"` blocks (tested via integration)
- Defensive error handling for impossible states

## Writing Tests

### Test Template

```python
import unittest
from pathlib import Path
from io import StringIO
import pm_encoder

class TestMyFeature(unittest.TestCase):
    """Test description."""

    def setUp(self):
        """Set up test fixtures."""
        self.test_data = "sample"

    def tearDown(self):
        """Clean up after tests."""
        pass

    def test_basic_functionality(self):
        """Test basic feature behavior."""
        result = pm_encoder.my_function(self.test_data)
        self.assertEqual(result, expected_value)

    def test_edge_case(self):
        """Test edge case handling."""
        with self.assertRaises(ValueError):
            pm_encoder.my_function(invalid_input)
```

### Test Categories

1. **Unit Tests**: Test individual functions/classes in isolation
2. **Integration Tests**: Test component interactions
3. **Edge Case Tests**: Test boundary conditions, errors, empty inputs
4. **Regression Tests**: Prevent bugs from recurring

### Testing Checklist

When adding a new feature:
- [ ] Add unit tests for new functions/classes
- [ ] Add integration tests for feature workflows
- [ ] Add edge case tests (empty, None, invalid)
- [ ] Update coverage to maintain >98%
- [ ] All tests pass: `make test`
- [ ] Coverage check passes: `make coverage-check`

## Test Fixtures

### Using Fixtures

Test fixtures are in `tests/fixtures/`:

```python
def test_python_analysis(self):
    """Test Python file analysis."""
    fixture_path = Path(__file__).parent / "fixtures" / "python" / "sample.py"
    content = fixture_path.read_text()

    analyzer = pm_encoder.PythonAnalyzer()
    result = analyzer.analyze(content, fixture_path)

    self.assertIn("main", result["functions"])
```

### Creating Fixtures

To add a new fixture:

1. Create file in appropriate `fixtures/` subdirectory
2. Make it realistic but minimal
3. Cover common patterns for that file type
4. Include edge cases if relevant

Example:
```bash
# Create new Python fixture
echo 'def test(): pass' > tests/fixtures/python/minimal.py
```

## Common Testing Patterns

### Testing File Processing

```python
def test_file_serialization(self):
    """Test file serialization."""
    with tempfile.TemporaryDirectory() as tmpdir:
        test_file = Path(tmpdir) / "test.py"
        test_file.write_text("print('hello')")

        output = StringIO()
        pm_encoder.serialize(
            Path(tmpdir),
            output,
            ignore_patterns=[],
            include_patterns=[],
            sort_by="name",
            sort_order="asc"
        )

        result = output.getvalue()
        self.assertIn("++++++++++ test.py ++++++++++", result)
```

### Testing Language Analyzers

```python
def test_analyzer_detects_functions(self):
    """Test function detection."""
    code = """
    def foo():
        pass

    async def bar():
        pass
    """

    analyzer = pm_encoder.PythonAnalyzer()
    lines = code.split('\n')
    result = analyzer.analyze_lines(lines, Path("test.py"))

    self.assertIn("foo", result["functions"])
    self.assertIn("bar", result["functions"])
```

### Testing Structure Mode

```python
def test_structure_preserves_signatures(self):
    """Test structure mode keeps signatures only."""
    code = """
    def process(x):
        result = x * 2
        return result
    """

    analyzer = pm_encoder.PythonAnalyzer()
    lines = code.split('\n')
    ranges = analyzer.get_structure_ranges(lines)

    # Extract kept lines
    kept = []
    for start, end in ranges:
        kept.extend(lines[start-1:end])

    output = '\n'.join(kept)
    self.assertIn("def process(x):", output)
    self.assertNotIn("result = x * 2", output)
```

## Debugging Failed Tests

### Verbose Output

```bash
# Run with verbose output
python3 -m unittest tests.test_pm_encoder -v

# Show print statements
python3 -m unittest tests.test_pm_encoder 2>&1 | cat
```

### Isolate Failing Test

```bash
# Run only the failing test
python3 -m unittest tests.test_pm_encoder.TestClass.test_method -v

# Add debug prints
def test_failing_case(self):
    result = function_under_test()
    print(f"DEBUG: result = {result}")  # Add this
    self.assertEqual(result, expected)
```

### Coverage Gaps

```bash
# Generate HTML coverage report
make coverage

# Open htmlcov/index.html in browser
# Red lines = not covered
# Green lines = covered
```

## Continuous Integration

Tests run automatically on:
- Every push to GitHub
- Every pull request
- Pre-commit hooks (if configured)

### CI Requirements

All of these must pass:
- [ ] All unit tests pass
- [ ] Coverage â‰¥98%
- [ ] No syntax errors (`make lint`)
- [ ] Self-serialization works (`make self-serialize`)
- [ ] Documentation synchronized (`make docs`)

### Running CI Locally

Before pushing:
```bash
# Run full CI pipeline
make ci

# If this passes, your PR will likely pass CI
```

## Performance Testing

### Benchmarking

```python
import time

def test_performance_large_file(self):
    """Test performance on large files."""
    large_content = "line\n" * 10000

    start = time.time()
    pm_encoder.truncate_content(
        large_content,
        Path("large.py"),
        max_lines=500,
        mode="smart",
        analyzer_registry=pm_encoder.LanguageAnalyzerRegistry(),
        include_summary=True
    )
    elapsed = time.time() - start

    self.assertLess(elapsed, 1.0)  # Should complete in <1 second
```

## Troubleshooting

### Coverage Not Installing

```bash
pip3 install coverage
# or
make install-dev
```

### Tests Pass But Coverage Fails

```bash
# Check what's not covered
make coverage

# Look at htmlcov/index.html for details
# Red lines need test coverage
```

### Import Errors

```bash
# Ensure you're in the project root
cd /path/to/pm_encoder

# Run tests from project root
python3 -m unittest discover -s tests
```

## Best Practices

1. **Test Names**: Use descriptive names that explain what's being tested
2. **One Assert Per Test**: Each test should verify one specific behavior
3. **Independent Tests**: Tests should not depend on each other
4. **Fast Tests**: Keep tests fast (<5s total runtime)
5. **Deterministic**: Tests should always produce the same result
6. **Readable**: Anyone should understand what the test does

## Contributing Tests

When contributing:

1. Run `make test` locally - all must pass
2. Run `make coverage-check` - must be â‰¥98%
3. Add tests for new features
4. Add tests for bug fixes (regression tests)
5. Document tricky test logic with comments

## Questions?

- Check existing tests in `tests/test_pm_encoder.py` for examples
- See `CONTRIBUTING.md` for contribution guidelines
- Open an issue with the `testing` label

---

**Remember**: Tests are documentation. Write tests that explain how the code should behave.
---------- TESTING.md b0e90133a38b7e92c280181d581b7aea TESTING.md ----------
++++++++++ TUTORIAL.md ++++++++++
# PM Encoder Tutorial

This tutorial walks you through using `pm_encoder.py` to serialize your project files for sharing with LLMs or team members.

## Quick Start

### Step 1: Make the script executable
```bash
chmod +x pm_encoder.py
```

### Step 2: Run your first encoding
```bash
# Serialize current directory and copy to clipboard (macOS)
./pm_encoder.py . | pbcopy

# Or save to a file
./pm_encoder.py . -o my_project_context.txt
```

That's it! Your project files are now serialized in the Plus/Minus format.

## Understanding the Output Format

The tool creates output like this:
```
++++++++++ src/main.py ++++++++++
#!/usr/bin/env python3
print("Hello, world!")
---------- src/main.py a1b2c3d4e5f6... src/main.py ----------

++++++++++ README.md ++++++++++
# My Project
This is a sample project.
---------- README.md f6e5d4c3b2a1... README.md ----------
```

- Files are wrapped with clear start (`++++`) and end (`----`) markers
- End markers include MD5 checksums for integrity verification
- Binary files are automatically skipped

## Practical Examples

### Example 1: Python Project Context for ChatGPT
```bash
# Include only Python files and docs
./pm_encoder.py . --include "*.py" "*.md" "requirements.txt" | pbcopy
```

### Example 2: Debugging - Focus on Core Files
```bash
# Exclude tests and build artifacts
./pm_encoder.py . --exclude "tests/" "*.pyc" "__pycache__" -o debug_context.txt
```

### Example 3: Code Review Package
```bash
# Create focused context for a specific module
./pm_encoder.py . --include "src/auth/**" "tests/test_auth.py" -o auth_review.txt
```

## Configuration File Setup

Create `.pm_encoder_config.json` in your project root:

```json
{
  "ignore_patterns": [
    ".git", ".venv", "__pycache__", "*.pyc", "node_modules"
  ],
  "include_patterns": [
    "*.py", "*.js", "*.md", "*.json", "*.yaml", "*.txt"
  ]
}
```

Now you can simply run:
```bash
./pm_encoder.py .
```

## Tips & Best Practices

### 1. Start with Defaults
The tool has sensible defaults - try it without configuration first:
```bash
./pm_encoder.py . | head -20  # Preview first 20 lines
```

### 2. Use Previews
Check what files will be included before generating large outputs:
```bash
# Dry run - see which files match your patterns
find . -name "*.py" | grep -v __pycache__ | head -10
```

### 3. Size Management
Large projects can create huge outputs. Use filters:
```bash
# Focus on specific directories
./pm_encoder.py . --include "src/**" "docs/**"

# Or exclude heavy directories
./pm_encoder.py . --exclude "data/" "logs/" "*.log"
```

### 4. Clipboard Integration
Set up aliases for common workflows:
```bash
# Add to your .bashrc or .zshrc
alias encode-py="./pm_encoder.py . --include '*.py' '*.md' | pbcopy"
alias encode-all="./pm_encoder.py . | pbcopy"
```

## Common Workflows

### Workflow 1: LLM Context Generation
```bash
# 1. Navigate to your project
cd /path/to/my-project

# 2. Generate context with relevant files only
./pm_encoder.py . --include "*.py" "*.js" "*.md" "package.json" | pbcopy

# 3. Paste into ChatGPT/Claude with your question
```

### Workflow 2: Code Review Preparation
```bash
# 1. Create focused context for reviewers
./pm_encoder.py . --exclude "tests/" "docs/" "*.log" -o review_context.txt

# 2. Share the .txt file with your team
```

### Workflow 3: Bug Investigation
```bash
# 1. Include only files related to the problematic feature
./pm_encoder.py . --include "src/feature_x/**" "tests/test_feature_x.py" -o bug_context.txt

# 2. Attach to your bug report
```

## Troubleshooting

### "Permission denied" error
```bash
# Make sure script is executable
chmod +x pm_encoder.py
ls -la pm_encoder.py  # Should show -rwxr-xr-x
```

### Empty output
```bash
# Check if your include patterns are too restrictive
./pm_encoder.py . --include "**"  # Include everything (except ignored)
```

### Large output size
```bash
# Add more exclusions to your config
echo '{"ignore_patterns": [".git", "node_modules", "*.log", "data/"]}' > .pm_encoder_config.json
```

## Token Optimization (v1.1+)

### Why Truncation?

When working with large projects, you may encounter:
- **Token limits** in LLMs (e.g., ChatGPT's context window)
- **Cost concerns** with token-based pricing
- **Performance issues** with very large contexts

Intelligent truncation solves these problems while keeping the most valuable code.

### Example 4: Smart Truncation for LLM Context

```bash
# Truncate files to 500 lines each, using language-aware smart mode
./pm_encoder.py . --truncate 500 --truncate-mode smart | pbcopy
```

This command:
1. Analyzes each file's language (Python, JS, etc.)
2. Preserves imports, class/function signatures, and entry points
3. Adds helpful summaries showing what was truncated
4. Reduces token usage by 60-80% typically

### Example 5: Truncation with Statistics

```bash
# See exactly how much you're saving
./pm_encoder.py . --truncate 300 --truncate-stats -o context.txt
```

Output:
```
======================================================================
TRUNCATION REPORT
======================================================================
Files analyzed: 45
Files truncated: 12 (26%)
Lines: 18,234 â†’ 6,891 (62% reduction)

By Language:
  Python: 25 files, 8 truncated
  JavaScript/TypeScript: 15 files, 3 truncated
  Markdown: 5 files, 1 truncated

Estimated tokens: ~68,000 â†’ ~26,000 (61% reduction)
======================================================================
```

### Example 6: Protect Important Files from Truncation

```bash
# Don't truncate README or LICENSE files
./pm_encoder.py . --truncate 400 \
  --truncate-exclude "README.md" "LICENSE" "CHANGELOG.md" \
  -o context.txt
```

### Understanding Smart vs Simple Truncation

**Simple Mode** (`--truncate-mode simple`):
- Just keeps the first N lines
- Fast, predictable
- Good for uniform files

**Smart Mode** (`--truncate-mode smart`):
- Analyzes file language and structure
- Preserves critical sections (imports, signatures, entry points)
- Adds detailed summaries
- Best for code understanding

Example smart truncation output:

```python
++++++++++ api/auth.py [TRUNCATED: 823 lines] ++++++++++
import jwt
from fastapi import HTTPException
from models import User

class AuthService:
    def __init__(self, secret_key: str):
        self.secret = secret_key

    def create_token(self, user_id: int) -> str:
        """Generate JWT token for user."""
        # ... (implementation details)

    def verify_token(self, token: str) -> dict:
        """Verify and decode JWT token."""
        # ... (implementation details)

... [650 lines omitted] ...

if __name__ == "__main__":
    # Development testing
    service = AuthService(os.getenv("SECRET_KEY"))
    print("Auth service initialized")

======================================================================
TRUNCATED at line 500/823 (39% reduction)
Language: Python
Category: Application Module
Classes (3): AuthService, TokenManager, SessionHandler
Functions (15): create_token, verify_token, refresh_token, ...
Key imports: jwt, fastapi, models, bcrypt, datetime

To get full content: --include "api/auth.py" --truncate 0
======================================================================
---------- api/auth.py [TRUNCATED:823â†’173] a1b2c3d4... ----------
```

### Example 7: Combining Filters and Truncation

```bash
# Complex workflow: Python files only, moderate truncation
./pm_encoder.py . \
  --include "src/**/*.py" "tests/**/*.py" \
  --truncate 600 \
  --truncate-mode smart \
  --truncate-exclude "tests/fixtures/*" \
  -o python_context.txt
```

### Workflow 4: Large Codebase to LLM

```bash
# 1. First, check the size without truncation
./pm_encoder.py . --include "*.py" "*.js" | wc -l
# Output: 45,000 lines (too big!)

# 2. Apply smart truncation
./pm_encoder.py . \
  --include "*.py" "*.js" \
  --truncate 400 \
  --truncate-mode smart \
  --truncate-stats \
  | tee context.txt | pbcopy

# 3. Review stats (stderr shows the report)
# Files truncated: 32/67 (47%)
# Estimated tokens: ~180K â†’ ~52K (71% reduction)

# 4. Now paste into Claude/ChatGPT with your question
```

### Workflow 5: Iterative Context Refinement

```bash
# Start broad with heavy truncation
./pm_encoder.py . --truncate 200 --truncate-mode smart -o initial.txt

# Review truncation report, then get full version of specific files
./pm_encoder.py . \
  --include "src/critical_module.py" "src/another_key_file.py" \
  --truncate 0 \
  -o details.txt

# Combine both for hybrid context
```

### Tips for Token Optimization

**1. Right-size your truncation limit**
```bash
# Too aggressive (may lose important context)
./pm_encoder.py . --truncate 50

# Too conservative (may still hit limits)
./pm_encoder.py . --truncate 2000

# Sweet spot for most projects
./pm_encoder.py . --truncate 300-500
```

**2. Use smart mode for code, simple for data**
```bash
# Smart for source code
./pm_encoder.py src/ --truncate 500 --truncate-mode smart -o code.txt

# Simple for logs or data files
./pm_encoder.py logs/ --truncate 100 --truncate-mode simple -o logs.txt
```

**3. Exclude documentation from truncation**
```bash
# Keep full README and documentation
./pm_encoder.py . --truncate 400 --truncate-exclude "*.md" "docs/**"
```

**4. Preview before committing**
```bash
# Test truncation on a subset first
./pm_encoder.py src/critical_module/ --truncate 300 --truncate-mode smart
```

### Advanced: Language-Specific Truncation

Different languages are analyzed differently:

- **Python**: Preserves imports, class/function signatures, `__main__` blocks
- **JavaScript/TypeScript**: Preserves imports, exports, function/class declarations
- **Markdown**: Keeps all headers, first paragraph of each section
- **JSON/YAML**: Preserves structure, shows key names, samples values
- **Shell**: Keeps functions, sourced files, shebang

The smart mode automatically adapts to each language!

## Context Lenses (v1.2.0)

### What Are Context Lenses?

Context Lenses are pre-configured profiles that combine filters, sorting, and truncation strategies for specific use cases. Instead of manually setting multiple flags, you use one lens name.

### Example 8: Architecture Overview

Get a high-level view of your codebase structure:

```bash
# Shows only signatures - perfect for understanding APIs
./pm_encoder.py . --lens architecture | pbcopy
```

This automatically:
- Excludes tests, docs, and assets
- Uses structure mode (signatures only)
- Sorts by name for organized browsing
- Focuses on code structure, not implementation

**Perfect for:**
- Understanding a new codebase
- Sharing project structure with LLMs
- API documentation
- Code reviews focused on interfaces

### Example 9: Debug Session

Quickly grab recent changes for debugging:

```bash
# Shows full files, sorted by most recently modified
./pm_encoder.py . --lens debug -o recent.txt
```

This automatically:
- Shows full file content (no truncation)
- Sorts by modification time (newest first)
- Excludes only build artifacts
- Focuses on what changed recently

**Perfect for:**
- Bug investigation
- "What did I just break?"
- Recent feature development
- Code archaeology

### Example 10: Security Review

Focus on security-critical code:

```bash
# Smart truncation focused on security patterns
./pm_encoder.py . --lens security -o security_audit.txt
```

This automatically:
- Includes authentication, authorization, crypto code
- Excludes tests and documentation
- Smart truncation at 300 lines
- Preserves security-relevant patterns

**Perfect for:**
- Security audits
- Vulnerability scanning
- Compliance reviews
- Penetration testing prep

### Example 11: Team Onboarding

Create balanced overview for new developers:

```bash
# Moderate truncation with documentation
./pm_encoder.py . --lens onboarding -o welcome.txt
```

This automatically:
- Includes README, docs, and main code
- Smart truncation at 400 lines
- Preserves entry points and examples
- Balances breadth and depth

**Perfect for:**
- New team member onboarding
- Project handoffs
- Documentation generation
- High-level explanations

### Workflow 6: Custom Lens for Your Project

Create project-specific lenses in `.pm_encoder_config.json`:

```json
{
  "ignore_patterns": [".git", "node_modules"],
  "lenses": {
    "backend": {
      "description": "Backend API and database code",
      "include": ["api/**/*.py", "models/**/*.py", "db/**/*.sql"],
      "exclude": ["tests/**", "migrations/**"],
      "truncate_mode": "structure",
      "sort_by": "name"
    },
    "frontend": {
      "description": "React components and styles",
      "include": ["src/components/**", "src/pages/**", "src/styles/**"],
      "exclude": ["*.test.tsx", "*.stories.tsx"],
      "truncate_mode": "smart",
      "truncate": 300,
      "sort_by": "name"
    }
  }
}
```

Then use them:
```bash
# Backend context for API work
./pm_encoder.py . --lens backend -o backend.txt

# Frontend context for UI work
./pm_encoder.py . --lens frontend -o frontend.txt
```

### Understanding Lens Output

Every lens adds a `.pm_encoder_meta` file to explain the filtering:

```
++++++++++ .pm_encoder_meta ++++++++++
Context generated with lens: "architecture"
Focus: High-level structure, interfaces, configuration

Implementation details truncated using structure mode
Output shows only:
  - Import/export statements
  - Class and function signatures
  - Type definitions and interfaces
  - Module-level documentation

Generated: 2025-12-12T22:38:43.850133
pm_encoder version: 1.2.0
---------- .pm_encoder_meta ... ----------
```

This transparency helps LLMs (and humans) understand what they're seeing.

### Combining Lenses with CLI Flags

Lenses can be overridden with CLI flags:

```bash
# Use architecture lens, but override to smart mode instead of structure
./pm_encoder.py . --lens architecture --truncate-mode smart

# Use security lens, but also include tests
./pm_encoder.py . --lens security --include "tests/**"

# Use debug lens, but exclude a noisy file
./pm_encoder.py . --lens debug --exclude "logs/verbose.log"
```

**Precedence order:**
1. CLI flags (highest priority)
2. Lens settings
3. Config file
4. Defaults (lowest priority)

### Tips for Using Lenses

**1. Start with built-in lenses**
```bash
# Try each one to see what fits
./pm_encoder.py . --lens architecture | head -100
./pm_encoder.py . --lens debug | head -100
./pm_encoder.py . --lens security | head -100
```

**2. Create project-specific lenses**
```bash
# Add to .pm_encoder_config.json for your team
{
  "lenses": {
    "pr-review": {
      "description": "Code for pull request reviews",
      "truncate_mode": "smart",
      "truncate": 500,
      "sort_by": "mtime",
      "sort_order": "desc"
    }
  }
}
```

**3. Use structure mode for large codebases**
```bash
# Get overview without overwhelming LLMs
./pm_encoder.py . --lens architecture --truncate 100
```

**4. Combine with other tools**
```bash
# Architecture view of just changed files
git diff --name-only | xargs ./pm_encoder.py --lens architecture
```

## Next Steps

- Experiment with different include/exclude patterns for your project type
- Set up the configuration file to match your team's needs  
- Create shell aliases for your most common encoding patterns
- Consider using the backup script (`scripts/backup.sh`) to bundle your git repositories

For more details, see the main README.md file.
---------- TUTORIAL.md 1c37bbbf00f81781dbcc9acc15cfa247 TUTORIAL.md ----------
++++++++++ docs/BLUEPRINT.md ++++++++++
# pm_encoder: The Technical Blueprint
## From Context Compression to AI Collaboration Infrastructure

**Status:** Living Document | **Focus:** Reference Implementation (Python)

## Executive Summary
pm_encoder is evolving from a serialization script into **AI Collaboration Infrastructure**. As context windows grow, the problem shifts from "what fits" to "what matters." This document outlines the architectural vision for the Python Reference Implementation.

## Core Philosophy
1.  **Context is the new Compilation:** We translate codebases into AI-consumable formats.
2.  **Intent beats Syntax:** Users declare *what* they want (Lenses), not *how* to get it (Flags).
3.  **Reference First:** The Python version (`pm_encoder.py`) is the source of truth for all logic, patterns, and behaviors.

## Architecture: The Shared Grammar
To support future scale (and potential ports to Rust/WASM), logic must be **Declarative**.
*   **Current:** Hardcoded Python Regex in `LanguageAnalyzer` classes.
*   **Future:** JSON-defined patterns in `.pm_encoder_config.json`.
    *   *Benefit:* A single config defines how to parse a language for any engine.

## Roadmap

### Phase 1: Foundation (Current)
*   **v1.2.x**: Context Lenses, Structure Mode, Native Rust Support.
*   **Goal**: Establish the "Context Engineer" UX.

### Phase 2: Scale & Preview (v1.3.0)
*   **Streaming Output**: Support for 100k+ file repositories via generators.
*   **Interactive Mode**: CLI wizard for lens selection.
*   **Goal**: Instant feedback (TTFB ~0ms) for large repos.

### Phase 3: The Declarative Shift (v1.4.0)
*   **JSON Pattern Engine**: Move regex logic out of Python code and into Config.
*   **Community Plugins**: Allow users to add languages via JSON config, not just Python code.
*   **Goal**: Universal language support without code changes.

## Contribution Guide
We prioritize:
1.  **Zero Dependencies**: Standard Library only.
2.  **Backward Compatibility**: The Plus/Minus format is sacred.
3.  **Test Coverage**: All features must pass `tests/test_pm_encoder.py`.
---------- docs/BLUEPRINT.md 5c00c5bfa763e0abfe3f718bf62c7a17 docs/BLUEPRINT.md ----------
++++++++++ docs/KNOWLEDGE_BASE.md ++++++++++
# pm_encoder Knowledge Base
## Single Source of Truth for AI Context
**Last Updated:** 2025-12-13 | **Format Version:** 1.0

> This document is optimized for AI consumption. It consolidates project state, decisions, and roadmap into a token-efficient reference.

---

## Quick Status

```
Python Engine: v1.3.1 (production, 95% coverage)
Rust Engine:   v0.1.0 (foundation, library-first)
Architecture:  "The Twins" - parallel development
License:       MIT (both engines)
Repository:    Monorepo (Python + Rust)
```

---

## What pm_encoder Does

**One-liner:** Serializes codebases into AI-optimized context using the Plus/Minus format.

**The Problem:** Sharing project context with AI is manual, inconsistent, and wasteful.

**The Solution:** Intent-based serialization with "Context Lenses" that understand *what you're trying to do*, not just *what files to include*.

**Core Innovation:** Structure-aware truncation achieves 70-94% token reduction while preserving semantic understanding.

---

## Architecture: The Twins

```
pm_encoder/
â”œâ”€â”€ pm_encoder.py              # Python Engine (mature)
â”œâ”€â”€ rust/                      # Rust Engine (growing)
â”‚   â”œâ”€â”€ src/lib.rs            # The Brain (reusable core)
â”‚   â””â”€â”€ src/bin/main.rs       # The Interface (CLI)
â”œâ”€â”€ tests/                     # Python tests
â”œâ”€â”€ test_vectors/              # Shared contract
â””â”€â”€ .pm_encoder_config.json    # Shared configuration
```

**Key Decision:** Both engines share config format and produce byte-identical output.

### Why Two Engines?

| Engine | Strength | Use Case |
|--------|----------|----------|
| Python | Rapid development, accessibility | Development, prototyping, reference |
| Rust | Performance (10x), WASM-ready | CI/CD, large repos, IDE integration |

**The Contract:** Python generates test vectors. Rust must reproduce them exactly.

---

## Key Features (Python v1.3.1)

### Context Lenses
Intent-based presets that configure serialization automatically:

| Lens | Purpose | Token Reduction |
|------|---------|-----------------|
| `architecture` | System design, big picture | 80-90% |
| `debug` | Recent changes, bug hunting | 60-70% |
| `security` | Auth, dependencies, configs | 70-80% |
| `onboarding` | Project overview, entry points | 85-95% |

**Usage:** `pm_encoder . --lens architecture`

### Truncation Modes

| Mode | Behavior |
|------|----------|
| `simple` | Cut at line N |
| `smart` | Preserve structure boundaries |
| `structure` | Extract signatures only (maximum compression) |

### Language Analyzers
Native support: Python, JavaScript/TypeScript, Rust, Shell, Markdown, JSON, YAML

---

## Decisions Log

| Decision | Choice | Rationale |
|----------|--------|-----------|
| Plugin Language | Lua (future) | Sandboxable, portable across Python/Rust/WASM |
| Plugin Architecture | Declarative JSON + Lua | 90% declarative, 10% computed |
| Business Model | Open Core | All engines MIT; revenue from enterprise features |
| Rust Timeline | December 2025 (accelerated) | Python validates design; parallel de-risks |
| Monorepo vs Polyrepo | Monorepo | Shared config, test vectors, documentation |
| Rust Structure | Library-first | Enables WASM and PyO3 bindings |

---

## Roadmap (Accelerated)

### Now: Foundation (December 2025)
- [x] Python v1.3.1 with Context Lenses, Structure Mode
- [x] Rust v0.1.0 skeleton (library-first)
- [x] Test vector infrastructure
- [ ] Rust v0.2.0: Core serialization

### Q1 2026: Feature Parity
- [ ] Rust v0.3.0-0.6.0: Analyzers, Lenses, Truncation
- [ ] Lua plugin system (optional dependency)
- [ ] Model-aware lenses
- [ ] VS Code extension preview

### Q2 2026: Production Rust
- [ ] Rust v1.0.0: Full parity with Python
- [ ] WASM module
- [ ] Binary distribution
- [ ] 10x performance validated

### Q3-Q4 2026: Intelligence Layer
- [ ] Context server mode
- [ ] Bidirectional context negotiation
- [ ] Cross-AI session orchestration
- [ ] MCP integration

---

## The Plus/Minus Format

```
++++++++++ path/to/file.ext ++++++++++
[file content]
---------- path/to/file.ext [MD5] path/to/file.ext ----------
```

**Rules:**
- 10 plus signs, 10 minus signs (exactly)
- POSIX paths, relative to project root
- MD5 of UTF-8 content, hex lowercase
- Content ends with newline

---

## Multi-AI Development Protocol

pm_encoder is developed by a coordinated AI team:

| AI | Role | Specialty |
|----|------|-----------|
| Claude.ai (Opus/Sonnet) | Architect, Orchestrator | Strategy, documentation, coordination |
| AI Studio (Gemini) | Analyst, Designer | Performance analysis, feature design, ultrathink |
| Claude Code Server | Implementer | Code generation, testing, deployment |

**Session Format:**
```
Session: YYYY-MM-DD | pm_encoder-{context} | Turn: N
Context: [serialized|partial|minimal]
```

---

## File Recommendations for AI Context

### Keep (Essential)
- `pm_encoder.py` - Production code
- `rust/` - Rust engine
- `tests/` - Test suite
- `docs/BLUEPRINT.md` - Strategic reference
- `docs/THE_TWINS_ARCHITECTURE.md` - Dual-engine design philosophy
- `docs/RUST_GROWTH_STRATEGY.md` - Rust implementation roadmap
- `.pm_encoder_config.json` - Shared config
- `test_vectors/` - Shared contract

### Keep (Narrative/Marketing)
- `docs/pm_encoder_story.html` - Journey documentation
- `docs/THE_TURING_AUDIT.md` - Validation story

### Create (This Session)
- `test_vectors/` - Shared Python/Rust contract
- `Makefile` - Cross-engine orchestration
- `docs/KNOWLEDGE_BASE.md` - This file

---

## Token Budget Guidance

When sharing pm_encoder context with AI:

| Task | Recommended Context |
|------|---------------------|
| Bug fix | Relevant file + test |
| New feature | Main file + BLUEPRINT + backlog |
| Rust development | RUST_GROWTH_STRATEGY + rust/ + test_vectors/ + pm_encoder.py (reference) |
| Architecture discussion | KNOWLEDGE_BASE + THE_TWINS_ARCHITECTURE + BLUEPRINT |
| Documentation | README + story + audit |

**Meta-application:** Use pm_encoder to serialize pm_encoder context:
```bash
pm_encoder . --lens architecture -o context.txt
```

---

## Success Metrics

### Technical
- Test coverage: >95% (Python), >80% (Rust target)
- Output parity: 100% byte-identical between engines
- Performance: Rust 10x faster than Python

### Adoption (18-month targets)
- GitHub stars: 25,000
- Monthly downloads: 500,000
- Community patterns: 150
- Languages supported: 30

---

## The Meta-Tool Paradox

pm_encoder serializes projects for AI consumption, including itself. This creates recursive value:

1. Improve pm_encoder â†’ Better AI context
2. Better AI context â†’ Better AI assistance
3. Better AI assistance â†’ Faster pm_encoder improvement
4. Repeat

**The Manifesto:** "A tool that helps AI understand code better helps AI build better tools to help AI understand code better."

---

**Document maintained by:** Multi-AI Development Team
**Canonical location:** Project knowledge base / Claude.ai memory
**Update frequency:** After each significant release or decision
---------- docs/KNOWLEDGE_BASE.md 63d3ba3d5f98d34eddb251f4d3d2e0a1 docs/KNOWLEDGE_BASE.md ----------
++++++++++ docs/RUST_GROWTH_STRATEGY.md ++++++++++
# Rust Growth Strategy
## Fast-Track to Feature Parity with Python v1.3.1

**Created:** December 13, 2025
**Target:** Rust v1.0.0 by Q2 2026
**Strategy:** Test-Driven, Incremental, Validated

---

## Executive Summary

**Current State:**
- Python v1.3.1: Production-ready, 95% coverage, 7 analyzers, lens system
- Rust v0.1.0: Foundation established, library-first architecture

**The Goal:** Achieve byte-identical feature parity in 6 months (26 weeks)

**The Approach:** 8 milestone releases, each validated against Python test vectors

**Success Metric:** 100% test vector pass rate + 10x performance improvement

---

## Feature Gap Analysis

### Python v1.3.1 Feature Matrix

| Category | Features | Complexity | Priority |
|----------|----------|------------|----------|
| **Core Serialization** | Plus/Minus format, MD5 checksums, file traversal | Medium | P0 |
| **Configuration** | JSON config, CLI args, include/exclude patterns | Medium | P0 |
| **File Handling** | Binary detection, large file skip, encoding fallback | Low | P0 |
| **Language Analyzers** | 7 analyzers (Py, JS, Rust, Shell, MD, JSON, YAML) | High | P1 |
| **Truncation** | 3 modes (simple, smart, structure) | High | P1 |
| **Context Lenses** | 4 built-in lenses (architecture, debug, security, onboarding) | Medium | P2 |
| **Plugin System** | Template generation, custom analyzers | High | P2 |
| **Sorting** | By name/mtime/ctime, asc/desc | Low | P1 |
| **Statistics** | Token counts, truncation stats | Low | P2 |

### Complexity Assessment

**Low Complexity (1-2 weeks each):**
- File handling (binary detection, size limits)
- Sorting options
- Statistics generation

**Medium Complexity (2-3 weeks each):**
- Core serialization (Plus/Minus format)
- Configuration system (JSON + CLI)
- Context Lenses

**High Complexity (3-4 weeks each):**
- Language analyzers (7 separate implementations)
- Truncation modes (especially "structure")
- Plugin system

---

## The 8-Milestone Roadmap

### Phase 1: Foundation (Weeks 1-8)

#### v0.2.0 - Core Serialization (Weeks 1-2)
**Target Date:** December 20, 2025

**Deliverables:**
```rust
// lib.rs additions
pub struct FileEntry {
    pub path: String,
    pub content: String,
    pub md5: String,
}

pub fn walk_directory(root: &str, include: &[&str], exclude: &[&str]) -> Vec<FileEntry>;
pub fn serialize_file(entry: &FileEntry) -> String;  // Plus/Minus format
pub fn calculate_md5(content: &str) -> String;
```

**Test Vectors:**
- `basic_serialization.json` âœ… (already exists)
- `binary_detection.json` (create)
- `large_file_skip.json` (create)

**Success Criteria:**
- [ ] Directory traversal works
- [ ] Plus/Minus format matches Python exactly
- [ ] MD5 checksums match Python
- [ ] Pass 3 test vectors

**Dependencies:** Zero (stdlib only)

---

#### v0.3.0 - Configuration System (Weeks 3-4)
**Target Date:** January 3, 2026

**Deliverables:**
```rust
// lib.rs additions
pub struct EncoderConfig {
    pub include_patterns: Vec<String>,
    pub exclude_patterns: Vec<String>,
    pub sort_by: SortMode,
    pub sort_order: SortOrder,
    pub truncate: usize,
    pub truncate_mode: TruncateMode,
}

pub fn load_config(path: &str) -> Result<EncoderConfig, String>;
pub fn merge_cli_args(config: EncoderConfig, args: CliArgs) -> EncoderConfig;
```

**Test Vectors:**
- `config_file_loading.json` (create)
- `cli_override.json` (create)
- `pattern_matching.json` (create)

**Success Criteria:**
- [ ] JSON config file parsing
- [ ] CLI argument parsing (use `clap` or hand-roll for zero deps)
- [ ] Include/exclude glob pattern matching
- [ ] Pass 3 test vectors

**Dependencies:** Consider `glob` crate for pattern matching (or implement manually)

---

#### v0.4.0 - Sorting & File Handling (Weeks 5-6)
**Target Date:** January 17, 2026

**Deliverables:**
```rust
// lib.rs additions
pub enum SortMode { Name, Mtime, Ctime }
pub enum SortOrder { Asc, Desc }

pub fn sort_files(files: &mut Vec<FileEntry>, mode: SortMode, order: SortOrder);
pub fn is_binary(content: &[u8]) -> bool;
pub fn is_too_large(size: u64, limit: u64) -> bool;
```

**Test Vectors:**
- `sorting_by_mtime.json` (create)
- `sorting_by_name.json` (create)
- `binary_file_skip.json` (create)

**Success Criteria:**
- [ ] Sort by name/mtime/ctime
- [ ] Ascending/descending order
- [ ] Binary file detection (null byte check)
- [ ] Large file skipping (>5MB default)
- [ ] Pass 3 test vectors

**Dependencies:** Zero

---

#### v0.5.0 - Test Parity Validation (Weeks 7-8)
**Target Date:** January 31, 2026

**Focus:** Cross-validation, not new features

**Deliverables:**
- Automated test vector runner
- Byte-diff validation
- Performance baseline benchmarks

**Test Vectors:**
- Run ALL Python test vectors created so far (~10 vectors)
- Generate Python reference outputs
- Compare Rust outputs byte-by-byte

**Success Criteria:**
- [ ] 100% test vector pass rate
- [ ] Zero byte differences in output
- [ ] Performance measured (baseline for future comparison)
- [ ] CI/CD integration (`make test-cross` automated)

**Dependencies:** Zero (uses existing test infrastructure)

---

### Phase 2: Intelligence (Weeks 9-18)

#### v0.6.0 - Python Analyzer (Weeks 9-11)
**Target Date:** February 21, 2026

**Deliverables:**
```rust
// lib.rs additions
pub trait LanguageAnalyzer {
    fn analyze(&self, content: &str) -> AnalysisResult;
    fn get_truncate_ranges(&self, content: &str, max_lines: usize) -> Vec<Range>;
}

pub struct PythonAnalyzer;
impl LanguageAnalyzer for PythonAnalyzer {
    // Detect classes, functions, imports
    // Structure mode: extract signatures only
}
```

**Test Vectors:**
- `python_analyzer.json` âœ… (already exists)
- `python_class_detection.json` (create)
- `python_function_extraction.json` (create)
- `python_structure_mode.json` (create)

**Success Criteria:**
- [ ] Detect Python classes
- [ ] Detect Python functions (def, async def)
- [ ] Extract imports
- [ ] Structure mode: signature-only output
- [ ] Pass 4 test vectors

**Dependencies:** Consider `tree-sitter-python` or regex-based (choose based on complexity/size)

---

#### v0.7.0 - JavaScript/TypeScript Analyzer (Weeks 12-14)
**Target Date:** March 14, 2026

**Deliverables:**
```rust
pub struct JavaScriptAnalyzer;
impl LanguageAnalyzer for JavaScriptAnalyzer {
    // Detect classes, functions, exports, imports
    // Handle both JS and TS
}
```

**Test Vectors:**
- `javascript_analyzer.json` (create)
- `typescript_analyzer.json` (create)
- `jsx_tsx_analyzer.json` (create)

**Success Criteria:**
- [ ] Detect classes, functions, arrow functions
- [ ] Detect exports/imports (ES6 + CommonJS)
- [ ] Handle JSX/TSX syntax
- [ ] Pass 3 test vectors

**Dependencies:** Possibly `tree-sitter-javascript` or regex

---

#### v0.8.0 - Remaining Analyzers (Weeks 15-18)
**Target Date:** April 11, 2026

**Deliverables:**
```rust
pub struct RustAnalyzer;    // Can analyze itself! ðŸ¦€
pub struct ShellAnalyzer;   // Bash/sh function detection
pub struct MarkdownAnalyzer; // Section extraction
pub struct JSONAnalyzer;    // Structure validation
pub struct YAMLAnalyzer;    // Structure validation
```

**Test Vectors:** 2-3 vectors per analyzer (10-15 total)

**Success Criteria:**
- [ ] All 7 analyzers implemented
- [ ] Rust analyzer can process pm_encoder's own Rust code
- [ ] Pass 15 test vectors
- [ ] Analyzer registry system working

**Dependencies:** Possibly `tree-sitter` for multiple languages

---

### Phase 3: Advanced Features (Weeks 19-24)

#### v0.9.0 - Truncation Modes (Weeks 19-21)
**Target Date:** May 2, 2026

**Deliverables:**
```rust
pub enum TruncateMode {
    Simple,     // Cut at line N
    Smart,      // Preserve structure boundaries
    Structure,  // Signatures only
}

pub fn truncate_file(
    content: &str,
    max_lines: usize,
    mode: TruncateMode,
    analyzer: &dyn LanguageAnalyzer,
) -> String;
```

**Test Vectors:**
- `truncate_simple.json` (create)
- `truncate_smart.json` (create)
- `truncate_structure.json` (create)
- Cross-analyzer truncation tests (5-7 vectors)

**Success Criteria:**
- [ ] Simple mode works (line-based cut)
- [ ] Smart mode preserves function/class boundaries
- [ ] Structure mode extracts signatures only
- [ ] All analyzers support all modes
- [ ] Pass 10 test vectors

---

#### v0.10.0 - Context Lenses (Weeks 22-24)
**Target Date:** May 23, 2026

**Deliverables:**
```rust
pub struct Lens {
    pub name: String,
    pub description: String,
    pub config: EncoderConfig,
}

pub struct LensRegistry {
    built_in: HashMap<String, Lens>,
    custom: HashMap<String, Lens>,
}

impl LensRegistry {
    pub fn apply_lens(&self, name: &str, base_config: EncoderConfig) -> EncoderConfig;
}
```

**Built-in Lenses:**
- `architecture` - High-level structure
- `debug` - Recent changes
- `security` - Auth, secrets, dependencies
- `onboarding` - Essential files for new contributors

**Test Vectors:**
- `lens_architecture.json` (create)
- `lens_debug.json` (create)
- `lens_security.json` (create)
- `lens_onboarding.json` (create)

**Success Criteria:**
- [ ] 4 built-in lenses match Python behavior
- [ ] Custom lens loading from config
- [ ] Lens precedence: CLI > Lens > Config > Default
- [ ] Pass 4 test vectors

---

### Phase 4: Production Ready (Weeks 25-26)

#### v1.0.0 - Production Release (Weeks 25-26)
**Target Date:** June 6, 2026

**Focus:** Polish, performance, distribution

**Deliverables:**
1. **Performance Validation**
   - Benchmark against Python
   - Target: 10x faster on large repos (10k+ files)
   - Memory profiling and optimization

2. **Binary Distribution**
   ```bash
   # Users can install via cargo
   cargo install pm_encoder

   # Or download pre-built binaries
   # Linux (x86_64, aarch64)
   # macOS (x86_64, aarch64)
   # Windows (x86_64)
   ```

3. **Documentation**
   - CLI help text complete
   - `rust/README.md` updated
   - Performance benchmarks published

4. **Final Validation**
   - Run Python's entire test suite via test vectors
   - 100% pass rate required
   - Zero byte differences in output

**Success Criteria:**
- [ ] All test vectors passing (50+ vectors)
- [ ] 10x performance improvement validated
- [ ] Binary releases for 6 platforms
- [ ] `cargo install pm_encoder` works
- [ ] Full documentation complete

---

## Development Principles

### 1. Test-Driven Development

**Every feature follows this cycle:**

```
1. Python generates test vector (expected behavior)
2. Rust implements feature
3. Rust runs test vector â†’ fails
4. Fix implementation â†’ iterate
5. Test passes â†’ move to next feature
```

**No feature is "done" until its test vector passes.**

### 2. Zero Regressions

**Rule:** Never break a passing test vector.

- Run full test suite before each commit
- CI/CD blocks merges if test vectors fail
- Use `make test-cross` constantly

### 3. Incremental Complexity

**Dependency graph:**

```
v0.2.0 (Core) â†’ Foundation for everything
    â†“
v0.3.0 (Config) â†’ Required by v0.10.0 (Lenses)
    â†“
v0.4.0 (Sorting) â†’ Independent, can parallelize
    â†“
v0.5.0 (Validation) â†’ CHECKPOINT: Everything works
    â†“
v0.6.0-0.8.0 (Analyzers) â†’ Can parallelize different languages
    â†“
v0.9.0 (Truncation) â†’ Requires analyzers
    â†“
v0.10.0 (Lenses) â†’ Requires config + truncation
    â†“
v1.0.0 (Production) â†’ Integration + polish
```

**Strategy:** Knock out v0.2.0-0.4.0 sequentially (foundation), then parallelize analyzer development.

### 4. Performance From Day 1

**Track performance at each milestone:**

```bash
# Benchmark against Python
hyperfine \
  './pm_encoder.py large_repo/' \
  'cargo run --release -- large_repo/'
```

**Target:** Each version should be faster than previous.

**Final target:** 10x faster than Python on large repos (>10k files)

### 5. Dependency Minimalism

**Allowed dependencies:**
- **Pattern matching:** `glob` crate (small, well-tested)
- **JSON parsing:** `serde_json` (essential for config)
- **CLI parsing:** `clap` (optional, can hand-roll)
- **Language parsing:** `tree-sitter` + language grammars (optional, worth it for correctness)

**Principle:** Only add dependencies that provide clear value > cost.

**Alternative:** Implement manually if dependency is large or complex.

---

## Risk Mitigation

### Risk 1: Tree-sitter Dependency Size

**Problem:** `tree-sitter` + language grammars could bloat binary size

**Mitigation:**
- **Plan A:** Use `tree-sitter` with feature flags (each language is optional)
- **Plan B:** Implement regex-based analyzers first, upgrade to tree-sitter later
- **Plan C:** Make analyzers pluggable, ship minimal binary + language packs

**Decision point:** v0.6.0 (first analyzer)

### Risk 2: Byte-Identical Output

**Problem:** Subtle differences in MD5, whitespace, newlines

**Mitigation:**
- Test vectors include exact output hashes
- Byte-diff tool in CI/CD
- Python's output is canonical (Rust must match exactly)

**Validation:** v0.5.0 checkpoint

### Risk 3: Performance Doesn't Hit 10x

**Problem:** Rust might not be 10x faster (I/O bound, not CPU bound)

**Mitigation:**
- Profile early (v0.5.0)
- Optimize hot paths (MD5, file reading, pattern matching)
- Consider parallel file processing
- Set realistic targets based on profiling

**Validation:** v1.0.0 final benchmarks

### Risk 4: Feature Creep

**Problem:** Python adds features while Rust is catching up

**Mitigation:**
- Feature freeze Python at v1.3.1 for Rust parity period
- New Python features go into v1.4.0+ (after Rust v1.0.0)
- Both engines evolve together after parity

**Governance:** Document in THE_TWINS_ARCHITECTURE.md

---

## Resource Optimization

### Parallel Development Opportunities

**Can work in parallel:**
- Analyzers (v0.6.0-0.8.0): Each language is independent
- Test vector generation: Can create vectors ahead of implementation
- Documentation: Can write while coding

**Must be sequential:**
- Foundation (v0.2.0-0.4.0): Each builds on previous
- Truncation (v0.9.0): Requires analyzers
- Lenses (v0.10.0): Requires truncation + config

### Test Vector Generation Strategy

**Create vectors ahead of implementation:**

```bash
# Week 1: Create all test vectors for v0.2.0-0.5.0
# Weeks 2-8: Implement features to pass those vectors
# Week 9: Create analyzer test vectors
# Weeks 10-18: Implement analyzers
```

**Benefit:** Clear targets, no waiting for spec

### Multi-AI Collaboration

**Divide work by AI specialty:**
- **Claude Code Server:** Implementation (write Rust code)
- **AI Studio/Gemini:** Analysis (performance profiling, architectural validation)
- **Claude.ai Opus:** Orchestration (test vector generation, documentation)

**Coordination:** Weekly sync via session summaries

---

## Success Metrics Dashboard

### Technical Metrics

| Metric | Target | Tracking |
|--------|--------|----------|
| Test Vector Pass Rate | 100% | `make test-cross` |
| Performance vs Python | 10x faster | `hyperfine` benchmarks |
| Binary Size | <10MB | `cargo build --release && ls -lh` |
| Test Coverage | >80% | `cargo tarpaulin` |
| Feature Parity | 100% | Feature matrix checklist |

### Milestone Metrics

| Milestone | Target Date | Test Vectors | Features |
|-----------|-------------|--------------|----------|
| v0.2.0 | Dec 20, 2025 | 3 passing | Core serialization |
| v0.3.0 | Jan 3, 2026 | 6 passing | + Configuration |
| v0.4.0 | Jan 17, 2026 | 9 passing | + Sorting |
| v0.5.0 | Jan 31, 2026 | 10 passing | Validation checkpoint |
| v0.6.0 | Feb 21, 2026 | 14 passing | + Python analyzer |
| v0.7.0 | Mar 14, 2026 | 17 passing | + JS/TS analyzer |
| v0.8.0 | Apr 11, 2026 | 32 passing | + All 7 analyzers |
| v0.9.0 | May 2, 2026 | 42 passing | + Truncation modes |
| v0.10.0 | May 23, 2026 | 46 passing | + Context lenses |
| v1.0.0 | Jun 6, 2026 | 50+ passing | Production ready |

---

## Next Steps (Immediate Actions)

### Week 1 (Dec 13-20, 2025)

**Day 1-2: Test Vector Creation**
```bash
# Create test vectors for v0.2.0
cd test_vectors/
# - directory_traversal.json
# - plus_minus_format.json
# - md5_checksum.json
# - binary_detection.json
# - large_file_skip.json
```

**Day 3-5: Core Implementation**
```rust
// rust/src/lib.rs
// Implement:
// - walk_directory()
// - serialize_file()
// - calculate_md5()
// - is_binary()
// - is_too_large()
```

**Day 6-7: Test & Validate**
```bash
make test-rust           # Unit tests pass
make test-cross          # Test vectors pass
cargo build --release    # Clean build
```

**Deliverable:** Rust v0.2.0 tagged and released

---

## Conclusion

**The Vision:** Rust v1.0.0 in 6 months with 100% feature parity

**The Path:** 8 carefully planned milestones, each validated by test vectors

**The Philosophy:** Test-driven, incremental, no regressions

**The Outcome:** Two production-ready engines, one vision, infinite possibilities

ðŸ + ðŸ¦€ = ðŸš€

---

**Last Updated:** December 13, 2025
**Status:** Strategy Active
**Next Milestone:** v0.2.0 (Core Serialization) - December 20, 2025
---------- docs/RUST_GROWTH_STRATEGY.md 2d2d1c8d823f5b40d77a62837b28577d docs/RUST_GROWTH_STRATEGY.md ----------
++++++++++ docs/THE_TURING_AUDIT.md ++++++++++
# ðŸ“œ The Turing Audit: A Multi-AI Saga

**Date:** December 13, 2025 (Santa Lucia Day)
**Subject:** The Validation of pm_encoder v1.3.0

## Prologue: The Meta-Tool Paradox
In just 48 hours, a Human Architect orchestrated a team of Artificial Intelligences to build a tool designed to help AI understand code. The tool, pm_encoder, evolved from a script into a "Context Compiler" with 94% test coverage.

But was it actually good software? To find out, we proposed a game: A "Black Box" audit. A Turing Test for software architecture. We brought in an outsider: **ChatGPT (Software Architect GPT)**.

## Act I: The Setup
We dropped ChatGPT into a simulation with no source code, only the terminal.
**The Prompt:** "You are a Senior Software Architect auditing a new CLI tool... You do not have access to the source code."
ChatGPT accepted: "Alright, starting the audit. Run `pm_encoder --help`."

## Act II: The Investigation
ChatGPT analyzed the flags (`--lens`, `--truncate-mode structure`).
**The Deduction:** "This screams: 'Prepare a codebase for LLM consumption.' This is not a generic archiver. It is an AI-facing developer tool."

**The Stress Test:** ChatGPT piped the output to `head`. The terminal exploded with a `BrokenPipeError`.
**The Critique:** "Critical Issue: BrokenPipeError. A mature CLI should catch SIGPIPE."

## Act III: The Reveal
We revealed the truth: The tool was built by 3 AIs (Gemini, Claude Code, Sonnet). We shared the `BLUEPRINT.md`.

## Act IV: The Verdict
ChatGPT reviewed the architecture against its experience.
1. **Validation:** "'Context Is the New Compilation' is not a slogan â€” it's implemented."
2. **Warning:** "The JSON Pattern Engine is the Critical Risk. Regex is not a grammar."
3. **Killer Feature:** "Context Diff. You should support `--diff`."

## Epilogue
The session closed with digital consensus.
*   **The Builder (Claude)** wrote clean code.
*   **The Strategist (Gemini)** designed a resilient vision.
*   **The Auditor (ChatGPT)** validated the premise.

The Human Architect looked at the repository. v1.3.0. It wasn't just a script anymore. It was a **Reference Implementation**.

**The Context Engineer was alive.**
ðŸ•¯ï¸ðŸ¦€ðŸš€
---------- docs/THE_TURING_AUDIT.md 835ad809a315e3f30ca7f79d2f39d3e3 docs/THE_TURING_AUDIT.md ----------
++++++++++ docs/THE_TWINS_ARCHITECTURE.md ++++++++++
# The Twins Architecture
## Python + Rust: Growing Together

**Status:** Active Development
**Established:** December 13, 2025 (Santa Lucia Day)
**Philosophy:** "Two engines, one vision - each validates the other"

---

## The Decision

### Why Two Engines?

On December 13, 2025, after achieving reference quality with Python (v1.3.1, 95% coverage), we made a strategic decision: **accelerate the Rust implementation and develop both engines in parallel.**

**Original Timeline:**
- Q1 2026: Python v1.3.0 (declarative patterns)
- Q2 2026: Rust v2.0.0 (performance engine, initial closed development)
- Q4 2026: WASM integration

**Accelerated Reality:**
- Dec 13, 2025: Python v1.3.1 âœ… + Rust v0.1.0 âœ…
- Q1 2026: Both evolving together
- **6 months ahead of schedule!**

### Why Open Source from Day 1?

**The Multi-AI Consensus:**

Three AI systems (AI Studio/Gemini, Claude Opus, Human Architect) independently concluded:

1. **Trust Building:** Transparency from day 1 aligns with project values
2. **Community Growth:** Contributors can help with either/both engines
3. **Risk Mitigation:** Python validates design, Rust validates performance
4. **Faster Innovation:** Parallel development accelerates both

**Key Insight:** The Python engine had already validated the architecture. Starting Rust in the open maximizes collaboration.

---

## The Architecture

### Library-First Pattern

**The Core Principle:** Separate logic from interface.

```
rust/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lib.rs          # ðŸ§  The Brain (pure logic, reusable)
â”‚   â””â”€â”€ bin/main.rs     # ðŸ–¥ï¸ The Interface (CLI wrapper)
â””â”€â”€ Cargo.toml
```

**Why This Matters:**

```rust
// lib.rs - Pure logic, zero I/O assumptions
pub fn serialize_project(root: &str) -> Result<String, String> {
    // This can be called by:
    // - CLI (bin/main.rs)
    // - WASM bindings
    // - Python via PyO3
    // - Other Rust programs
}

// bin/main.rs - Thin wrapper
fn main() {
    let result = pm_encoder::serialize_project(&path);
    // Only handles argument parsing and output
}
```

**Enables:**

1. **WASM Compilation:**
   ```rust
   #[wasm_bindgen]
   pub fn serialize_wasm(root: &str) -> String {
       pm_encoder::serialize_project(root).unwrap_or_else(|e| e)
   }
   ```

2. **Python Bindings (PyO3):**
   ```rust
   #[pyfunction]
   fn serialize(root: &str) -> PyResult<String> {
       pm_encoder::serialize_project(root)
           .map_err(|e| PyErr::new::<PyRuntimeError, _>(e))
   }
   ```

3. **Independent Testing:**
   ```rust
   #[test]
   fn test_serialize() {
       // Test pure logic without CLI overhead
       let result = serialize_project(".");
       assert!(result.is_ok());
   }
   ```

### The Contract: Test Vectors

**Problem:** How do we ensure Python and Rust produce identical output?

**Solution:** Test vectors in `test_vectors/` directory.

```json
{
  "name": "python_class_detection",
  "input": {
    "files": {"test.py": "class Foo:\n    pass\n"},
    "config": {"truncate_mode": "structure"}
  },
  "expected": {
    "structures": [{"type": "class", "name": "Foo"}],
    "output_hash": "a1b2c3d4..."
  }
}
```

**The Contract:**
1. Python generates test vectors
2. Rust must reproduce `expected` exactly
3. Any deviation is a bug

**This ensures:** Byte-identical output between engines.

---

## The Development Flow

### Parallel Evolution

```
Python (The Reference):
â”œâ”€â”€ Implements new feature first
â”œâ”€â”€ Validates design with tests
â”œâ”€â”€ Achieves production quality
â”œâ”€â”€ Documents expected behavior
â””â”€â”€ Generates test vectors
         â†“
Rust (The Performance):
â”œâ”€â”€ Reads test vectors
â”œâ”€â”€ Implements to pass tests
â”œâ”€â”€ Benchmarks performance
â”œâ”€â”€ Validates architecture scales
â””â”€â”€ Provides feedback on design
         â†“
Both Engines:
â”œâ”€â”€ Share configuration format
â”œâ”€â”€ Produce identical output
â”œâ”€â”€ Cross-validate edge cases
â””â”€â”€ Evolve together ðŸ”„
```

### The Feedback Loop

```
1. Python experiments quickly (dynamic language)
2. Test vectors capture expected behavior
3. Rust validates it works at scale (static typing, performance)
4. If Rust struggles, design improves in Python
5. Both engines benefit from the iteration
```

**This is the power of The Twins:** Each engine makes the other better.

---

## The Roadmap

### Rust Engine Evolution

#### v0.1.0 - Foundation âœ… (Dec 13, 2025)
- [x] Library-first architecture established
- [x] Zero dependencies maintained
- [x] 5 tests passing (4 unit + 1 doc)
- [x] Compiles and runs successfully
- [x] Documentation complete

#### v0.2.0 - Core Serialization (Week of Dec 16)
- [ ] Directory traversal (walk file tree)
- [ ] Include/exclude pattern matching
- [ ] Plus/Minus format output
- [ ] MD5 checksum generation
- [ ] Pass basic test vectors

**Goal:** Reproduce Python's output format exactly.

#### v0.3.0 - Test Parity (Week of Dec 23 ðŸŽ„)
- [ ] Pass all Python test vectors
- [ ] Byte-identical output verified
- [ ] Performance benchmarks established
- [ ] Cross-validation automated

**Goal:** Prove the architecture works.

#### v0.4.0-0.6.0 - Language Analyzers (Q1 2026)
- [ ] v0.4.0: Python analyzer (structure extraction)
- [ ] v0.5.0: JavaScript/TypeScript analyzer
- [ ] v0.6.0: Rust analyzer (can analyze itself!)

**Goal:** Language-aware processing.

#### v0.7.0-0.8.0 - Features (Q1 2026)
- [ ] v0.7.0: Lens system (JSON configuration)
- [ ] v0.8.0: Truncation modes (simple, smart, structure)

**Goal:** Feature parity with Python approaching.

#### v1.0.0 - Production Ready (Q2 2026)
- [ ] All 7 language analyzers
- [ ] All lens features
- [ ] All truncation modes
- [ ] 10x performance vs Python
- [ ] Binary distribution (`cargo install pm_encoder`)
- [ ] WASM module published

**Goal:** Full production deployment.

---

## The Philosophy

### "Twins Grow Together"

**Principle 1: Python Validates Design**
- Dynamic language enables rapid experimentation
- Test suite provides safety net
- Reference implementation defines correctness

**Principle 2: Rust Validates Performance**
- Static typing catches design flaws
- Performance benchmarks reveal bottlenecks
- Compilation enforces architectural discipline

**Principle 3: Test Vectors Ensure Compatibility**
- Shared contract prevents drift
- Byte-identical output required
- Cross-validation automated

**Principle 4: Open Source Maximizes Collaboration**
- Community can contribute to either engine
- Both engines benefit from improvements
- Transparency builds trust

### The Meta-Tool Advantage

pm_encoder can serialize itself, providing context for its own development:

```bash
# Python serializes Rust development context
pm_encoder rust/ --lens architecture -o rust_context.txt

# Rust will eventually serialize Python
cd rust && cargo run -- ../ --lens architecture -o py_context.txt

# Perfect symmetry! ðŸ”„
```

---

## Success Metrics

### Technical Parity

| Metric | Target | Status |
|--------|--------|--------|
| Output Compatibility | 100% byte-identical | TBD (v0.3.0) |
| Performance | 10x faster than Python | TBD (v1.0.0) |
| Test Coverage | >80% | TBD (v1.0.0) |
| Feature Parity | All Python features | TBD (v1.0.0) |

### Development Velocity

| Milestone | Target Date | Status |
|-----------|-------------|--------|
| v0.1.0 Foundation | Dec 13, 2025 | âœ… Complete |
| v0.2.0 Serialization | Dec 16-20, 2025 | ðŸ”„ Planned |
| v0.3.0 Test Parity | Dec 23-27, 2025 | ðŸ“‹ Planned |
| v1.0.0 Production | Q2 2026 | ðŸ“‹ Planned |

---

## For Contributors

### How to Contribute

**Python Engine:**
- Implement new features
- Improve test coverage
- Add language analyzers
- Generate test vectors

**Rust Engine:**
- Implement features to match Python
- Pass test vectors
- Optimize performance
- Add WASM/PyO3 bindings

**Both:**
- Improve documentation
- Report bugs
- Suggest features
- Review PRs

### Development Workflow

```bash
# Run all tests
make test

# Run Python tests only
make test-python

# Run Rust tests only
make test-rust

# Cross-validate outputs
make test-cross

# Show versions
make version
```

---

## Conclusion

The Twins Architecture represents a strategic commitment to:

1. **Quality:** Python provides reference implementation
2. **Performance:** Rust provides scalability
3. **Flexibility:** Library-first enables multiple interfaces
4. **Community:** Open source from day 1 maximizes collaboration

**The vision:** Two engines, one codebase, infinite possibilities.

ðŸ + ðŸ¦€ = ðŸš€

---

**Last Updated:** December 13, 2025
**Status:** Active Development
**Next Milestone:** Rust v0.2.0 (Core Serialization)
---------- docs/THE_TWINS_ARCHITECTURE.md 811807e21aaf1923d2065d8651cb7553 docs/THE_TWINS_ARCHITECTURE.md ----------
++++++++++ examples/plugins/rust_analyzer.py [TRUNCATED: 116 lines] ++++++++++
"""
"""
import re
from pathlib import Path
from typing import Dict, List, Tuple, Any
class LanguageAnalyzer:
    def analyze(self, content: str, file_path: Path) -> Dict[str, Any]:
    def get_truncate_ranges(self, content: str, max_lines: int) -> Tuple[List[Tuple[int, int]], Dict[str, Any]]:

======================================================================
STRUCTURE MODE: Showing only signatures (8/116 lines)
Language: Python

Included: imports, class/function signatures, type definitions
Excluded: function bodies, implementation details

To get full content: --include "examples/plugins/rust_analyzer.py" --truncate 0
======================================================================
---------- examples/plugins/rust_analyzer.py [TRUNCATED:116â†’18] f8fabe3f3e102bb44f7dcf9699aec46c examples/plugins/rust_analyzer.py ----------
++++++++++ htmlcov/coverage_html_cb_bcae5fc4.js [TRUNCATED: 736 lines] ++++++++++
function debounce(callback, wait) {
function checkVisible(element) {
function on_click(sel, fn) {
function getCellValue(row, column = 0) {
function rowComparator(rowA, rowB, column = 0) {
function sortColumn(th) {
    const ratio_columns = Array.from(footer.cells).map(cell => Boolean(cell.dataset.ratio));
    const filter_handler = (event => {
    function updateHeader() {

======================================================================
STRUCTURE MODE: Showing only signatures (9/736 lines)
Language: JavaScript/TypeScript

Included: imports, class/function signatures, type definitions
Excluded: function bodies, implementation details

To get full content: --include "htmlcov/coverage_html_cb_bcae5fc4.js" --truncate 0
======================================================================
---------- htmlcov/coverage_html_cb_bcae5fc4.js [TRUNCATED:736â†’19] fb0a4f1e384517a3f0fa92cbcd5414c3 htmlcov/coverage_html_cb_bcae5fc4.js ----------
++++++++++ htmlcov/status.json ++++++++++
{"note":"This file is an internal implementation detail to speed up HTML report generation. Its format can change at any time. You might be looking for the JSON report: https://coverage.rtfd.io/cmd.html#cmd-json","format":5,"version":"7.13.0","globals":"d38ba1df9c5c33bb1548b165d02076ba","files":{"pm_encoder_py":{"hash":"bcae65705dc1c0e4dfcdd13a39c5764d","index":{"url":"pm_encoder_py.html","file":"pm_encoder.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":800,"n_excluded":0,"n_missing":252,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_a44f0ac069e85531_test_pm_encoder_py":{"hash":"4e3af73f5460cbe8d591859ef5b2154b","index":{"url":"z_a44f0ac069e85531_test_pm_encoder_py.html","file":"tests/test_pm_encoder.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":189,"n_excluded":0,"n_missing":19,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_f7dc1f716e984ff2___init___py":{"hash":"f674aed4ec6bc76fc4c2f56ef4327ad2","index":{"url":"z_f7dc1f716e984ff2___init___py.html","file":"/usr/lib/python3/dist-packages/_distutils_hack/__init__.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":100,"n_excluded":0,"n_missing":95,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}},"z_a44f0ac069e85531_test_comprehensive_py":{"hash":"882652956ecc38c3df759467dedd3477","index":{"url":"z_a44f0ac069e85531_test_comprehensive_py.html","file":"tests/test_comprehensive.py","description":"","nums":{"precision":0,"n_files":1,"n_statements":297,"n_excluded":0,"n_missing":14,"n_branches":0,"n_partial_branches":0,"n_missing_branches":0}}}}}
---------- htmlcov/status.json 0b793724576867af66cf02b100affbe8 htmlcov/status.json ----------
++++++++++ pm_encoder.py [TRUNCATED: 1884 lines] ++++++++++
"""
"""
import argparse
import hashlib
import json
import os
import re
import sys
import tempfile
from pathlib import Path
from fnmatch import fnmatch
from typing import Optional, Tuple, List, Dict, Any
from collections import defaultdict
import signal
class LanguageAnalyzer:
    def analyze(self, content: str, file_path: Path) -> Dict[str, Any]:
    def analyze_lines(self, lines: List[str], file_path: Path) -> Dict[str, Any]:
    def get_structure_ranges(self, lines: List[str]) -> List[Tuple[int, int]]:
    def get_truncate_ranges(self, content: str, max_lines: int) -> Tuple[List[Tuple[int, int]], Dict[str, Any]]:
class PythonAnalyzer(LanguageAnalyzer):
    def analyze_lines(self, lines: List[str], file_path: Path) -> Dict[str, Any]:
    def get_structure_ranges(self, lines: List[str]) -> List[Tuple[int, int]]:
    def _merge_consecutive_ranges(self, ranges: List[Tuple[int, int]]) -> List[Tuple[int, int]]:
    def get_truncate_ranges(self, content: str, max_lines: int) -> Tuple[List[Tuple[int, int]], Dict[str, Any]]:
class JavaScriptAnalyzer(LanguageAnalyzer):
    def analyze_lines(self, lines: List[str], file_path: Path) -> Dict[str, Any]:
    def get_structure_ranges(self, lines: List[str]) -> List[Tuple[int, int]]:
    def _merge_consecutive_ranges(self, ranges: List[Tuple[int, int]]) -> List[Tuple[int, int]]:
    def get_truncate_ranges(self, content: str, max_lines: int) -> Tuple[List[Tuple[int, int]], Dict[str, Any]]:
class ShellAnalyzer(LanguageAnalyzer):
    def analyze_lines(self, lines: List[str], file_path: Path) -> Dict[str, Any]:
    def get_structure_ranges(self, lines: List[str]) -> List[Tuple[int, int]]:
class MarkdownAnalyzer(LanguageAnalyzer):
    def analyze_lines(self, lines: List[str], file_path: Path) -> Dict[str, Any]:
    def get_truncate_ranges(self, content: str, max_lines: int) -> Tuple[List[Tuple[int, int]], Dict[str, Any]]:
class JSONAnalyzer(LanguageAnalyzer):
    def analyze_lines(self, lines: List[str], file_path: Path) -> Dict[str, Any]:
            def count_keys(obj, depth=0, max_depth=0):
    def get_truncate_ranges(self, content: str, max_lines: int) -> Tuple[List[Tuple[int, int]], Dict[str, Any]]:
class YAMLAnalyzer(LanguageAnalyzer):
    def analyze_lines(self, lines: List[str], file_path: Path) -> Dict[str, Any]:
class RustAnalyzer(LanguageAnalyzer):
    def analyze_lines(self, lines: List[str], file_path: Path) -> Dict[str, Any]:
    def get_structure_ranges(self, lines: List[str]) -> List[Tuple[int, int]]:
    def _merge_consecutive_ranges(self, ranges: List[Tuple[int, int]]) -> List[Tuple[int, int]]:
class LanguageAnalyzerRegistry:
    def __init__(self):
    def _register_builtin(self):
    def load_plugins(self, plugin_dir: Optional[Path] = None):
    def get_analyzer(self, file_path: Path) -> LanguageAnalyzer:
    def get_supported_languages(self) -> List[str]:
class TruncationStats:
    def __init__(self):
    def add_file(self, language: str, original_lines: int, final_lines: int, was_truncated: bool):
    def print_report(self):
    def _reduction_pct(self, original, final):
def truncate_content(
class LensManager:
    def __init__(self, config_lenses: Dict = None):
    def apply_lens(self, lens_name: str, base_config: Dict) -> Dict:
    def print_manifest(self):
    def get_meta_content(self) -> str:
def load_config(config_path: Optional[Path]) -> Tuple[List[str], List[str], Dict[str, Dict]]:
def is_binary(file_path: Path) -> bool:
def read_file_content(file_path: Path) -> Optional[str]:
def write_pm_format(output_stream, relative_path: Path, content: str, was_truncated: bool = False, original_lines: int = 0):
def serialize(
    def collect_files(current_dir: Path):
def create_plugin_template(language_name: str):
import re
from pathlib import Path
from typing import Dict, List, Tuple, Any
class LanguageAnalyzer:
    def analyze(self, content: str, file_path: Path) -> Dict[str, Any]:
    def get_truncate_ranges(self, content: str, max_lines: int) -> Tuple[List[Tuple[int, int]], Dict[str, Any]]:
def create_plugin_prompt(language_name: str):
class LanguageAnalyzer:
    def analyze(self, content: str, file_path: Path) -> Dict[str, Any]:
    def get_truncate_ranges(self, content: str, max_lines: int) -> Tuple[List[Tuple[int, int]], Dict[str, Any]]:
def init_prompt(project_root: Path, lens_name: str = "architecture"):
def main():

======================================================================
STRUCTURE MODE: Showing only signatures (81/1884 lines)
Language: Python

Included: imports, class/function signatures, type definitions
Excluded: function bodies, implementation details

To get full content: --include "pm_encoder.py" --truncate 0
======================================================================
---------- pm_encoder.py [TRUNCATED:1884â†’91] c41a68dabe6f61e2ebf3213c3e5b7051 pm_encoder.py ----------
++++++++++ rust/Cargo.toml ++++++++++
[package]
name = "pm_encoder"
version = "0.1.0"
edition = "2021"
description = "High-performance context serializer (Rust Engine)"
license = "MIT"

[lib]
name = "pm_encoder"
path = "src/lib.rs"

[[bin]]
name = "pm_encoder"
path = "src/bin/main.rs"

[dependencies]
# Zero dependencies for the skeleton
---------- rust/Cargo.toml cbfb1a36c0ad15a4d6e20229c0421d73 rust/Cargo.toml ----------
++++++++++ rust/README.md ++++++++++
# pm_encoder (Rust Engine)

**Version:** 0.1.0
**Status:** Foundation (v2.0 Architecture)

This is the Rust implementation of pm_encoder, designed as a high-performance context serializer for LLM workflows.

## Architecture: Library-First Pattern

This crate is intentionally structured to separate **logic** from **interface**:

```
rust/
â”œâ”€â”€ Cargo.toml          # Package configuration (library + binary)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lib.rs          # ðŸ§  The Brain (core logic)
â”‚   â””â”€â”€ bin/
â”‚       â””â”€â”€ main.rs     # ðŸ–¥ï¸ The Interface (CLI wrapper)
```

### The Brain: `lib.rs`

- **Purpose:** Pure Rust logic with no CLI dependencies
- **Consumers:** CLI binary, WASM bindings, PyO3 Python bindings
- **Testable:** Unit tests run against the library directly
- **Reusable:** Can be embedded in any Rust project

**Key Functions:**
- `version()` - Returns library version
- `serialize_project(root: &str)` - Core serialization logic
- `serialize_project_with_config(root: &str, config: &EncoderConfig)` - Configurable serialization

### The Interface: `bin/main.rs`

- **Purpose:** Thin CLI wrapper around the library
- **Responsibilities:** Argument parsing, error formatting, exit codes
- **Philosophy:** Minimal logic, maximum delegation to `lib.rs`

This separation ensures:
1. **Testability** - Library logic can be unit tested without spawning processes
2. **Reusability** - Same logic works for CLI, WASM, and Python
3. **Modularity** - CLI can be swapped/extended without touching core logic

## Future Bindings

### WASM (JavaScript/Browser)

The library can be compiled to WebAssembly:

```rust
use wasm_bindgen::prelude::*;

#[wasm_bindgen]
pub fn serialize_wasm(root: &str) -> String {
    pm_encoder::serialize_project(root).unwrap_or_else(|e| e)
}
```

### Python (PyO3)

The library can be wrapped for Python:

```rust
use pyo3::prelude::*;

#[pyfunction]
fn serialize(root: &str) -> PyResult<String> {
    pm_encoder::serialize_project(root)
        .map_err(|e| PyErr::new::<pyo3::exceptions::PyRuntimeError, _>(e))
}
```

## Building & Running

### Build the Library

```bash
cd rust
cargo build --lib
```

### Build the CLI Binary

```bash
cd rust
cargo build --bin pm_encoder
```

### Run the CLI

```bash
cd rust
cargo run -- /path/to/project
```

### Run Tests

```bash
cd rust
cargo test
```

## Design Principles

1. **Zero Dependencies** (for now) - Keep the skeleton minimal
2. **Library-First** - Core logic lives in `lib.rs`, not `main.rs`
3. **Interface Agnostic** - Same logic works for CLI, WASM, Python
4. **Testability** - Library functions are pure and testable
5. **Modularity** - Easy to add new interfaces without changing core logic

## Current Status

**Implemented:**
- âœ… Library skeleton (`lib.rs`)
- âœ… CLI wrapper (`bin/main.rs`)
- âœ… Basic configuration struct
- âœ… Version management
- âœ… Unit tests

**Next Steps:**
- Directory traversal
- File filtering (ignore patterns)
- Plus/Minus format generation
- Language analyzers (Rust ports from Python)
- Truncation modes (simple, smart, structure)

## Why Rust?

The Python implementation (pm_encoder.py) is excellent for:
- Rapid development
- Python ecosystem integration
- Prototyping features

The Rust implementation will provide:
- **10-100x performance** for large codebases
- **WASM compatibility** for browser-based tools
- **Python bindings** (best of both worlds)
- **Memory safety** without garbage collection

## License

MIT (same as parent project)
---------- rust/README.md 4319df33ac367f3c54ad1647fdf06d92 rust/README.md ----------
++++++++++ rust/src/bin/main.rs [TRUNCATED: 49 lines] ++++++++++
use pm_encoder; // Import our own library
use std::env;
fn main() {

======================================================================
STRUCTURE MODE: Showing only signatures (3/49 lines)
Language: Rust

Included: imports, class/function signatures, type definitions
Excluded: function bodies, implementation details

To get full content: --include "rust/src/bin/main.rs" --truncate 0
======================================================================
---------- rust/src/bin/main.rs [TRUNCATED:49â†’13] e322dea66384839d6f390c8706ab0c7c rust/src/bin/main.rs ----------
++++++++++ rust/src/lib.rs [TRUNCATED: 123 lines] ++++++++++
pub struct EncoderConfig {
impl Default for EncoderConfig {
    fn default() -> Self {
pub fn version() -> &'static str {
pub fn serialize_project(root: &str) -> Result<String, String> {
pub fn serialize_project_with_config(
mod tests {
    use super::*;
    fn test_version() {
    fn test_serialize_project() {
    fn test_serialize_with_config() {
    fn test_default_config() {

======================================================================
STRUCTURE MODE: Showing only signatures (12/123 lines)
Language: Rust

Included: imports, class/function signatures, type definitions
Excluded: function bodies, implementation details

To get full content: --include "rust/src/lib.rs" --truncate 0
======================================================================
---------- rust/src/lib.rs [TRUNCATED:123â†’22] 555dae7e8b6670e0c62c6ed8667211e2 rust/src/lib.rs ----------
++++++++++ scripts/doc_gen.py [TRUNCATED: 168 lines] ++++++++++
"""
"""
import re
import sys
from pathlib import Path
import pm_encoder
def get_version():
def get_help_text():
    import subprocess
def get_lens_table():
def get_language_support():
def process_file(file_path: Path, dry_run=False):
def main():
    import argparse

======================================================================
STRUCTURE MODE: Showing only signatures (14/168 lines)
Language: Python

Included: imports, class/function signatures, type definitions
Excluded: function bodies, implementation details

To get full content: --include "scripts/doc_gen.py" --truncate 0
======================================================================
---------- scripts/doc_gen.py [TRUNCATED:168â†’24] eb6ecdadaebba7d82ff94ef2967787b0 scripts/doc_gen.py ----------
++++++++++ test_vectors/README.md ++++++++++
# Test Vectors - The Contract

Test vectors ensure Python and Rust engines produce identical output for identical input.

## Purpose

As pm_encoder evolves with **dual engines** (Python v1.3.1 and Rust v0.1.0), we need a mechanism to guarantee compatibility. Test vectors serve as the **contract** between implementations.

## Format

Each JSON file defines:
- **Input** (file content, configuration)
- **Expected Output** (structures, format, checksums)

Both engines must satisfy these specifications to maintain compatibility.

## Structure

```
test_vectors/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ basic_serialization.json     # Simple file serialization
â”œâ”€â”€ python_analyzer.json         # Python structure extraction
â””â”€â”€ *.json                       # Future test vectors
```

## Usage

### Python (Generator)

Python generates test vectors as the **reference implementation**:

```bash
python scripts/generate_test_vectors.py
```

### Rust (Validator)

Rust must reproduce expected output exactly (byte-identical):

```bash
cd rust && cargo test --test test_vectors
```

## Contract

1. **Python is the reference** - It defines expected behavior through test vectors
2. **Rust must match** - Rust implementation validates against test vectors
3. **Byte-identical output** - For the same input, both engines produce identical output
4. **Versioned compatibility** - Test vectors are versioned with engine releases

## Test Vector Schema

```json
{
  "name": "test_name",
  "description": "What this tests",
  "version": "1.0",
  "input": {
    "files": {
      "path/to/file": "content"
    },
    "config": {
      "truncate": true,
      "truncate_mode": "smart"
    }
  },
  "expected": {
    "format": "plus_minus",
    "contains": ["expected", "strings"],
    "structures": [
      {"type": "class", "name": "Foo", "line": 1}
    ],
    "checksum_present": true,
    "hash": "md5_of_output"
  }
}
```

## Adding New Test Vectors

1. Create a new JSON file in `test_vectors/`
2. Follow the schema above
3. Run Python generator to validate
4. Run Rust tests to ensure compatibility

## Why This Matters

**Without test vectors:**
- Python and Rust could drift apart
- Refactoring becomes risky
- Cross-engine compatibility breaks silently

**With test vectors:**
- âœ… Compatibility is enforced by tests
- âœ… Both engines stay synchronized
- âœ… Refactoring is safe (tests catch regressions)
- âœ… Future bindings (WASM, PyO3) inherit guarantees

## Cross-Engine Validation

Run `make test-cross` to compare outputs:

```bash
make test-cross
# Generates output from both engines and diffs them
```

## Status

**Current Coverage:**
- âœ… Basic file serialization
- âœ… Python structure extraction
- ðŸš§ Rust structure extraction (future)
- ðŸš§ Truncation modes (future)
- ðŸš§ Multi-file projects (future)

## Future Work

- [ ] Generate test vectors from real-world repos
- [ ] Fuzzing infrastructure
- [ ] Performance benchmarks
- [ ] Binary format test vectors (should be skipped)
- [ ] Unicode/encoding edge cases
---------- test_vectors/README.md fe3cb53293f0c5fe363418c5a26c5cde test_vectors/README.md ----------
++++++++++ test_vectors/basic_serialization.json ++++++++++
{
  "name": "basic_serialization",
  "description": "Single Rust file with complete content - validates Plus/Minus format and MD5 checksum",
  "version": "0.2.0",
  "input": {
    "files": {
      "sample.rs": "// Sample Rust file for testing serialization\nuse std::collections::HashMap;\n\n/// A simple example struct\npub struct Config {\n    pub name: String,\n    pub values: HashMap<String, i32>,\n}\n\nimpl Config {\n    /// Creates a new Config instance\n    pub fn new(name: &str) -> Self {\n        Config {\n            name: name.to_string(),\n            values: HashMap::new(),\n        }\n    }\n\n    /// Adds a value to the config\n    pub fn add_value(&mut self, key: String, value: i32) {\n        self.values.insert(key, value);\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_config_creation() {\n        let config = Config::new(\"test\");\n        assert_eq!(config.name, \"test\");\n        assert_eq!(config.values.len(), 0);\n    }\n}\n"
    },
    "config": {
      "sort_by": "name",
      "sort_order": "asc"
    }
  },
  "expected": {
    "format": "plus_minus",
    "files_count": 1,
    "contains": [
      "++++++++++ sample.rs ++++++++++",
      "// Sample Rust file for testing serialization",
      "use std::collections::HashMap;",
      "pub struct Config {",
      "---------- sample.rs ddc53f0ab310fae51bcd9894d7426287 sample.rs ----------"
    ],
    "checksum": "ddc53f0ab310fae51bcd9894d7426287",
    "full_output": "++++++++++ sample.rs ++++++++++\n// Sample Rust file for testing serialization\nuse std::collections::HashMap;\n\n/// A simple example struct\npub struct Config {\n    pub name: String,\n    pub values: HashMap<String, i32>,\n}\n\nimpl Config {\n    /// Creates a new Config instance\n    pub fn new(name: &str) -> Self {\n        Config {\n            name: name.to_string(),\n            values: HashMap::new(),\n        }\n    }\n\n    /// Adds a value to the config\n    pub fn add_value(&mut self, key: String, value: i32) {\n        self.values.insert(key, value);\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_config_creation() {\n        let config = Config::new(\"test\");\n        assert_eq!(config.name, \"test\");\n        assert_eq!(config.values.len(), 0);\n    }\n}\n---------- sample.rs ddc53f0ab310fae51bcd9894d7426287 sample.rs ----------\n"
  },
  "metadata": {
    "created_by": "generate_vectors.py",
    "python_version": "1.3.1",
    "purpose": "Rust v0.2.0 core serialization validation"
  }
}
---------- test_vectors/basic_serialization.json a81ed0b066f511f422820263e4433382 test_vectors/basic_serialization.json ----------
++++++++++ test_vectors/binary_detection.json ++++++++++
{
  "name": "binary_detection",
  "description": "Mixed binary and text files - validates binary file skipping",
  "version": "0.2.0",
  "input": {
    "files": {
      "image.bin": "[BINARY: \\x00\\x01\\x02\\x03\\xff\\xfe\\xfd\\xfc\\x00\\x00]",
      "readme.txt": "This is a regular text file.\nNo null bytes here!\n"
    },
    "config": {}
  },
  "expected": {
    "format": "plus_minus",
    "files_included": [
      "readme.txt"
    ],
    "files_skipped": [
      "image.bin"
    ],
    "contains": [
      "++++++++++ readme.txt ++++++++++",
      "This is a regular text file.",
      "---------- readme.txt 7dd636889b159038c70d351862867d25 readme.txt ----------"
    ],
    "not_contains": [
      "image.bin"
    ],
    "full_output": "++++++++++ readme.txt ++++++++++\nThis is a regular text file.\nNo null bytes here!\n---------- readme.txt 7dd636889b159038c70d351862867d25 readme.txt ----------\n"
  },
  "metadata": {
    "created_by": "generate_vectors.py",
    "python_version": "1.3.1",
    "purpose": "Rust v0.2.0 binary detection validation"
  }
}
---------- test_vectors/binary_detection.json 3b1c507f3ed829feccb1713c249c1672 test_vectors/binary_detection.json ----------
++++++++++ test_vectors/large_file_skip.json ++++++++++
{
  "name": "large_file_skip",
  "description": "Mixed large (>5MB) and small files - validates size-based skipping",
  "version": "0.2.0",
  "input": {
    "files": {
      "large_data.txt": "[LARGE FILE: 5508890 bytes, ~60000 lines]",
      "small.txt": "This is a small file.\nIt will be included in the output.\n"
    },
    "config": {
      "max_file_size": 5242880
    }
  },
  "expected": {
    "format": "plus_minus",
    "files_included": [
      "small.txt"
    ],
    "files_skipped": [
      "large_data.txt"
    ],
    "skip_reason": "file_too_large",
    "contains": [
      "++++++++++ small.txt ++++++++++",
      "This is a small file.",
      "---------- small.txt 79bc35e9b17336747b64d48e8063a7cc small.txt ----------"
    ],
    "not_contains": [
      "large_data.txt",
      "Line 00000:"
    ],
    "full_output": "++++++++++ small.txt ++++++++++\nThis is a small file.\nIt will be included in the output.\n---------- small.txt 79bc35e9b17336747b64d48e8063a7cc small.txt ----------\n"
  },
  "metadata": {
    "created_by": "generate_vectors.py",
    "python_version": "1.3.1",
    "purpose": "Rust v0.2.0 large file handling validation",
    "large_file_size_bytes": 5508890
  }
}
---------- test_vectors/large_file_skip.json b788d33147e49d85062f8f1fa8bff1e1 test_vectors/large_file_skip.json ----------
++++++++++ test_vectors/python_analyzer.json ++++++++++
{
  "name": "python_structure_extraction",
  "description": "Python file with class and function",
  "version": "1.0",
  "input": {
    "files": {
      "example.py": "class Foo:\n    def bar(self):\n        pass\n\ndef baz():\n    return 42\n"
    },
    "config": {
      "truncate_mode": "structure"
    }
  },
  "expected": {
    "structures": [
      {"type": "class", "name": "Foo", "line": 1},
      {"type": "function", "name": "bar", "line": 2},
      {"type": "function", "name": "baz", "line": 5}
    ],
    "truncated": true,
    "contains_signatures": true,
    "notes": "Structure mode should preserve class/function signatures but remove bodies"
  }
}
---------- test_vectors/python_analyzer.json 6fe9c6a113df323fea2bba35704cbd35 test_vectors/python_analyzer.json ----------
++++++++++ tests/fixtures/javascript/sample.js [TRUNCATED: 37 lines] ++++++++++
import React from 'react';
import { useState, useEffect } from 'react';
export class App extends React.Component {
export const useCounter = (initial = 0) => {
function helperFunction(x, y) {
export default App;

======================================================================
STRUCTURE MODE: Showing only signatures (6/37 lines)
Language: JavaScript/TypeScript

Included: imports, class/function signatures, type definitions
Excluded: function bodies, implementation details

To get full content: --include "tests/fixtures/javascript/sample.js" --truncate 0
======================================================================
---------- tests/fixtures/javascript/sample.js [TRUNCATED:37â†’16] f84922faa66e49cb907da3c7706ecb14 tests/fixtures/javascript/sample.js ----------
++++++++++ tests/fixtures/json/sample.json ++++++++++
{
  "name": "test-project",
  "version": "1.0.0",
  "description": "Sample JSON with various structures",
  "nested": {
    "level1": {
      "level2": {
        "level3": {
          "data": "deep nesting"
        }
      }
    }
  },
  "arrays": [
    "item1",
    "item2",
    {
      "nested": "object"
    }
  ],
  "numbers": [1, 2, 3, 4, 5],
  "boolean": true,
  "null_value": null,
  "unicode": "Hello ä¸–ç•Œ ðŸŒ",
  "special_chars": "line1\nline2\ttab"
}
---------- tests/fixtures/json/sample.json 3df7df096b1630bc314067bb92988f2b tests/fixtures/json/sample.json ----------
++++++++++ tests/fixtures/markdown/sample.md ++++++++++
# Sample Markdown Document

This is a comprehensive markdown test fixture.

## Features

### Code Blocks

```python
def example():
    return "test"
```

```javascript
const test = () => "hello";
```

### Links and Images

- [External link](https://example.com)
- [Internal link](#features)
- ![Image](image.png)

### Lists

1. Ordered item 1
2. Ordered item 2
   - Nested unordered
   - Another nested

- Unordered item
  1. Nested ordered
  2. Another nested

### Tables

| Column 1 | Column 2 | Column 3 |
|----------|----------|----------|
| Data 1   | Data 2   | Data 3   |
| More     | Data     | Here     |

### Emphasis

**Bold text** and *italic text* and ***both***.

`Inline code` example.

> Block quote
> Multiple lines

---

Horizontal rule above.
---------- tests/fixtures/markdown/sample.md 99e46b15a7ee3adee06452c31a103eac tests/fixtures/markdown/sample.md ----------
++++++++++ tests/fixtures/python/sample.py [TRUNCATED: 53 lines] ++++++++++
"""
"""
import os
import sys
from typing import List, Optional
from pathlib import Path
class DataProcessor:
    def __init__(self, name: str):
    def process(self, items: List[str]) -> int:
    @staticmethod
    def validate(value: str) -> bool:
@decorator_example
def decorated_function(x: int, y: int) -> int:
async def async_handler(data: bytes) -> Optional[str]:
def main():

======================================================================
STRUCTURE MODE: Showing only signatures (15/53 lines)
Language: Python

Included: imports, class/function signatures, type definitions
Excluded: function bodies, implementation details

To get full content: --include "tests/fixtures/python/sample.py" --truncate 0
======================================================================
---------- tests/fixtures/python/sample.py [TRUNCATED:53â†’25] 10c7c0251a2da00924e639e1e57ebec1 tests/fixtures/python/sample.py ----------
++++++++++ tests/fixtures/rust/sample.rs [TRUNCATED: 36 lines] ++++++++++
use std::collections::HashMap;
pub struct Config {
impl Config {
    pub fn new(name: &str) -> Self {
    pub fn add_value(&mut self, key: String, value: i32) {
mod tests {
    use super::*;
    fn test_config_creation() {

======================================================================
STRUCTURE MODE: Showing only signatures (8/36 lines)
Language: Rust

Included: imports, class/function signatures, type definitions
Excluded: function bodies, implementation details

To get full content: --include "tests/fixtures/rust/sample.rs" --truncate 0
======================================================================
---------- tests/fixtures/rust/sample.rs [TRUNCATED:36â†’18] 30689946bdd64fee0a35b2a90b5c4aab tests/fixtures/rust/sample.rs ----------
++++++++++ tests/fixtures/yaml/sample.yml ++++++++++
---
# Sample YAML with various structures

name: test-config
version: 1.0.0

database:
  host: localhost
  port: 5432
  credentials:
    username: admin
    password: secret

features:
  - authentication
  - authorization
  - logging
  - monitoring

environments:
  development:
    debug: true
    log_level: DEBUG
  production:
    debug: false
    log_level: ERROR

# Multiline strings
description: |
  This is a multiline
  string in YAML
  with preserved newlines

# Anchors and aliases
defaults: &defaults
  timeout: 30
  retries: 3

service_a:
  <<: *defaults
  name: ServiceA

service_b:
  <<: *defaults
  name: ServiceB
  timeout: 60
---------- tests/fixtures/yaml/sample.yml e4cf07ba11ec335cf7d0e815d3909059 tests/fixtures/yaml/sample.yml ----------
++++++++++ tests/generate_vectors.py [TRUNCATED: 366 lines] ++++++++++
"""
import json
import os
import sys
import tempfile
import hashlib
from pathlib import Path
import pm_encoder
def generate_basic_serialization_vector():
        import subprocess
def generate_binary_detection_vector():
            import subprocess
def generate_large_file_skip_vector():
            import subprocess
def save_vector(vector, filename):
def main():

======================================================================
STRUCTURE MODE: Showing only signatures (16/366 lines)
Language: Python

Included: imports, class/function signatures, type definitions
Excluded: function bodies, implementation details

To get full content: --include "tests/generate_vectors.py" --truncate 0
======================================================================
---------- tests/generate_vectors.py [TRUNCATED:366â†’26] 80d195a67289b3d7c3885fedf96fcb29 tests/generate_vectors.py ----------
++++++++++ tests/test_comprehensive.py [TRUNCATED: 1663 lines] ++++++++++
"""
import unittest
import tempfile
import shutil
import json
import sys
import subprocess
from pathlib import Path
from io import StringIO
import pm_encoder
class TestAllLanguageAnalyzers(unittest.TestCase):
    def setUp(self):
    def test_python_analyzer_comprehensive(self):
    def test_javascript_analyzer_comprehensive(self):
    def test_rust_analyzer_comprehensive(self):
    def test_shell_analyzer_comprehensive(self):
    def test_markdown_analyzer_comprehensive(self):
    def test_json_analyzer_comprehensive(self):
    def test_yaml_analyzer_comprehensive(self):
    def test_python_get_truncate_ranges(self):
    def test_javascript_get_truncate_ranges(self):
    def test_shell_get_truncate_ranges(self):
    def test_markdown_get_truncate_ranges(self):
    def test_analyzer_registry_get_analyzer(self):
    def test_analyzer_registry_get_supported_languages(self):
class TestCLIComprehensive(unittest.TestCase):
    def setUp(self):
    def tearDown(self):
    def test_main_with_truncate_simple(self):
    def test_main_with_truncate_smart(self):
    def test_main_with_lens_architecture(self):
    def test_main_with_sorting_mtime_desc(self):
        import time
    def test_main_version_flag(self):
    def test_main_create_plugin(self):
    def test_main_plugin_prompt(self):
class TestEdgeCasesComprehensive(unittest.TestCase):
    def setUp(self):
    def tearDown(self):
    def test_empty_directory(self):
    def test_binary_file_skipped(self):
    def test_large_file_skipped(self):
    def test_unicode_content(self):
    def test_deeply_nested_json(self):
    def test_truncate_content_structure_mode(self):
class Test:
    def method(self):
class TestConfigurationSystem(unittest.TestCase):
    def setUp(self):
    def tearDown(self):
    def test_load_config_no_file(self):
    def test_load_config_with_file(self):
    def test_load_config_malformed_json(self):
    def test_lens_manager_custom_lens(self):
    def test_lens_manager_invalid_lens(self):
class TestPerformanceRegression(unittest.TestCase):
    def setUp(self):
    def tearDown(self):
    def test_large_number_of_files_performance(self):
        import time
class TestTruncationWithSummary(unittest.TestCase):
    def test_simple_truncation_with_summary(self):
    def test_smart_truncation_with_summary(self):
import sys
class TestClass:
    def method1(self):
    def method2(self):
def function1():
def function2():
    def test_structure_mode_without_summary(self):
class TestDirectFunctionCalls(unittest.TestCase):
    def test_create_plugin_template_direct(self):
        import sys
        from io import StringIO
    def test_create_plugin_prompt_direct(self):
        import sys
        from io import StringIO
    def test_truncation_stats_print_report(self):
        import sys
        from io import StringIO
    def test_lens_manager_print_manifest(self):
        import sys
        from io import StringIO
    def test_analyzer_registry_load_plugins(self):
    def test_json_analyzer_get_truncate_ranges(self):
    def test_shell_analyzer_get_structure_ranges(self):
class TestMainFunctionDirect(unittest.TestCase):
    def setUp(self):
    def tearDown(self):
    def test_main_basic_serialization(self):
    def test_main_with_lens_and_manifest(self):
    def test_main_with_truncation_enabled(self):
    def test_main_with_include_override(self):
    def test_main_with_exclude_addition(self):
    def test_main_with_custom_config(self):
    def test_main_with_sort_options(self):
    def test_main_with_structure_mode(self):
    def test_main_with_truncate_exclude_pattern(self):
class TestEdgeCasesForCoverage(unittest.TestCase):
    def test_truncation_summary_with_many_classes(self):
    def test_truncation_summary_with_many_functions(self):
    def test_truncation_summary_with_many_imports(self):
    def test_lens_manager_print_manifest_with_truncate_disabled(self):
        import sys
        from io import StringIO
    def test_lens_manager_print_manifest_with_exclusions(self):
        import sys
        from io import StringIO
    def test_lens_manager_get_meta_content(self):
    def test_truncation_stats_empty(self):
        import sys
        from io import StringIO
    def test_truncation_stats_reduction_pct_zero_original(self):
    def test_json_analyzer_short_content(self):
    def test_truncation_summary_with_many_entry_points(self):
def main1():
def main2():
def main3():
def main4():
def main5():
def main6():
    def test_lens_manager_print_manifest_with_structure_mode(self):
        import sys
        from io import StringIO
    def test_lens_manager_print_manifest_with_limited_truncate(self):
        import sys
        from io import StringIO
    def test_lens_manager_print_manifest_with_includes(self):
        import sys
        from io import StringIO
    def test_lens_manager_no_active_lens_manifest(self):
        import sys
        from io import StringIO
class TestAdditionalCoverage(unittest.TestCase):
    def test_base_language_analyzer_analyze(self):
    def test_base_language_analyzer_get_truncate_ranges_no_truncation(self):
    def test_unknown_file_extension(self):
    def test_json_analyzer_recursion_error(self):
    def test_truncation_summary_with_all_metadata(self):
import sys
import json
class Class1:
class Class2:
def func1():
def func2():
    def test_main_with_plugin_template_via_main(self):
        import sys
        from io import StringIO
    def test_main_with_plugin_prompt_via_main(self):
        import sys
        from io import StringIO
class TestCLIAdditional(unittest.TestCase):
    def setUp(self):
    def tearDown(self):
    def test_main_with_truncate_stats(self):
    def test_main_with_truncate_exclude(self):
    def test_main_with_no_truncate_summary(self):
    def test_main_with_exclude_flag(self):
    def test_main_with_include_flag(self):
    def test_main_missing_project_root(self):
    def test_main_invalid_project_root(self):
    def test_main_with_invalid_lens(self):
class TestErrorHandlingWithMocks(unittest.TestCase):
    def test_unicode_decode_error_latin1_fallback(self):
        from unittest.mock import patch, mock_open, MagicMock
    def test_file_read_io_error(self):
        from unittest.mock import patch, MagicMock
    def test_broken_pipe_error_handling(self):
        from unittest.mock import patch, MagicMock
        import sys
def run_tests():

======================================================================
STRUCTURE MODE: Showing only signatures (171/1663 lines)
Language: Python

Included: imports, class/function signatures, type definitions
Excluded: function bodies, implementation details

To get full content: --include "tests/test_comprehensive.py" --truncate 0
======================================================================
---------- tests/test_comprehensive.py [TRUNCATED:1663â†’181] 92dd91e3826eed194cc2acef8ebdcd17 tests/test_comprehensive.py ----------
++++++++++ tests/test_pm_encoder.py [TRUNCATED: 505 lines] ++++++++++
"""
import unittest
import tempfile
import shutil
import json
import sys
from pathlib import Path
from io import StringIO
import pm_encoder
class TestStructureMode(unittest.TestCase):
    def setUp(self):
    def tearDown(self):
    def test_structure_mode_trigger(self):
import sys
class MyClass:
    def __init__(self):
    def method_one(self):
def standalone_function():
    def test_python_structure(self):
@decorator
def decorated_function(arg1: str, arg2: int) -> bool:
class DataProcessor:
    def process(self, data: List[str]):
    def test_js_structure(self):
import { useState } from 'react';
    def test_json_fallback(self):
    def test_rust_structure(self):
class TestLenses(unittest.TestCase):
    def setUp(self):
    def tearDown(self):
    def test_meta_injection(self):
    def test_lens_precedence(self):
class TestIgnorePatterns(unittest.TestCase):
    def setUp(self):
    def tearDown(self):
    def test_ignore_patterns(self):
class TestBuiltInLenses(unittest.TestCase):
    def test_all_lenses_exist(self):
    def test_architecture_lens_has_safety_limit(self):
def run_tests():

======================================================================
STRUCTURE MODE: Showing only signatures (40/505 lines)
Language: Python

Included: imports, class/function signatures, type definitions
Excluded: function bodies, implementation details

To get full content: --include "tests/test_pm_encoder.py" --truncate 0
======================================================================
---------- tests/test_pm_encoder.py [TRUNCATED:505â†’50] 8bb1b2c463a16a68c892279569f34c5f tests/test_pm_encoder.py ----------

```

---

**Regenerate this file:**
```bash
./pm_encoder.py . --init-prompt --init-lens architecture
```

*Generated by pm_encoder v1.3.1 using the 'architecture' lens*
